{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 6 (RNN).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marissa-graham/deep_learning/blob/master/Lab_6_(RNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "atZ2YR-q_mc2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installs and imports"
      ]
    },
    {
      "metadata": {
        "id": "wPjW3rTF8_jg",
        "colab_type": "code",
        "outputId": "fa4b55f0-5756-49f5-ff8b-1639d7fb5f99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n",
        "!tar -xzf text_files.tar.gz\n",
        "!pip3 install torch torchvision tqdm unidecode"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-16 13:41:35--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 34.237.217.71, 34.200.202.18, 52.20.136.189, ...\n",
            "Connecting to piazza.com (piazza.com)|34.237.217.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2018-10-16 13:41:36--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 54.192.119.146, 54.192.119.175, 54.192.119.123, ...\n",
            "Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|54.192.119.146|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  2.64MB/s    in 0.6s    \n",
            "\n",
            "2018-10-16 13:41:37 (2.64 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 30kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5986c000 @  0x7f83708b12a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.26.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision, unidecode\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1 unidecode-1.0.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7hi6xrWG9ECp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import unidecode # used\n",
        "import string # used\n",
        "import random # used\n",
        "import time\n",
        "import re\n",
        " \n",
        "import pdb\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch import matmul as MM\n",
        "\n",
        "from torchvision import transforms, models\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn\n",
        "\n",
        "assert torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wkpDBCnR_qps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classes and small helpers"
      ]
    },
    {
      "metadata": {
        "id": "iBa-ajeL9GF0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, num_directions=1):\n",
        "        \n",
        "        super(GRU, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Zero initialization worked bad, (-1,1) worked bad\n",
        "        m = self.hidden_size\n",
        "        n = self.input_size\n",
        "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
        "        a = -stdv\n",
        "        b = stdv\n",
        "        \n",
        "        self.Wir = torch.empty(m, n).uniform_(a,b)\n",
        "        self.Wiz = torch.empty(m, n).uniform_(a,b)\n",
        "        self.Win = torch.empty(m, n).uniform_(a,b)\n",
        "        \n",
        "        self.bir = torch.empty(n).uniform_(a,b)\n",
        "        self.biz = torch.empty(n).uniform_(a,b)\n",
        "        self.bin = torch.empty(n).uniform_(a,b)\n",
        "        \n",
        "        self.Whr = torch.empty(m, n).uniform_(a,b)\n",
        "        self.Whz = torch.empty(m, n).uniform_(a,b)\n",
        "        self.Whn = torch.empty(m, n).uniform_(a,b)\n",
        "        \n",
        "        self.bhr = torch.empty(n).uniform_(a,b)\n",
        "        self.bhz = torch.empty(n).uniform_(a,b)\n",
        "        self.bhn = torch.empty(n).uniform_(a,b)\n",
        "        \n",
        "        self.S = nn.Sigmoid() # Maybe try different ones for r and z?\n",
        "        self.T = nn.Tanh()\n",
        "        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"\n",
        "        Compute the forward step for the GRU.\n",
        "        \n",
        "        r (reset_gate) : Sigmoid(W_ir * x + b_ir + W_hr * h + b_hr)\n",
        "        z (update_gate) : Sigmoid(W_iz * x + b_iz + W_hz * h + b_hz)\n",
        "        n (new_gate) = tanh(W_in * x + b_in + r * (W_hn * h + b_hn) )\n",
        "        h = (1-z)*n + z*h\n",
        "        \n",
        "        Inputs\n",
        "        ------\n",
        "        x is shape (seq_len, batch_size, input_size)\n",
        "        h is shape (num_layers * num_directions, batch_size, hidden_size)\n",
        "        \n",
        "        Outputs\n",
        "        -------\n",
        "        output is shape (seq_len, batch, num_directions * hidden_size)\n",
        "        (1, 1, hidden_size)\n",
        "        hidden is shape (num_layers * num_directions, batch_size, hidden_size)\n",
        "        (1, 1, hidden_size)\n",
        "        \n",
        "        Hidden size is unchanged, x is num_directions * 100 instead of 100\n",
        "        \"\"\"\n",
        "        x = input.view(self.input_size)\n",
        "        h = hidden.view(self.hidden_size)\n",
        "        \n",
        "        r = self.S(MM(self.Wir, x) + self.bir + MM(self.Whr, h) + self.bhr)\n",
        "        z = self.S(MM(self.Wiz, x) + self.biz + MM(self.Whz, h) + self.bhz)\n",
        "        n = self.T(MM(self.Win, x) + self.bin + r*(MM(self.Whn, h)+self.bhn))\n",
        "        out = (1-z)*n + z*h\n",
        "        return out.view(1, 1, self.input_size), out.view(1, 1, self.hidden_size)\n",
        "        \n",
        "    \n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"RNN implementation used to encode inputs and decode possible outputs.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, myGRU=True):\n",
        "        \n",
        "        super(RNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        if myGRU:\n",
        "            self.gru = GRU(hidden_size, hidden_size)\n",
        "        else:\n",
        "            self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, input_char, hidden):\n",
        "        \"\"\"\n",
        "        Implement a forward function that uses the output of the GRU.\n",
        "        Return output and hidden.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_char).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size)#.requires_grad_()\n",
        "\n",
        "\n",
        "def train(decoder, decoder_optimizer, criterion, inp, target, chunk_len=200):\n",
        "    \"\"\"\n",
        "    Trains the model against a single training instance and returns the loss.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    decoder (RNN): An instance of the RNN encoder/decoder you implemented.\n",
        "    decoder_optimizer (torch.optim.Optimizer): An instance of the desired \n",
        "                                               optimization algorithm.\n",
        "    criterion (nn.CrossEntropyLoss): An instance of the desired loss class.\n",
        "    inp (str): The starting \"priming\" string. (a chunk, I think)\n",
        "    target (str): The resulting target string which completes the input.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int: The loss for this training instance.\n",
        "    \"\"\"\n",
        "    # Initialize hidden layers; set up gradient and loss.\n",
        "    hidden = decoder.init_hidden()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    \n",
        "    # Train through the chunk\n",
        "    for c in range(chunk_len):\n",
        "        \n",
        "        # This is with teacher forcing\n",
        "        output, hidden = decoder(inp[c], hidden)\n",
        "        loss += criterion(output, target[c].unsqueeze(0))\n",
        "\n",
        "    # Calculate backwards loss and step the optimizer (globally)\n",
        "    loss.backward()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / chunk_len\n",
        "\n",
        "def evaluate(decoder, all_chars, prime_str='A', predict_len=100, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Generates or predicts a reasonable completion to some starting string.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    decoder (RNN): An instance of the RNN encoder/decoder which you implemented.\n",
        "    all_chars (str): The sequence of all characters in the dataset.\n",
        "    prime_str (str): The starting \"priming\" string; defaults to 'A'.\n",
        "    predict_len (int): The desired length of the resulting predicted string;\n",
        "               defaults to 100.\n",
        "    temperature (float): The degree of randomness used when sampling possible\n",
        "                 character predictions.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str: The predicted completion of the string.\n",
        "    \"\"\"\n",
        "    # Initialization of hidden variable and tensor of indices for starting str\n",
        "    hidden = decoder.init_hidden()\n",
        "    prime_input = char_tensor(prime_str, all_chars)\n",
        "\n",
        "    # Use the priming string to \"build up\" hidden state.\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[p], hidden)\n",
        "    inp = prime_input[-1]\n",
        "\n",
        "    predicted = \"\"\n",
        "    for p in range(predict_len):\n",
        "        \n",
        "        # Run your RNN/decoder forward on the input to complete the next line.\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "\n",
        "        # Sample possible outputs from the network as a multinomial distribution.\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "              \n",
        "        # Get character from your list of all characters, add it to the\n",
        "        # predicted str sequence, and set input for next pass through the model\n",
        "        predicted += all_chars[top_i]\n",
        "        inp = top_i\n",
        "\n",
        "    return predicted\n",
        "\n",
        "# Small helper functions\n",
        "\n",
        "def random_chunk(full_string, chunk_len=200):\n",
        "    start = random.randint(0, len(full_string) - chunk_len)\n",
        "    return full_string[start:start+chunk_len+1]\n",
        "\n",
        "def char_tensor(mystr, all_chars=string.printable):\n",
        "    return torch.tensor([all_chars.index(mystr[i]) for i in range(len(mystr))],\n",
        "                       dtype=torch.long)\n",
        "\n",
        "def random_training_set(full_string, chunk_len, all_chars):\n",
        "    chunk = random_chunk(full_string, chunk_len=chunk_len)\n",
        "    return char_tensor(chunk[:-1]), char_tensor(chunk[1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a1cGK2Pp_tE8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Main"
      ]
    },
    {
      "metadata": {
        "id": "ccsmURlv9J0p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_testing(filename='./text_files/lotr.txt', myGRU=True):\n",
        "\n",
        "    # Read a file from the downloaded file sources into memory.\n",
        "    file_contents = unidecode.unidecode(open(filename, errors='backslashreplace').read())\n",
        "    print(\"File length:\", len(file_contents))\n",
        "    \n",
        "    # My dataset has lots of gross Unicode chars and this is the easiest way to\n",
        "    # deal with that\n",
        "    \n",
        "    readable = \"\"\n",
        "    for c in range(len(file_contents)):\n",
        "        try:\n",
        "            readable += file_contents[c]\n",
        "        except ValueError:\n",
        "            print(\"failed at\", c)\n",
        "    file_contents = readable\n",
        "    print(\"Readable length:\", len(readable))\n",
        "    \n",
        "    # Set up some tunable parameters.\n",
        "    \n",
        "    chunk_len = 200\n",
        "    num_epochs = 10000\n",
        "    print_every = 200\n",
        "    plot_every = 10\n",
        "    final_samples = 15\n",
        "\n",
        "    hidden_size = 100\n",
        "    num_layers = 1\n",
        "    learning_rate = 0.005\n",
        "    all_chars = string.printable\n",
        "\n",
        "    # Define the RNN encoder/decoder and our optimization function.\n",
        "    num_chars = len(all_chars)\n",
        "    decoder = RNN(num_chars, hidden_size, num_chars, num_layers, myGRU=myGRU)\n",
        "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Keep track of the training time and losses and print them periodically.\n",
        "    start = time.time()\n",
        "    all_losses = []\n",
        "    loss_avg = 0\n",
        "\n",
        "    # Begin training. Print predicted strings so we know how we're doing.\n",
        "    for epoch in range(num_epochs+1):\n",
        "\n",
        "        loss_ = train(decoder, decoder_optimizer, criterion,\n",
        "              *random_training_set(file_contents, chunk_len, all_chars))       \n",
        "        loss_avg += loss_\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            ctime = time.time() - start\n",
        "            print('>>> Epoch %d/%d: Time %.2f, %.2f its/s, Loss: %.2f' % \n",
        "                  (epoch, num_epochs, ctime, epoch/ctime, loss_))\n",
        "            print(evaluate(decoder, all_chars, 'Wh', 100), \"\\n>>>\")\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            all_losses.append(loss_avg / plot_every)\n",
        "            loss_avg = 0\n",
        "        \n",
        "        if num_epochs - epoch == final_samples:\n",
        "            print(\"\\n\\nFINAL SAMPLES\\n\\n\")\n",
        "        if num_epochs - epoch < final_samples:\n",
        "            print(evaluate(decoder, all_chars, 'Wh', 100), \"\\n>>>\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "an1IdmNLfVvO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make my own dataset\n",
        "\n",
        "For my thesis project, I made a citation network of papers within the field of network/graph comparison. I hand-collected and cleaned .txt files containing plaintext citations of the reference lists of 221 papers. I am training the RNN on a concatenation of these. It's about half the size of the LOTR dataset."
      ]
    },
    {
      "metadata": {
        "id": "6lcOzojQOW8P",
        "colab_type": "code",
        "outputId": "c672a147-3b90-4af8-ea74-32baea02e361",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "file_dict = files.upload() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f70df860-865e-4c81-bd17-5702eeaddbdc\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f70df860-865e-4c81-bd17-5702eeaddbdc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ALL_CITATIONS.txt to ALL_CITATIONS.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E1mZugHjAeIL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LOTR Results (my GRU and nn.GRU)"
      ]
    },
    {
      "metadata": {
        "id": "N0pWwJyK0-rf",
        "colab_type": "code",
        "outputId": "65acbef3-a025-4dc1-808a-e19740db11e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5542
        }
      },
      "cell_type": "code",
      "source": [
        "run_testing(myGRU=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File length: 2579888\n",
            "Readable length: 2579888\n",
            ">>> Epoch 0/10000: Time 0.21, 0.00 its/s, Loss: 4.61\n",
            "8B6c}o]&7\tLsmoz3Gc\n",
            "s61d54%kCDF.!1\n",
            " )C`VzByl^9U*{.<u1~l(yA^vy\tCjbo(]CCg(atMuxKuz=GzmC \n",
            ">>>\n",
            ">>> Epoch 200/10000: Time 42.77, 4.68 its/s, Loss: 2.03\n",
            "ed whible all of at yourw's \n",
            "the the as aall the the wingh ford ar a therond wave sat and \n",
            "the fver  \n",
            ">>>\n",
            ">>> Epoch 400/10000: Time 86.78, 4.61 its/s, Loss: 1.99\n",
            "e doust i sing at all and aruter of the Ellol.' \n",
            "'I \n",
            "ghall you howled \n",
            "caye seed up evenes, is all i \n",
            ">>>\n",
            ">>> Epoch 600/10000: Time 129.85, 4.62 its/s, Loss: 1.79\n",
            "ing dalled loge sheems! Me the Cair ever the rame shemed. \n",
            "\n",
            "The hes as of \n",
            "nowing that lell the whig \n",
            ">>>\n",
            ">>> Epoch 800/10000: Time 173.57, 4.61 its/s, Loss: 1.83\n",
            "ere say whirked be days in the bent in the Darcheres, itly and his now the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tranduly glole the M \n",
            ">>>\n",
            ">>> Epoch 1000/10000: Time 216.56, 4.62 its/s, Loss: 1.90\n",
            "je and you the dorn oon at he cold was door rang a craming in a stoomeries in spase for \n",
            "the could w \n",
            ">>>\n",
            ">>> Epoch 1200/10000: Time 260.48, 4.61 its/s, Loss: 1.64\n",
            "ur. The \n",
            "eprested a shad were was he werclves of \n",
            "the the retwere is sevens beastering will was now  \n",
            ">>>\n",
            ">>> Epoch 1400/10000: Time 304.36, 4.60 its/s, Loss: 1.50\n",
            " to moment a was \n",
            "was one other want sword he has or gold it should made to bearmint arm \n",
            "here was h \n",
            ">>>\n",
            ">>> Epoch 1600/10000: Time 348.77, 4.59 its/s, Loss: 1.62\n",
            "at, and is heard a black that into misgot gare then I lake was said Lorder hope. As to endeadol. The \n",
            ">>>\n",
            ">>> Epoch 1800/10000: Time 393.51, 4.57 its/s, Loss: 1.66\n",
            "t see come hoppecul, and though somes on cance dees band he came come bit \n",
            "on the pach hoping strain \n",
            ">>>\n",
            ">>> Epoch 2000/10000: Time 436.38, 4.58 its/s, Loss: 1.54\n",
            "al a was coned lands of Bris lad,' said any and Mather of came of me ring in a goudden of Moor stran \n",
            ">>>\n",
            ">>> Epoch 2200/10000: Time 479.46, 4.59 its/s, Loss: 1.91\n",
            "tted theerrees wided and light to do and side roud showe, doy and rair if \n",
            "\n",
            "they day the Shire,' sai \n",
            ">>>\n",
            ">>> Epoch 2400/10000: Time 522.74, 4.59 its/s, Loss: 1.41\n",
            "-tower, 'and \n",
            "Elveshed he had Gilline of Slown But you even shing,' said Sidrom, where west demed it \n",
            ">>>\n",
            ">>> Epoch 2600/10000: Time 566.04, 4.59 its/s, Loss: 1.83\n",
            " your is paped \n",
            "in he gretter in the thought throught of the had \n",
            "merak be from the rought do be fro \n",
            ">>>\n",
            ">>> Epoch 2800/10000: Time 608.56, 4.60 its/s, Loss: 1.50\n",
            " that winder some \n",
            "prodoning. He \n",
            "wistern. \n",
            "\n",
            "'In spoke!' \n",
            "\n",
            "'I will fagor. 'I go the for a copping ro \n",
            ">>>\n",
            ">>> Epoch 3000/10000: Time 651.66, 4.60 its/s, Loss: 1.97\n",
            " saw to see, he \n",
            "grew the had near his from the sunething in the right assed him said Gollum, you cu \n",
            ">>>\n",
            ">>> Epoch 3200/10000: Time 694.12, 4.61 its/s, Loss: 1.58\n",
            ", the dark a there way mayss to a nome broker, in eest hought bride tagues only had not to behind; b \n",
            ">>>\n",
            ">>> Epoch 3400/10000: Time 736.33, 4.62 its/s, Loss: 1.69\n",
            "t greath the grousad went. There the greed had an \n",
            "of the look. \n",
            "\n",
            "The had him \n",
            "the old Thjodens thro \n",
            ">>>\n",
            ">>> Epoch 3600/10000: Time 778.91, 4.62 its/s, Loss: 1.61\n",
            "e them its there would newer oners the carp we I know you silues, and it were \n",
            "flows looking, \n",
            "and h \n",
            ">>>\n",
            ">>> Epoch 3800/10000: Time 821.39, 4.63 its/s, Loss: 1.57\n",
            " evir the shares tood he waybell. On would befering trounds to be nonger toobout your frows somethin \n",
            ">>>\n",
            ">>> Epoch 4000/10000: Time 864.93, 4.62 its/s, Loss: 1.65\n",
            " way of usuble come and beore, Matil for Gald we shew in the way, and the talk and side call the wal \n",
            ">>>\n",
            ">>> Epoch 4200/10000: Time 907.79, 4.63 its/s, Loss: 1.51\n",
            " said them it way. And the \n",
            "were eyes. But ret the ever \n",
            "brage the guale down head on him strangern  \n",
            ">>>\n",
            ">>> Epoch 4400/10000: Time 951.30, 4.63 its/s, Loss: 1.76\n",
            "ates left young: Gimplealed, and no words mid of the Manstlent with with watching him that \n",
            "mirnend  \n",
            ">>>\n",
            ">>> Epoch 4600/10000: Time 996.11, 4.62 its/s, Loss: 1.71\n",
            " \n",
            "from the hill be had beind. The concally him firsh and some is spence: \n",
            "getting \n",
            "path deep an \n",
            "int \n",
            ">>>\n",
            ">>> Epoch 4800/10000: Time 1041.15, 4.61 its/s, Loss: 1.86\n",
            "at of Glad them: the pate, the momened \n",
            "with wallured fell \n",
            "turn find camour plame mounther the terr \n",
            ">>>\n",
            ">>> Epoch 5000/10000: Time 1084.40, 4.61 its/s, Loss: 1.81\n",
            "an \n",
            "with his, and his stoppring in the than and Boromir grahiver, and the Dight and was is the \n",
            "his  \n",
            ">>>\n",
            ">>> Epoch 5200/10000: Time 1127.44, 4.61 its/s, Loss: 1.61\n",
            " Iss a mast.' \n",
            "\n",
            "All meces. \n",
            "He was to our hill her not lay siam. \n",
            "\n",
            "'That Is dark prolk up save and a \n",
            ">>>\n",
            ">>> Epoch 5400/10000: Time 1171.06, 4.61 its/s, Loss: 1.69\n",
            "atter to their last o \n",
            "only to \n",
            "me me the wore you cang spetless of My cands to get the \n",
            "enom be to  \n",
            ">>>\n",
            ">>> Epoch 5600/10000: Time 1214.15, 4.61 its/s, Loss: 1.42\n",
            " the full to go been him log one, and say deep of plames from the right the lorl must that trough fr \n",
            ">>>\n",
            ">>> Epoch 5800/10000: Time 1257.30, 4.61 its/s, Loss: 1.46\n",
            "un \n",
            "streaps wonder mackly in I and the shall and the elve and lost shadows beiven and me into the wa \n",
            ">>>\n",
            ">>> Epoch 6000/10000: Time 1300.41, 4.61 its/s, Loss: 1.47\n",
            "at the there a glick, and no darking the at gater any out, and Are a givers a pool no do no was was  \n",
            ">>>\n",
            ">>> Epoch 6200/10000: Time 1344.12, 4.61 its/s, Loss: 1.65\n",
            "e topeal. \n",
            "\n",
            "It were fire. 'Lum them by a way from the dreath lead \n",
            "by the blozed the \n",
            "remain of then \n",
            ">>>\n",
            ">>> Epoch 6400/10000: Time 1390.30, 4.60 its/s, Loss: 1.44\n",
            "em of deep do not many I came in him the voice he curning both for a Peregrin his head. \n",
            "\n",
            "Wene, Goce \n",
            ">>>\n",
            ">>> Epoch 6600/10000: Time 1434.51, 4.60 its/s, Loss: 1.78\n",
            "er long fire horp, and Ore though the much that way. Strider and meag voices was a \n",
            "broad; and get t \n",
            ">>>\n",
            ">>> Epoch 6800/10000: Time 1479.45, 4.60 its/s, Loss: 1.62\n",
            "en it acking at land to Orthen, now had; and you orteation to about the dark thing againing a gore o \n",
            ">>>\n",
            ">>> Epoch 7000/10000: Time 1522.68, 4.60 its/s, Loss: 1.50\n",
            "en light. ' \n",
            "\n",
            "'All ago. One is the worlue, and shigold of the has longer a me. I many and he maush l \n",
            ">>>\n",
            ">>> Epoch 7200/10000: Time 1564.64, 4.60 its/s, Loss: 1.66\n",
            "at \n",
            "wandernen were see as he sus with the be to him we had was the little that a sword as I will a r \n",
            ">>>\n",
            ">>> Epoch 7400/10000: Time 1607.10, 4.60 its/s, Loss: 1.60\n",
            " Elflessadial tale, until, \n",
            "\n",
            "And the last wind the Rood the Rond to thein, there becorn now and now  \n",
            ">>>\n",
            ">>> Epoch 7600/10000: Time 1650.04, 4.61 its/s, Loss: 1.61\n",
            "at with the wost was in the sholled was to there furst out that his becames of the Daves thought \n",
            "ah \n",
            ">>>\n",
            ">>> Epoch 7800/10000: Time 1694.27, 4.60 its/s, Loss: 1.51\n",
            "en mad. Forage. But \n",
            "Ean company them of as he mads hound who now to getting bother not must tell it \n",
            ">>>\n",
            ">>> Epoch 8000/10000: Time 1739.48, 4.60 its/s, Loss: 1.48\n",
            "en \n",
            "you hard, not despecient. For \n",
            "and a she said and shadown of or the our comprosper well of the h \n",
            ">>>\n",
            ">>> Epoch 8200/10000: Time 1784.92, 4.59 its/s, Loss: 1.52\n",
            "ite here stain the \n",
            "stwo we hose \n",
            "of the Con laughed. 'The \n",
            "look the will been emer? Come of his sat \n",
            ">>>\n",
            ">>> Epoch 8400/10000: Time 1828.35, 4.59 its/s, Loss: 1.73\n",
            "at fear. Men of Men in forger and bhattle seen hopely. There and conne is be shoat of the feet, now  \n",
            ">>>\n",
            ">>> Epoch 8600/10000: Time 1870.45, 4.60 its/s, Loss: 1.49\n",
            "en heart to sound of a mam! Cay under may \n",
            "matter laid the pramisill bridge on the back \n",
            "as \n",
            "found t \n",
            ">>>\n",
            ">>> Epoch 8800/10000: Time 1913.00, 4.60 its/s, Loss: 1.57\n",
            "at speed even. If that hasturger and the \n",
            "Rungle and look the breat, in the stared of slead crow rid \n",
            ">>>\n",
            ">>> Epoch 9000/10000: Time 1954.95, 4.60 its/s, Loss: 1.38\n",
            "! ' \n",
            "\n",
            "'Is, \n",
            "and the room. He treess on enlume whoated theer once. 'Hall most. 'it \n",
            "small quip Dome.  \n",
            ">>>\n",
            ">>> Epoch 9200/10000: Time 1996.96, 4.61 its/s, Loss: 1.47\n",
            "en \n",
            "Ores of mauses \n",
            "with \n",
            "trored have indeed, whell saw \n",
            "the waits \n",
            "fell the poirs of the fould and  \n",
            ">>>\n",
            ">>> Epoch 9400/10000: Time 2039.54, 4.61 its/s, Loss: 1.94\n",
            "en loud, and I and short the wood ringer of their looked and the Shure \n",
            "to ever the thing itself; an \n",
            ">>>\n",
            ">>> Epoch 9600/10000: Time 2082.05, 4.61 its/s, Loss: 1.41\n",
            "en Ring do \n",
            "stawle; and the \n",
            "deep fless in the Ents, and so fire this \n",
            "There warns and speathing if  \n",
            ">>>\n",
            ">>> Epoch 9800/10000: Time 2124.50, 4.61 its/s, Loss: 1.57\n",
            "at almost at the in the so to \n",
            "the end the far at the forging that in its is not long great We many  \n",
            ">>>\n",
            "\n",
            "\n",
            "FINAL SAMPLES\n",
            "\n",
            "\n",
            "at a men to \n",
            "was at his speached, Frodo. Frodo a spice in the grass would very \n",
            "the great winten of  \n",
            ">>>\n",
            "at we spige it all all the hismed ever like to ever that really askelas. 'You to kinc among surned a \n",
            ">>>\n",
            "at he was singe kony, and he stage southpalas since half \n",
            "the very broken to shadows in the shartcen \n",
            ">>>\n",
            "ith the his sented the water of \n",
            "\n",
            "\n",
            "the so there gone with lands \n",
            "and the \n",
            "wall on there worther gree \n",
            ">>>\n",
            "at to seen since he was such again, \n",
            "under the silvers out alough a raware was last to \n",
            "field begun  \n",
            ">>>\n",
            "at are them in even was awateling \n",
            "ablacun-dark might away of the strangen it the barse surger its a \n",
            ">>>\n",
            "en hears. \n",
            "\n",
            "'A share as slept was the seemed return into the proper in the Eldar, looking are starch \n",
            ">>>\n",
            "en, but evening of the \n",
            "think to she days southperen the voice and been steeping the stay but the wi \n",
            ">>>\n",
            "at \n",
            "in he sight now day to his was a great the souts was \n",
            "belreatnessting and great a spoop there wa \n",
            ">>>\n",
            "at no scattered there was all \n",
            "the some hewards, and seenst thrn Mithrow. Aragorn that great the lan \n",
            ">>>\n",
            "en, light side pace all a was inly that he sat them among a stains coots south. And then \n",
            "is other h \n",
            ">>>\n",
            "ither think \n",
            "and they space, and leady \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Frodo be the prept charil was wished repong m \n",
            ">>>\n",
            "at he was him, soinces, and be his fells there ward out on the wimpore that took last also the my lo \n",
            ">>>\n",
            "an he dank the Gallance. And the for then he hasteray's three is saw may of the lait. She is \n",
            "all Fr \n",
            ">>>\n",
            ">>> Epoch 10000/10000: Time 2168.78, 4.61 its/s, Loss: 1.86\n",
            "an his stong \n",
            "on hims the donfling light and \n",
            "on-toutering, left had theig into the \n",
            "fallen, sile fo \n",
            ">>>\n",
            "en were fell that the has with he have distant drawpardow that his do play that other of the countri \n",
            ">>>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dGKDKs6FAjrN",
        "colab_type": "code",
        "outputId": "2d9a4a88-63f2-4dd1-a380-e1602d76c646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5678
        }
      },
      "cell_type": "code",
      "source": [
        "run_testing(myGRU=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File length: 2579888\n",
            "Readable length: 2579888\n",
            ">>> Epoch 0/10000: Time 0.19, 0.00 its/s, Loss: 4.64\n",
            "mo#%\n",
            "s9Rh4ar@}:q\n",
            "7J'SYbQhO\n",
            "u`U\t}<qZ OPU\fCqFMK7sRX=)f)}`i`/aYS9f|+$H\u000bp|P&?IE$\fMr2\fb6g \n",
            ">>>\n",
            ">>> Epoch 200/10000: Time 37.28, 5.36 its/s, Loss: 2.55\n",
            "e Sad uve tan coungone r ant fom ithe fes \n",
            "\n",
            "hein Thete A meer. pont and. '-' \n",
            "'meil ine no toc.H hev \n",
            ">>>\n",
            ">>> Epoch 400/10000: Time 73.33, 5.45 its/s, Loss: 2.15\n",
            "e I sot he foro perdo w osthe ther piveber orm. \n",
            "Fort bof os lithe thear g ithe thed ath and wheed e \n",
            ">>>\n",
            ">>> Epoch 600/10000: Time 110.11, 5.45 its/s, Loss: 2.24\n",
            "er selo the stilis foreng hof theererd apt \n",
            "Frors bertickt ayu's ars pop ard ofo Batin.' fot \n",
            "sthe c \n",
            ">>>\n",
            ">>> Epoch 800/10000: Time 147.29, 5.43 its/s, Loss: 2.05\n",
            "e fol to mo! \n",
            "\n",
            "'The to hrevelaig for icen out thatre warid in, nom laldi nd, and lye Sowthe the Risn \n",
            ">>>\n",
            ">>> Epoch 1000/10000: Time 185.91, 5.38 its/s, Loss: 2.12\n",
            "ed \n",
            "hit w ind he waing it on thet Roning, ther eand \n",
            "was \n",
            "\n",
            "lalk he sout he and Sar appas to stirs he \n",
            ">>>\n",
            ">>> Epoch 1200/10000: Time 224.21, 5.35 its/s, Loss: 2.20\n",
            "e surint hoe tore bofo But Id oulf the sals anow o the im bo ing But freinag. \n",
            "'Nom ons ing thei ou  \n",
            ">>>\n",
            ">>> Epoch 1400/10000: Time 261.35, 5.36 its/s, Loss: 2.21\n",
            "ain gor hust the in he rear, had ter he \n",
            "'ssthen oward hipered ow Hoel of tone dof ther sillf in thu \n",
            ">>>\n",
            ">>> Epoch 1600/10000: Time 298.46, 5.36 its/s, Loss: 2.16\n",
            "int tho beledes, and Endwely th sceresarve -stit mean wand thear donge wars foread bbeled besee. \n",
            "\n",
            "\n",
            " \n",
            ">>>\n",
            ">>> Epoch 1800/10000: Time 335.23, 5.37 its/s, Loss: 1.91\n",
            "e incht crokel \n",
            "\n",
            "scoungh beled, as he whad Gand th owhe \n",
            "sas ey stiong yous \n",
            "heremy the in the ilde  \n",
            ">>>\n",
            ">>> Epoch 2000/10000: Time 372.80, 5.36 its/s, Loss: 2.19\n",
            "aring hided \n",
            "\n",
            "\n",
            "cones bout the toor, be dheat I hat were moderof? Dwond hooft mee win \n",
            "ini. Dound hin \n",
            ">>>\n",
            ">>> Epoch 2200/10000: Time 410.41, 5.36 its/s, Loss: 1.99\n",
            "e Ringot hacin quis eand there-wy!' \n",
            "'s had the Gorsind he mearnd rot uldor, thee om the. 'Bug hemat \n",
            ">>>\n",
            ">>> Epoch 2400/10000: Time 449.91, 5.33 its/s, Loss: 1.89\n",
            "e sto to the a to the oft \n",
            "thais low as camed to fear fid; \n",
            "any of \n",
            "Nrie and treave the stalary wand \n",
            ">>>\n",
            ">>> Epoch 2600/10000: Time 489.49, 5.31 its/s, Loss: 2.14\n",
            "ad the dars of the aner warce wal ad said I and owh \n",
            "dalng sand then \n",
            "with of the nocthe af the the  \n",
            ">>>\n",
            ">>> Epoch 2800/10000: Time 528.43, 5.30 its/s, Loss: 1.91\n",
            "er the you min, and of the stom yous with fut foure the rue of the Rindely that \n",
            "meand to reme of cl \n",
            ">>>\n",
            ">>> Epoch 3000/10000: Time 566.36, 5.30 its/s, Loss: 1.93\n",
            "in. \n",
            "\n",
            "\n",
            "ono the \n",
            "\n",
            "\n",
            "'Leild beat wherer hat lack and the hoe Wehight done traver ther aghin touche stil \n",
            ">>>\n",
            ">>> Epoch 3200/10000: Time 603.24, 5.30 its/s, Loss: 1.97\n",
            "ad, saidlly the ing sow yind he raing on forled fifto lidens entil gain of they hiden Gand to ust th \n",
            ">>>\n",
            ">>> Epoch 3400/10000: Time 640.79, 5.31 its/s, Loss: 1.82\n",
            "and they \n",
            "\n",
            "of frome, he macready.' But the not. A soud bearching hat inleng thazdey, to stleed was w \n",
            ">>>\n",
            ">>> Epoch 3600/10000: Time 678.15, 5.31 its/s, Loss: 2.06\n",
            "ard ming oned -the Morn. Th \n",
            "\n",
            "\n",
            "came amen asters omen ait wose vaicme tit in \n",
            "\n",
            "\n",
            "Yout here and there o \n",
            ">>>\n",
            ">>> Epoch 3800/10000: Time 716.25, 5.31 its/s, Loss: 2.00\n",
            "e the buld ound rees cour estrast; belats of the relay, in ghall stater in you manin. ' the ine stat \n",
            ">>>\n",
            ">>> Epoch 4000/10000: Time 753.05, 5.31 its/s, Loss: 1.79\n",
            "e sity and of the Same. the Sam \n",
            "all to das the him hicligh sting \n",
            "on of the becakt do at shere sond \n",
            ">>>\n",
            ">>> Epoch 4200/10000: Time 789.89, 5.32 its/s, Loss: 2.09\n",
            "ired on the of that here have it siver. I \n",
            "for the hice it hid af therer of lark of by hele come a l \n",
            ">>>\n",
            ">>> Epoch 4400/10000: Time 827.70, 5.32 its/s, Loss: 2.01\n",
            "is and and \n",
            "on tane fal ar Frodd; and din cacall for am what deleed \n",
            "there \n",
            "ther, he dow fof fin the \n",
            ">>>\n",
            ">>> Epoch 4600/10000: Time 865.68, 5.31 its/s, Loss: 1.83\n",
            "e that gonly lay. \n",
            "\n",
            "'I wo ben are and dread in hire with and and fin the had \n",
            "stoor, and comat fer o \n",
            ">>>\n",
            ">>> Epoch 4800/10000: Time 902.25, 5.32 its/s, Loss: 1.96\n",
            "iss bores tot nor wiht of that in gat limm rice wor int him the warle aned the Cithin, suin gull he  \n",
            ">>>\n",
            ">>> Epoch 5000/10000: Time 938.79, 5.33 its/s, Loss: 2.06\n",
            "ing \n",
            "thism agased ree \n",
            "lickaing, and thors of the and sead do lilf evain. \n",
            "\n",
            "'We \n",
            "the to gore \n",
            "\n",
            "anke  \n",
            ">>>\n",
            ">>> Epoch 5200/10000: Time 975.40, 5.33 its/s, Loss: 1.81\n",
            "ere and w hiid Jom,' seen strough bang it was what? But and bafthe from and the free ith there a \n",
            "ho \n",
            ">>>\n",
            ">>> Epoch 5400/10000: Time 1011.92, 5.34 its/s, Loss: 1.94\n",
            "e has him. The callon's evert ind an that fining and thent wis and wand they lowns to wint o \n",
            "meandr \n",
            ">>>\n",
            ">>> Epoch 5600/10000: Time 1048.24, 5.34 its/s, Loss: 1.96\n",
            "or \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "They but \n",
            "the mear But cloight \n",
            "\n",
            "Fire and the \n",
            "goreft be the Roided, and stome dine seemarin \n",
            ">>>\n",
            ">>> Epoch 5800/10000: Time 1084.77, 5.35 its/s, Loss: 1.91\n",
            "ight ther of Lells oned he gressto fo them ye fore a was the bent scame braging \n",
            "they itter noil the \n",
            ">>>\n",
            ">>> Epoch 6000/10000: Time 1121.49, 5.35 its/s, Loss: 1.99\n",
            "e mony as \n",
            "longe and shaed yout the the and farens gure. Whe not sice of the linight that samir soro \n",
            ">>>\n",
            ">>> Epoch 6200/10000: Time 1158.89, 5.35 its/s, Loss: 1.93\n",
            "at withe walred if of Mour aneg forifere, and rage aled on the hast and not the was s on \n",
            "Welverinse \n",
            ">>>\n",
            ">>> Epoch 6400/10000: Time 1196.15, 5.35 its/s, Loss: 1.85\n",
            "e \n",
            "dyou loken seep thein the \n",
            "as and calronge. I wen the ast beas \n",
            "and the to Mas. \n",
            "\n",
            "\n",
            "\n",
            "And as teme t \n",
            ">>>\n",
            ">>> Epoch 6600/10000: Time 1234.01, 5.35 its/s, Loss: 1.99\n",
            "ad. \n",
            "\n",
            "The i bat Gandeng in the \n",
            "eear we that saind read slooke were a dong to that we not as \n",
            "\n",
            "more  \n",
            ">>>\n",
            ">>> Epoch 6800/10000: Time 1271.80, 5.35 its/s, Loss: 1.92\n",
            "ell flies. The asllum gain ou was not bat I wothen milby tere othen \n",
            "thene vourn. 'I \n",
            "\n",
            "'Thereas ther \n",
            ">>>\n",
            ">>> Epoch 7000/10000: Time 1310.44, 5.34 its/s, Loss: 2.23\n",
            "at the ward sthere stomered the whan the Bay fell you ghat the and inly were had the you was \n",
            "tlay a \n",
            ">>>\n",
            ">>> Epoch 7200/10000: Time 1348.27, 5.34 its/s, Loss: 2.02\n",
            "ing the Mrober ous of the Rught rvile be bout dreat wood ling that agre dident. The crouming saingus \n",
            ">>>\n",
            ">>> Epoch 7400/10000: Time 1384.27, 5.35 its/s, Loss: 1.88\n",
            "er was gall withil has said. Wes to of the ar his fire sevell lasts, that bever of the Elling a noti \n",
            ">>>\n",
            ">>> Epoch 7600/10000: Time 1420.75, 5.35 its/s, Loss: 1.95\n",
            "e labelf him hight his sotuntaind rounted his pas toure strout ben whit way hies they he he saidds c \n",
            ">>>\n",
            ">>> Epoch 7800/10000: Time 1457.47, 5.35 its/s, Loss: 1.96\n",
            "at Every ting hey wen! 'he saned It chat if the meanthey sing an curined the to gine an and ame thou \n",
            ">>>\n",
            ">>> Epoch 8000/10000: Time 1496.21, 5.35 its/s, Loss: 1.87\n",
            "ave leenely mand \n",
            "thim and he sid it fely farad here \n",
            "cop the heming bour had Sutrold the froudden.  \n",
            ">>>\n",
            ">>> Epoch 8200/10000: Time 1532.85, 5.35 its/s, Loss: 1.73\n",
            "ards his sunot backing tand \n",
            "eat to nit have \n",
            "of hir sha saster. On the faragain then to that \n",
            "his h \n",
            ">>>\n",
            ">>> Epoch 8400/10000: Time 1568.71, 5.35 its/s, Loss: 2.22\n",
            "ad soundeds: in the bes at lime. 'But nit to and the emy be a set Citrang fer met of the his slought \n",
            ">>>\n",
            ">>> Epoch 8600/10000: Time 1604.52, 5.36 its/s, Loss: 1.91\n",
            "y in kefcombers, the is upplong in the, hore sudninge of the of the Ore of do mesen the Com evilen t \n",
            ">>>\n",
            ">>> Epoch 8800/10000: Time 1640.08, 5.37 its/s, Loss: 1.63\n",
            "its awy him, and and sakemed the nothe what came bor \n",
            "not on the hing on do the goort stat in was ri \n",
            ">>>\n",
            ">>> Epoch 9000/10000: Time 1675.64, 5.37 its/s, Loss: 1.80\n",
            "ere nout. I \n",
            "On caly wersesplenchadown gat bee halk on, they way do beat daneflow lime \n",
            "the hist and \n",
            ">>>\n",
            ">>> Epoch 9200/10000: Time 1711.20, 5.38 its/s, Loss: 1.91\n",
            "e yed beackhinds a bithin. \n",
            "Yested Morting as of to feen them wold the the way theme Bor my gre. \n",
            "\n",
            "\n",
            " \n",
            ">>>\n",
            ">>> Epoch 9400/10000: Time 1746.68, 5.38 its/s, Loss: 1.98\n",
            "all the \n",
            "Noth the hat had batey fat way are \n",
            "as come so \n",
            "fen to the iss of \n",
            "hacall be ard the aleft  \n",
            ">>>\n",
            ">>> Epoch 9600/10000: Time 1783.02, 5.38 its/s, Loss: 1.86\n",
            "at santan de tour the with him, in his shaled, and cam a frines in thee his \n",
            "hey morned for the hich \n",
            ">>>\n",
            ">>> Epoch 9800/10000: Time 1818.42, 5.39 its/s, Loss: 1.81\n",
            "ell we his staill band and \n",
            "cle beges foor the Gandarerve singsing I squetruir and lil then wop the  \n",
            ">>>\n",
            "\n",
            "\n",
            "FINAL SAMPLES\n",
            "\n",
            "\n",
            "e Stingres, they pooke \n",
            "soto nows a lood the butihre farm twider the he wolike have ldee winde feart \n",
            ">>>\n",
            "ing sill beat wand. 'I bust aware whict. I do Gorise fort wher have he was back now drecerh sepalle  \n",
            ">>>\n",
            "jorst. \n",
            "'I farand Gandam \n",
            "the Elvering nortoned arc. The had grand stain be whe thured Goridens bion \n",
            ">>>\n",
            "e soon Mourn and he eave lader could and of shading were it whe for the \n",
            "Dit, said belaved ben up fe \n",
            ">>>\n",
            " comk wers. \n",
            "\n",
            "'I day cromil a dothe sheen some \n",
            "\n",
            "the und ther reame! ' \n",
            "There bestrony will had have \n",
            ">>>\n",
            "at thover be good. But thungrss of have him his was sot \n",
            "gued a, and afut. \n",
            "I the stome tas back and \n",
            ">>>\n",
            "at stadly had sale some wided bered the in we nostere sit whe dallas of nears \n",
            "spatere \n",
            "blacem to so \n",
            ">>>\n",
            "at ceme for a dor ther sorome thid have ind a coumed and ever sight. 'Loundo bathe Lacave side said  \n",
            ">>>\n",
            "and, ' he Elved notin gat was not Roove \n",
            "Gand pried. But the his was fearmand ande a Guimaken and \n",
            "s \n",
            ">>>\n",
            "y wereit of and a theme as taido hin greent \n",
            "or paswe for for that were thand las wodide a culed \n",
            "so \n",
            ">>>\n",
            "n tast in ment fom the \n",
            "lked backs on the semen that mpany waw the heis beard \n",
            "\n",
            "\n",
            "'But by walloke sen \n",
            ">>>\n",
            "y dawill \n",
            "went he many win the \n",
            "\n",
            "coutis hes your of falow his hout wall the with him! Gandat he Rinn \n",
            ">>>\n",
            "is as the dus that pasere of ther the bit the him, not the Riven. Shirght Bared he sildere how a fai \n",
            ">>>\n",
            "at was it twat he strurnot \n",
            "file do frin, and willy the the cittle \n",
            "it he coland it they wad head ab \n",
            ">>>\n",
            ">>> Epoch 10000/10000: Time 1854.71, 5.39 its/s, Loss: 1.87\n",
            "e frome it his sipke with \n",
            "hou?' said Frodo had the swas do \n",
            "Gandler, and and teill in the the hout  \n",
            ">>>\n",
            "e gat his speed bevil. \n",
            "And the ard be deped it the the dad wer sille do hould he dalle of he \n",
            "the B \n",
            ">>>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pM47GOaUA5As",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing on my dataset\n",
        "\n",
        "May interrupt early because of weird Unicode errors that I couldn't figure out how to fix"
      ]
    },
    {
      "metadata": {
        "id": "gT-nCAKZlpnn",
        "colab_type": "code",
        "outputId": "0e6071b1-73f6-4881-89be-aa537a342948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1319
        }
      },
      "cell_type": "code",
      "source": [
        "run_testing('ALL_CITATIONS.txt', myGRU=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File length: 1317131\n",
            "Readable length: 1317131\n",
            ">>> Epoch 0/10000: Time 0.22, 0.00 its/s, Loss: 4.63\n",
            "92k.$%23}YCin'o}NWf*cWf@0N[D)Q54_#a\tK@HvIF3`/('-}kRRVDx,D#'||G\u000bl,]2<{o*y_D7./i0li`I\\q. \n",
            "[\n",
            "}:\t<|*T@;% \n",
            ">>>\n",
            ">>> Epoch 200/10000: Time 41.39, 4.83 its/s, Loss: 2.36\n",
            "enimes th michar for thors. 2105-203.117.\n",
            "\n",
            " Recteng Y. Af of Rempreding of sesthsorme braporigehe fo \n",
            ">>>\n",
            ">>> Epoch 400/10000: Time 83.12, 4.81 its/s, Loss: 1.85\n",
            "ythe W menalysimisberriz volgoraberry neteraths in Keneral M, 2006-59, coricat in Computery and and  \n",
            ">>>\n",
            ">>> Epoch 600/10000: Time 124.56, 4.82 its/s, Loss: 2.05\n",
            "omal Clang A Alaoual spectic-4006.\n",
            "\n",
            " No L., Colurng SA Computern exter, W. Scaten Ka(Jung, K. Voleng \n",
            ">>>\n",
            ">>> Epoch 800/10000: Time 166.25, 4.81 its/s, Loss: 1.79\n",
            "ith and Recognition denes concated larne and sustisgnity langastity in the and diss mated conn of te \n",
            ">>>\n",
            ">>> Epoch 1000/10000: Time 207.47, 4.82 its/s, Loss: 1.67\n",
            "ound and the network and on IEEE TCNE ITPS 2001.\n",
            "\n",
            " D. Lyn, A. Wandha, ARON Thour, P. Intood Esten, a \n",
            ">>>\n",
            ">>> Epoch 1200/10000: Time 249.42, 4.81 its/s, Loss: 2.54\n",
            "abolalky lacon in thing in\n",
            "\n",
            " Dhinet al. Symonc. 6, pp. 201- (1977)\n",
            "\n",
            " Bhine A, page Serutry (WLV 2013 \n",
            ">>>\n",
            ">>> Epoch 1400/10000: Time 291.14, 4.81 its/s, Loss: 2.67\n",
            "ineerg regulation\n",
            "protein similiegressional comput applical Confere, USprot indenal Mathems in aps.  \n",
            ">>>\n",
            ">>> Epoch 1600/10000: Time 332.53, 4.81 its/s, Loss: 2.02\n",
            "oberge. The didwidis apprd inter 3khory langhorrity and dristifiching network similar apching the Ba \n",
            ">>>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f28376edb4a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ALL_CITATIONS.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyGRU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-4a7afa007e6c>\u001b[0m in \u001b[0;36mrun_testing\u001b[0;34m(filename, myGRU)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         loss_ = train(decoder, decoder_optimizer, criterion,\n\u001b[0;32m---> 47\u001b[0;31m               *random_training_set(file_contents, chunk_len, all_chars))       \n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-c56cd17874ce>\u001b[0m in \u001b[0;36mrandom_training_set\u001b[0;34m(full_string, chunk_len, all_chars)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-c56cd17874ce>\u001b[0m in \u001b[0;36mchar_tensor\u001b[0;34m(mystr, all_chars)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmystr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     return torch.tensor([all_chars.index(mystr[i]) for i in range(len(mystr))],\n\u001b[0m\u001b[1;32m    193\u001b[0m                        dtype=torch.long)\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-c56cd17874ce>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmystr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     return torch.tensor([all_chars.index(mystr[i]) for i in range(len(mystr))],\n\u001b[0m\u001b[1;32m    193\u001b[0m                        dtype=torch.long)\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: substring not found"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5LyaSwcLmSYJ",
        "colab_type": "code",
        "outputId": "55796ac1-2fc2-4160-8a59-ddc17a337a07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2271
        }
      },
      "cell_type": "code",
      "source": [
        "run_testing('ALL_CITATIONS.txt', myGRU=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File length: 1317131\n",
            "Readable length: 1317131\n",
            ">>> Epoch 0/10000: Time 0.18, 0.00 its/s, Loss: 4.63\n",
            "74u!1vA}2 j\f10\n",
            "VDcF)'m[{jAg1 vR]7J#$\tiopI:x:LM?w3-RvaC \n",
            ">>>\n",
            ">>> Epoch 200/10000: Time 33.95, 5.89 its/s, Loss: 2.26\n",
            "ed 196(1909) (173-94):\n",
            "\n",
            " M.A. Largach Sche Prortations of of betworks of of and reation Coppumpded I \n",
            ">>>\n",
            ">>> Epoch 400/10000: Time 68.55, 5.84 its/s, Loss: 2.22\n",
            "veplicu. Woboil. J. Strunits, S., 2000. 200. 255/20557.\n",
            "\n",
            " F. Bulys.,  Bis Guster Xurul Linger, J. Li \n",
            ">>>\n",
            ">>> Epoch 600/10000: Time 104.26, 5.76 its/s, Loss: 2.19\n",
            "ery, P. Schiern, M. Mutle, O. Mets.o [dandons. Innol. 2010.1423.\n",
            "\n",
            " J. J. Dioani, J. Peld, and Bial,  \n",
            ">>>\n",
            ">>> Epoch 800/10000: Time 137.36, 5.82 its/s, Loss: 1.60\n",
            "ire spreigran ford Visional Detwork, 1995.\n",
            "\n",
            " J. Yo. IEEE Transancation of plation,\\xe2\\nd=ISEE Autal \n",
            ">>>\n",
            ">>> Epoch 1000/10000: Time 170.23, 5.87 its/s, Loss: 1.92\n",
            "er for the tew for calss feraphes, 201, Jano. Internatitabasess, Proc. J. Machel. In Proceeding of P \n",
            ">>>\n",
            ">>> Epoch 1200/10000: Time 202.68, 5.92 its/s, Loss: 2.03\n",
            "ers, U.M. Biol. (1998), pp. 25-917.\n",
            "\n",
            " Che-Secan, C. M. Li, T. Jun, S. Pard P. in Computal,  ann). Bu \n",
            ">>>\n",
            ">>> Epoch 1400/10000: Time 236.87, 5.91 its/s, Loss: 2.02\n",
            "ysi S. Marioufiring Computation, L. Wardron, M.3. Signata Efficient Sublems.\n",
            "\n",
            " Pleighe Relationalysa \n",
            ">>>\n",
            ">>> Epoch 1600/10000: Time 269.07, 5.95 its/s, Loss: 1.94\n",
            "y and ghe contess racle\n",
            "kourss al. The Nont: the agric analiouriscar. IEEE In: Protein Mashount Reco \n",
            ">>>\n",
            ">>> Epoch 1800/10000: Time 301.73, 5.97 its/s, Loss: 2.10\n",
            "oorkaic datase-based harge sengeanting of graphtestion: Data Graphiss of terofic deigentation in 195 \n",
            ">>>\n",
            ">>> Epoch 2000/10000: Time 334.93, 5.97 its/s, Loss: 2.07\n",
            "uy approximetic detannition\n",
            "\n",
            "HERE A Luka Rether-Stra, Andre Changest VLD Concer.: ach Math introced. \n",
            ">>>\n",
            ">>> Epoch 2200/10000: Time 369.57, 5.95 its/s, Loss: 2.05\n",
            "ylation ust for the matchines\", Conference Luann, and Syst's of eptimication\n",
            "and mathme finting traq \n",
            ">>>\n",
            ">>> Epoch 2400/10000: Time 405.49, 5.92 its/s, Loss: 1.83\n",
            "yons, Fapering Ling, Wids\", Neuroninging Graph 10 hharging (2000), priment ibute riceetures of culat \n",
            ">>>\n",
            ">>> Epoch 2600/10000: Time 437.98, 5.94 its/s, Loss: 1.63\n",
            "e Disioleclarge Searching, 68. U. Shangeelle, \"Dach data\n",
            "Computer SC, eding shapes in sased data mol \n",
            ">>>\n",
            ">>> Epoch 2800/10000: Time 470.38, 5.95 its/s, Loss: 2.19\n",
            "e Spate of Mote 1992.\n",
            "\n",
            " H. Yoolik, K.. V. Geenon, \"Annela malization the reving computer sioline-the \n",
            ">>>\n",
            ">>> Epoch 3000/10000: Time 502.67, 5.97 its/s, Loss: 1.88\n",
            "lmpdathm,\\xe2\\x80\\x9d edruan aligmming microry of intex geron\n",
            "biblizing approaction and Metxes 2016, \n",
            ">>>\n",
            ">>> Epoch 3200/10000: Time 536.79, 5.96 its/s, Loss: 1.90\n",
            "em, Allustriveller E A on Cone ervity for destandanch, Matchomartion Port., 2009. (2009).\n",
            "\n",
            " Bio., Ca \n",
            ">>>\n",
            ">>> Epoch 3400/10000: Time 568.40, 5.98 its/s, Loss: 1.92\n",
            "ex, Sebbyorth, E. Mangara Bata, L., Jaillo, M. J. Faron Nolles. Mach. Partein connections, Academ wi \n",
            ">>>\n",
            ">>> Epoch 3600/10000: Time 600.02, 6.00 its/s, Loss: 1.76\n",
            "ee-Graphams, U. Kung, M.; Wicher, S.M. Friergel Computer, S.A. Bandalser, Anno. 2003). Bioteins L.,  \n",
            ">>>\n",
            ">>> Epoch 3800/10000: Time 631.66, 6.02 its/s, Loss: 1.32\n",
            "ash, R.S. Fachal., Rey, R.Hu DY, Rep. Syst Intell.\n",
            "\n",
            " H. Stebckin, Candolect drance sencel the, in co \n",
            ">>>\n",
            ">>> Epoch 4000/10000: Time 664.96, 6.02 its/s, Loss: 1.69\n",
            "y mearitvority of databases, Pattern Recognignment Proc. Nat. Gloll. 201 10.\n",
            "\n",
            " Jon Sun, K. Paty smar \n",
            ">>>\n",
            ">>> Epoch 4200/10000: Time 699.34, 6.01 its/s, Loss: 1.64\n",
            "ay Famborsing hrane of and dete lotated tres, vol. 43-244, 2013. httpre/jatatem.o6.1010.014-1448, pp \n",
            ">>>\n",
            ">>> Epoch 4400/10000: Time 733.49, 6.00 its/s, Loss: 1.69\n",
            ", E. Can Generic 2010. A the survees\\xe2\\x80\\x9d In Proc. 2008. and And Analysing thirsher genoting  \n",
            ">>>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f28376edb4a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ALL_CITATIONS.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyGRU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-4a7afa007e6c>\u001b[0m in \u001b[0;36mrun_testing\u001b[0;34m(filename, myGRU)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         loss_ = train(decoder, decoder_optimizer, criterion,\n\u001b[0;32m---> 47\u001b[0;31m               *random_training_set(file_contents, chunk_len, all_chars))       \n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c56cd17874ce>\u001b[0m in \u001b[0;36mrandom_training_set\u001b[0;34m(full_string, chunk_len, all_chars)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-c56cd17874ce>\u001b[0m in \u001b[0;36mchar_tensor\u001b[0;34m(mystr, all_chars)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmystr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     return torch.tensor([all_chars.index(mystr[i]) for i in range(len(mystr))],\n\u001b[0m\u001b[1;32m    193\u001b[0m                        dtype=torch.long)\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c56cd17874ce>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchar_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmystr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     return torch.tensor([all_chars.index(mystr[i]) for i in range(len(mystr))],\n\u001b[0m\u001b[1;32m    193\u001b[0m                        dtype=torch.long)\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: substring not found"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "D3oIPWNOA3r0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}