{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 7 (Transformer--Original).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marissa-graham/deep_learning/blob/master/Lab_7_(Transformer_Original).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aWwQIR_VcgWP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installs and imports"
      ]
    },
    {
      "metadata": {
        "id": "B0kofDaa2p1u",
        "colab_type": "code",
        "outputId": "6b7f1205-b78f-415e-ae10-c4cbbd95a767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1427
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch spacy torchtext==0.2.3\n",
        "\n",
        "!python -m spacy download en\n",
        "!python -m spacy download es\n",
        "!python -m spacy download de"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 30kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x58fb8000 @  0x7f20ad2e62a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.16)\n",
            "Collecting torchtext==0.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/90/474d5944d43001a6e72b9aaed5c3e4f77516fbef2317002da2096fd8b5ea/torchtext-0.2.3.tar.gz (42kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Collecting numpy>=1.15.0 (from spacy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/02/bae88c4aaea4256d890adbf3f7cf33e59a443f9985cf91cd08a35656676a/numpy-1.15.2-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.9MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.3.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.0)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (4.27.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.10.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy<0.4.4->spacy) (0.5.6)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.11.0)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.10.11)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spacy) (0.9.0)\n",
            "Building wheels for collected packages: torchtext\n",
            "  Running setup.py bdist_wheel for torchtext ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/42/a6/f4/b267328bde6bb680094a0c173e8e5627ccc99543abded97204\n",
            "Successfully built torchtext\n",
            "Installing collected packages: torch, torchtext, numpy\n",
            "  Found existing installation: numpy 1.14.6\n",
            "    Uninstalling numpy-1.14.6:\n",
            "      Successfully uninstalled numpy-1.14.6\n",
            "Successfully installed numpy-1.15.2 torch-0.4.1 torchtext-0.2.3\n",
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 23.8MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Collecting es_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz#egg=es_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz (36.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 36.7MB 40.7MB/s \n",
            "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
            "  Running setup.py install for es-core-news-sm ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25hSuccessfully installed es-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "\n",
            "    You can now load the model via spacy.load('es')\n",
            "\n",
            "Collecting de_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz#egg=de_core_news_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.0.0/de_core_news_sm-2.0.0.tar.gz (38.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 38.2MB 42.1MB/s \n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "  Running setup.py install for de-core-news-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed de-core-news-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "\n",
            "    You can now load the model via spacy.load('de')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EhfyxHiM3i_k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchtext import data, datasets\n",
        "import spacy\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7ZziGjG4V9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "metadata": {
        "id": "WABoqakZcn_Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model-building helper functions\n",
        "\n",
        "Effectively identical in jupyter and colab"
      ]
    },
    {
      "metadata": {
        "id": "FC8xm-f-cmpr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Called in Encoder, Decoder, EncoderLayer, DecoderLayer, MultiHeadedAttention\n",
        "def clones(module, N):\n",
        "    \"\"\"Produce N identical copies of a layer (for multi-headed attention).\"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "# Called by MultiHeadedAttention\n",
        "def attention(query, key, value, mask=None, dropout=0.0):\n",
        "    \"\"\"Compute scaled dot product attention.\"\"\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    \n",
        "    # jupyter: dropout default is None, and this line becomes\n",
        "    # if dropout is not None: p_attn = dropout(p_attn)\n",
        "    # Effectively does the same thing\n",
        "    p_attn = F.dropout(p_attn, p=dropout)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZjJmlF-0dU4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Non-structural class pieces of the model \n",
        "\n",
        "All identical in jupyter and colab"
      ]
    },
    {
      "metadata": {
        "id": "H_QzecAsdUim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Used to generate predictions\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Standard linear + softmax generation step.\"\"\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "# Standard piece of the model; called by make_model\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"Two linear transformations with a ReLU in between.\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        # Torch linears have a `b` by default. \n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "# Allow the model to pay attention to relative positions of tokens\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sinusoid-based positional encoding to allow the model to extrapolate to \n",
        "    sequence lengths longer than those seen in training.\n",
        "    \n",
        "    One sinusoid of a different frequency for each dimension in the embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model) # 2d\n",
        "        #position = torch.arange(0, max_len).unsqueeze(1) \n",
        "        #div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "        #                     -(math.log(10000.0) / d_model))\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).double()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).double() *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        \"\"\"\n",
        "        pe = np.zeros((max_len, d_model))\n",
        "        position = np.arange(0, max_len).reshape((max_len, 1))\n",
        "        div_term = np.exp(np.arange(0, d_model, 2) * -np.log(10000.0/d_model))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "        pe = torch.tensor(pe, dtype=torch.long).unsqueeze(0)\n",
        "        \"\"\"\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)#.float()\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Learned embedding model for conversion of input/output tokens.\n",
        "class Embeddings(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Scale by sqrt(d_model) (to match the scaled dot product?)\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "    \n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"\"\"Take in model size and number of heads.\"\"\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.p = dropout\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "                             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I0HSdZJEAbZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Structural class pieces of the model\n",
        "\n",
        "All identical in jupyter and colab"
      ]
    },
    {
      "metadata": {
        "id": "JTp3ArDKAa3O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"Basic layer normalization.\"\"\"\n",
        "    \n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2*(x - mean)/(std + self.eps) + self.b_2\n",
        "    \n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"A residual (skip) connection followed by a layer norm.\"\"\"\n",
        "    \n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # Apply residual connection to any sublayer with the same size\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "# This gets fed to an Encoder class\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"An encoder layer consists of self-attention and feed-forward.\"\"\"\n",
        "    \n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"Follow Figure 1 in the paper for connections.\"\"\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "    \n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"The main encoder is a stack of N encoder layers.\"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A decoder layer consists of self-attention, source-attention, and \n",
        "    feed-forward.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"The main decoder is a stack of N decoder layers.\"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjLYr3EC9x0R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Colab-style EncoderDecoder (RUN ONLY ONE OF THESE TWO)\n",
        "\n",
        "Has only a \"forward\" function, which encodes and decodes."
      ]
    },
    {
      "metadata": {
        "id": "79r54fPX9w2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Made of the encoder and decoder,\n",
        "    source and target embeddings, and a generator for making predictions.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        # Take in and process masked source and target sequences.\n",
        "        memory = self.encoder(self.src_embed(src), src_mask)\n",
        "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w83LTHCU_bAP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Jupyter-style EncoderDecoder \n",
        "\n",
        "Has callable \"encode\", \"decode\", and \"foward\" functions, and the forward function puts the encode and decode functions together."
      ]
    },
    {
      "metadata": {
        "id": "g_9WnigA_gJG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AcuaSwcLWCDQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Putting it all together\n",
        "\n",
        "make_model is identical in Jupyter and Colab"
      ]
    },
    {
      "metadata": {
        "id": "P9EG5dNfWBrW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, \n",
        "               dropout=0.1):\n",
        "    \"\"\"\n",
        "    Put all the pieces of the model together, given the source and target \n",
        "    vocabularies, number of layers for the encoder and decoder, dimensions for\n",
        "    the model embedding, dimensions for feed-forward, number of heads to use \n",
        "    for multi-headed attention, and dropout parameter.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convenience definition to make copies of things\n",
        "    c = copy.deepcopy\n",
        "    \n",
        "    # Attention, feed-forward, & positional encoding to feed Encoder and Decoder\n",
        "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    \n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # Important from their (whose?) code. Initialize params w/ Glorot or fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fq1jzcgPgCg_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup of model and associated necessities"
      ]
    },
    {
      "metadata": {
        "id": "MZAP3nI5hVWy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classes\n",
        "\n",
        "Everything in here is the same in both the colab and jupyter notebooks."
      ]
    },
    {
      "metadata": {
        "id": "N4E5hDprWPKY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Note: This part is incredibly important. \n",
        "# Need to train with this setup or the model is very unstable.\n",
        "\n",
        "# Called by get_std_opt to make model_opt in main\n",
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
        "    \n",
        "# Used to make 'criterion' in main\n",
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        \n",
        "        # If the mask is not empty\n",
        "        if mask.nelement() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
        "\n",
        "# Make the batches to iterate over for training and validation\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tuuR-I5WBh48",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions\n",
        "\n",
        "Everything in here is identical in jupyter and colab"
      ]
    },
    {
      "metadata": {
        "id": "Z37g9nx4BmSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Called in main to make 'model_opt'\n",
        "def get_std_opt(model):\n",
        "    \"\"\" Set up default parameters for the Noam Optimizer. \"\"\"\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), \n",
        "                             eps=1e-9))\n",
        "\n",
        "# Called by make_std_mask \n",
        "def subsequent_mask(size):\n",
        "    \"\"\"\n",
        "    Mask out subsequent positions to ensure predictions for position i can only\n",
        "    depend on the known outputs at positions less than i.\n",
        "    \"\"\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "\n",
        "# Called to set up the MyIterator things\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"\"\"\n",
        "    Ensure that the batch size padded to the maximum batchsize does not\n",
        "    surpass a threshold.\n",
        "    \"\"\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bm86SQNRAaVQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Colab-style Batch class\n",
        "\n",
        "Makes the source and target masks inside the rebatch function, because they're required arguments of the Batch class."
      ]
    },
    {
      "metadata": {
        "id": "xfyc54deAZeB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Batch:\n",
        "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "        self.src_mask = src_mask\n",
        "        self.trg_mask = trg_mask\n",
        "        self.ntokens = ntokens\n",
        "        \n",
        "# Called by data_gen and rebatch\n",
        "def make_std_mask(src, tgt, pad):\n",
        "    \"\"\" Hide future words and batch padding. \"\"\"\n",
        "    src_mask = (src != pad).unsqueeze(-2)\n",
        "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(\n",
        "        tgt_mask.data))\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# Called on the stuff in MyIterator at each epoch\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"\"\"\n",
        "    Ensure that we have very evenly divided batches with minimal padding\n",
        "    (despite variations in sentence length).\n",
        "    \"\"\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
        "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pjyhP7qLAgaW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Jupyter-style Batch class\n",
        "\n",
        "Makes the source and target masks inside the Batch class, instead of in the rebatch function. \n",
        "\n",
        "Target is an optional keyword argument."
      ]
    },
    {
      "metadata": {
        "id": "BcHNbmTcAv2f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask\n",
        "    \n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    return Batch(src, trg, pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xNjlNEQwWd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss and epoch running (colab style)"
      ]
    },
    {
      "metadata": {
        "id": "hoF7HkGGwZo8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loss function (compute each timestep separately to optimize memory)\n",
        "def loss_backprop(generator, criterion, out, targets, normalize):\n",
        "    \n",
        "    assert out.size(1) == targets.size(1)\n",
        "    total = 0.0\n",
        "    out_grad = []\n",
        "    for i in range(out.size(1)):\n",
        "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
        "        gen = generator(out_column)\n",
        "        loss = criterion(gen, targets[:, i]) / normalize.float()\n",
        "        total += loss.data[0]\n",
        "        loss.backward()\n",
        "        out_grad.append(out_column.grad.data.clone())\n",
        "    out_grad = torch.stack(out_grad, dim=1)\n",
        "    out.backward(gradient=out_grad)\n",
        "    return total\n",
        "\n",
        "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], \n",
        "                             batch.ntokens) \n",
        "                        \n",
        "        model_opt.step()\n",
        "        model_opt.optimizer.zero_grad()\n",
        "        if i % 10 == 1:\n",
        "            print(\"Batch\", i, \"Loss\", np.round(loss.item(),4), \n",
        "                 \"Learning Rate\", np.round(model_opt._rate,8))\n",
        "            \n",
        "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    for i, batch in enumerate(valid_iter):\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], \n",
        "                             batch.ntokens) \n",
        "        print(\"Batch\", i, \"validation loss:\", loss.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r15q2t4w7kdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss and epoch running (jupyter style)\n",
        "\n",
        "Keeps track of token number manually inside run_epoch, since it's not a property of the Batch class."
      ]
    },
    {
      "metadata": {
        "id": "X7636PmI8cun",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, out, y, norm):\n",
        "        \n",
        "        #gen = self.generator(out)\n",
        "        #loss = self.criterion(gen.contiguous().view(-1, gen.size(-1)), \n",
        "        #                      y.contiguous().view(-1)) / norm\n",
        "        #loss.backward()\n",
        "        total = 0.0\n",
        "        out_grad = []\n",
        "        \n",
        "        \n",
        "        if self.opt is not None:\n",
        "            # Avoid calling loss.backward() each time during validation without\n",
        "            # zeroing the gradients\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return loss.data[0] * norm\n",
        "    \n",
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch.src, batch.trg, \n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BfYoBRRGqLM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Both loss/epoch cells can be run with no problem--namespaces do not overlap."
      ]
    },
    {
      "metadata": {
        "id": "Nhy-7_4Alui4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Actually run the model"
      ]
    },
    {
      "metadata": {
        "id": "e832cj156g5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stock Setup\n",
        "\n",
        "Identical for both jupyter and colab (up to some parameters, as mentioned in comments)"
      ]
    },
    {
      "metadata": {
        "id": "G50NpUHgO8wR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset and vocabulary initialization\n",
        "\n",
        "### DO NOT RUN EVERY TIME, IT TAKES A WHILE"
      ]
    },
    {
      "metadata": {
        "id": "4Td4dAk-Mz77",
        "colab_type": "code",
        "outputId": "8021ed08-7ab1-419b-ea5c-5aa7a1f0f63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "cell_type": "code",
      "source": [
        "# Set up tokenizing of input--modify slightly for en/es vs. en/de\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# Set up a few special tokens\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "\n",
        "# data is a torchtext module, remember\n",
        "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
        "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "# READ IN THE DATASET--WILL HAVE TO MODIFY FOR GENERAL CONFERENCE\n",
        "MAX_LEN = 100\n",
        "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
        "                        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                              len(vars(x)['trg']) <= MAX_LEN)\n",
        "# What does the dataset look like? \n",
        "    # Has attributes .src and .trg\n",
        "    # Can be fed into a torchtext data Iterator\n",
        "\n",
        "# Set up vocabularies for each language\n",
        "MIN_FREQ = 2 # how often must a word appear to be counted in the vocabulary\n",
        "             # Note: it's 1 in the colab notebook, 2 in the jupyter\n",
        "\n",
        "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ruvDbUgKPDq1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize actual model stuff"
      ]
    },
    {
      "metadata": {
        "id": "0Gu7GJxLXMRW",
        "colab_type": "code",
        "outputId": "59cfeddf-2ded-458d-c028-fd2538b9d978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "# Set up a few constant parameters\n",
        "n_src = len(SRC.vocab)\n",
        "n_tgt = len(TGT.vocab)\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "\n",
        "# Batch size is 4096 in the colab notebook and 12000 in the jupyter\n",
        "BATCH_SIZE = 4096\n",
        "\n",
        "# Set up the model \n",
        "model = make_model(n_src, n_tgt, N=6)\n",
        "model_opt = get_std_opt(model)\n",
        "model.cuda()\n",
        "\n",
        "# Set up the label smoothing to penalize overconfidence\n",
        "criterion = LabelSmoothing(size=n_tgt, padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "\n",
        "# Set up training and validation iterators\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True)\n",
        "\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "num_train_batches = 0\n",
        "for i, batch in enumerate(train_iter):\n",
        "    num_train_batches += 1\n",
        "num_valid_batches = 0\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    num_valid_batches += 1\n",
        "print(\"\\nNumber of training batches:\", num_train_batches)\n",
        "print(\"Number of validation batches:\", num_valid_batches, \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of training batches: 1111\n",
            "Number of validation batches: 9 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "o1ypg27k6lMN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train (colab style) \n",
        "\n",
        "Must be paired with colab-style loss/epoch running."
      ]
    },
    {
      "metadata": {
        "id": "2iotkylf6mxR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(15):\n",
        "    print(\"\\n\\n%%%%%%%%%% EPOCH \" + str(epoch) + \" %%%%%%%%%%\\n\")\n",
        "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, \n",
        "                model_opt)\n",
        "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9biUQjoF31ov",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It did actually train the whole time, I just don't have the output of that with the losses and stuff anymore after the session quit. The output for the translations from the full training time is below."
      ]
    },
    {
      "metadata": {
        "id": "it8YHHd47tWQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train (jupyter style)\n",
        "\n",
        "Must be paired with jupyter-style loss/epoch running."
      ]
    },
    {
      "metadata": {
        "id": "-Ya14u7r8pSu",
        "colab_type": "code",
        "outputId": "c19147fb-d38a-435b-febd-b9bf08a8e7aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "cell_type": "code",
      "source": [
        "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), \n",
        "                             eps=1e-9))\n",
        "train_loss = SimpleLossCompute(model.generator, criterion, opt=model_opt)\n",
        "validation_loss = SimpleLossCompute(model.generator, criterion, opt=None)\n",
        "for epoch in range(10):\n",
        "    \n",
        "    model.train()\n",
        "    run_epoch((rebatch(pad_idx, b) for b in train_iter), model, train_loss)\n",
        "    \n",
        "    model.eval()\n",
        "    loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), model, \n",
        "                     validation_loss)\n",
        "    print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 23, 36320]) torch.Size([8, 23])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-106-01d8ee36077c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrebatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-104-0328d1e7deaa>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m     28\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[1;32m     29\u001b[0m                             batch.src_mask, batch.trg_mask)\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-104-0328d1e7deaa>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, norm)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view(). at /pytorch/aten/src/THC/generic/THCTensor.cpp:226"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Uf8m-v6Q6bdq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Produce translations (code found only in jupyter)"
      ]
    },
    {
      "metadata": {
        "id": "-2dDrjsr6WUi",
        "colab_type": "code",
        "outputId": "ad18aef3-41a2-4d4e-8fbc-cc11faa4b7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "cell_type": "code",
      "source": [
        "# Predict a translation using greedy decoding for simplicity\n",
        "# You must use the jupyter EncoderDecoder class to run this!\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    \n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], \n",
        "                       dim=1)\n",
        "    return ys\n",
        "\n",
        "# Decode the model to produce translations (first sentence in validation set)\n",
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    \n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    \n",
        "    #print(out.size())\n",
        "    #print(out)\n",
        "    #print(batch.trg.data.size())\n",
        "    #print(batch.trg.data)\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    \n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Translation:\tSo I planted a plant in front of my house . \n",
            "Target:\tSo what I did , I planted a food forest in front of my house . \n",
            "\n",
            "Translation:\tYou can see the central figures , like the leaders are the group . \n",
            "Target:\tYou can see the hubs , like who are the leaders in the group . \n",
            "\n",
            "Translation:\tAnd I 'll tell you , \" I do n't know that 's because responsibility in my responsibility . \" \n",
            "Target:\tAnd the first answer is , \" I do n't know , they do n't put me in charge of that . \" \n",
            "\n",
            "Translation:\tOver four decades , <unk> regimes have broken the regimes , the infrastructure and the moral structure of society , the moral structure of society . \n",
            "Target:\tFor four decades Gaddafi 's tyrannical regime destroyed the infrastructure as well as the culture and the moral fabric of Libyan society . \n",
            "\n",
            "Translation:\tI was just three years old when my brother was born , and I was so excited that I had a new creature in my life . \n",
            "Target:\tI was just three years old when my brother came along , and I was so excited that I had a new being in my life . \n",
            "\n",
            "Translation:\tOur first project , which was my first book , \" <unk> , \" was inspired by a guy named <unk> , who wanted to show the people how to show food . \n",
            "Target:\tOur first project , the one that has inspired my first book , \" <unk> from the <unk> , \" was a project where we Italians decided to teach Zambian people how to grow food . \n",
            "\n",
            "Translation:\tBut I 'm going to go back to the thought of the things that I 've taught me about individuality , and I understand that love , that these things are not about things that I would n't want to share with . \n",
            "Target:\tBut I cast my mind back to the things that they 've taught me about individuality and communication and love , and I realize that these are things that I would n't want to change with normality . \n",
            "\n",
            "Translation:\tSo , in Afghanistan , there are a lot of great things that you could do about art , but I do n't want to make art , I do n't want to understand the kind of attitude and the world of interdependence , and the kind of world that we 're trying to understand . \n",
            "Target:\tNow there are a lot of wonderful things that you could make art about in Afghanistan , but personally I do n't want to paint rainbows ; I want to make art that <unk> identity and challenges authority and exposes hypocrisy and reinterprets reality and even uses kind of an imaginative <unk> to try and understand the world that we live in . \n",
            "\n",
            "Translation:\tIf there 's a photographer and the photographer 's light , it 's like a nice one , and a client says , \" Cameron , we want to run a picture , \" We want to run back to this arm , and then you go back to your head , and you see this beautiful arm , \n",
            "Target:\tSo if the photographer is right there and the light is right there , like a nice <unk> , and the client says , \" Cameron , we want a walking shot , \" well then this leg goes first , nice and long , this arm goes back , this arm goes forward , the head is at three quarters , and you just go back and forth , just do that , and then you look back at your imaginary friends , 300 , 400 , 500 times . \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WDtn0UqHA9lY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# General Conference Dataset"
      ]
    },
    {
      "metadata": {
        "id": "vzF8fPQuA_4_",
        "colab_type": "code",
        "outputId": "2cc0fac6-1997-46f6-d98b-8ecb36ef10d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -O ./text_files.tar.gz 'http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2018:es-en-general-conference.tar.gz' \n",
        "!tar -xzf text_files.tar.gz\n",
        "\n",
        "import glob\n",
        "print(glob.glob('./*'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-21 18:28:03--  http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2018:es-en-general-conference.tar.gz\n",
            "Resolving liftothers.org (liftothers.org)... 50.62.229.1\n",
            "Connecting to liftothers.org (liftothers.org)|50.62.229.1|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18318204 (17M) [application/octet-stream]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]  17.47M  12.5MB/s    in 1.4s    \n",
            "\n",
            "2018-10-21 18:28:04 (12.5 MB/s) - ‘./text_files.tar.gz’ saved [18318204/18318204]\n",
            "\n",
            "['./sample_data', './en-es_gc_2010-2017_en.txt', './text_files.tar.gz', './en-es_gc_2010-2017_es.txt', './en-es_conference.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYLaiCmhDcop",
        "colab_type": "code",
        "outputId": "7357b481-1033-46f1-f20f-9314098bfbd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "cell_type": "code",
      "source": [
        "gc_data_csv = './en-es_conference.csv'\n",
        "\n",
        "spacy_es = spacy.load('es')\n",
        "\n",
        "def tokenize_es(text):\n",
        "    return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "\n",
        "SRC = data.Field(tokenize=tokenize_es, pad_token=BLANK_WORD)\n",
        "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "gc_data = data.TabularDataset(gc_data_csv, 'csv',\n",
        "                              fields=[('Index', None),('trg', TGT),('src', SRC)],\n",
        "                              skip_header=True)\n",
        "\n",
        "print(gc_data[0])\n",
        "print(gc_data[0].src, gc_data[0].trg)\n",
        "train, val = gc_data.split(split_ratio=0.99)\n",
        "print(len(gc_data))\n",
        "print(len(train), \":\", len(val))\n",
        "\n",
        "MAX_LEN = 100\n",
        "MIN_FREQ = 1\n",
        "\n",
        "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
        "\n",
        "print(len(SRC.vocab))\n",
        "print(len(TGT.vocab))\n",
        "\n",
        "# Set up a few constant parameters\n",
        "n_src = len(SRC.vocab)\n",
        "n_tgt = len(TGT.vocab)\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.data.example.Example object at 0x7ffaac7feba8>\n",
            "['El', 'templo', 'tiene', 'un', 'lugar', 'en', 'el', 'centro', 'mismo', 'de', 'nuestras', 'creencias', 'más', 'sagradas', 'y', 'el', 'Señor', 'nos', 'pide', 'que', 'asistamos', ',', 'meditemos', ',', 'estudiemos', 'y', 'encontremos', 'significado', 'personal', 'y', 'aplicación', 'individual', '.'] ['The', 'temple', 'holds', 'a', 'place', 'at', 'the', 'very', 'center', 'of', 'our', 'most', 'sacred', 'beliefs', ',', 'and', 'the', 'Lord', 'asks', 'that', 'we', 'attend', ',', 'ponder', ',', 'study', ',', 'and', 'find', 'personal', 'meaning', 'and', 'application', 'individually', '.']\n",
            "105786\n",
            "104728 : 1058\n",
            "46981\n",
            "32034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ojEP6-S3D92Z",
        "colab_type": "code",
        "outputId": "9febd74a-816a-4ac7-c2eb-7e1d33853cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "cell_type": "code",
      "source": [
        "# Batch size is 4096 in the colab notebook and 12000 in the jupyter\n",
        "BATCH_SIZE = 4096\n",
        "\n",
        "# Set up the model \n",
        "model = make_model(n_src, n_tgt, N=6)\n",
        "model_opt = get_std_opt(model)\n",
        "model.cuda()\n",
        "\n",
        "# Set up the label smoothing to penalize overconfidence\n",
        "criterion = LabelSmoothing(size=n_tgt, padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "\n",
        "# Set up training and validation iterators\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True)\n",
        "\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "num_train_batches = 0\n",
        "for i, batch in enumerate(train_iter):\n",
        "    num_train_batches += 1\n",
        "num_valid_batches = 0\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    num_valid_batches += 1\n",
        "print(\"\\nNumber of training batches:\", num_train_batches)\n",
        "print(\"Number of validation batches:\", num_valid_batches, \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of training batches: 779\n",
            "Number of validation batches: 12 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ebyUq7RMD_rj",
        "colab_type": "code",
        "outputId": "ab4bce21-5723-4c7d-8e67-18fe2610f893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 24923
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(15):\n",
        "    print(\"\\n\\n%%%%%%%%%% EPOCH \" + str(epoch) + \" %%%%%%%%%%\\n\")\n",
        "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, \n",
        "                model_opt)\n",
        "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 0 %%%%%%%%%%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 1 Loss 8.7362 Learning Rate 7e-07\n",
            "Batch 11 Loss 8.1152 Learning Rate 4.19e-06\n",
            "Batch 21 Loss 8.2874 Learning Rate 7.69e-06\n",
            "Batch 31 Loss 9.1505 Learning Rate 1.118e-05\n",
            "Batch 41 Loss 8.8966 Learning Rate 1.467e-05\n",
            "Batch 51 Loss 8.2365 Learning Rate 1.817e-05\n",
            "Batch 61 Loss 8.5972 Learning Rate 2.166e-05\n",
            "Batch 71 Loss 8.1104 Learning Rate 2.516e-05\n",
            "Batch 81 Loss 6.8483 Learning Rate 2.865e-05\n",
            "Batch 91 Loss 8.0527 Learning Rate 3.214e-05\n",
            "Batch 101 Loss 7.3726 Learning Rate 3.564e-05\n",
            "Batch 111 Loss 7.3194 Learning Rate 3.913e-05\n",
            "Batch 121 Loss 7.3304 Learning Rate 4.263e-05\n",
            "Batch 131 Loss 7.0184 Learning Rate 4.612e-05\n",
            "Batch 141 Loss 6.6664 Learning Rate 4.961e-05\n",
            "Batch 151 Loss 6.8967 Learning Rate 5.311e-05\n",
            "Batch 161 Loss 6.3782 Learning Rate 5.66e-05\n",
            "Batch 171 Loss 6.1456 Learning Rate 6.009e-05\n",
            "Batch 181 Loss 6.4922 Learning Rate 6.359e-05\n",
            "Batch 191 Loss 5.629 Learning Rate 6.708e-05\n",
            "Batch 201 Loss 6.0124 Learning Rate 7.058e-05\n",
            "Batch 211 Loss 5.759 Learning Rate 7.407e-05\n",
            "Batch 221 Loss 5.8349 Learning Rate 7.756e-05\n",
            "Batch 231 Loss 5.0509 Learning Rate 8.106e-05\n",
            "Batch 241 Loss 5.4 Learning Rate 8.455e-05\n",
            "Batch 251 Loss 5.7969 Learning Rate 8.805e-05\n",
            "Batch 261 Loss 5.3831 Learning Rate 9.154e-05\n",
            "Batch 271 Loss 5.5149 Learning Rate 9.503e-05\n",
            "Batch 281 Loss 5.3123 Learning Rate 9.853e-05\n",
            "Batch 291 Loss 4.6906 Learning Rate 0.00010202\n",
            "Batch 301 Loss 5.795 Learning Rate 0.00010551\n",
            "Batch 311 Loss 5.0921 Learning Rate 0.00010901\n",
            "Batch 321 Loss 5.5284 Learning Rate 0.0001125\n",
            "Batch 331 Loss 5.4161 Learning Rate 0.000116\n",
            "Batch 341 Loss 5.4516 Learning Rate 0.00011949\n",
            "Batch 351 Loss 5.2349 Learning Rate 0.00012298\n",
            "Batch 361 Loss 5.7518 Learning Rate 0.00012648\n",
            "Batch 371 Loss 5.4622 Learning Rate 0.00012997\n",
            "Batch 381 Loss 5.0662 Learning Rate 0.00013347\n",
            "Batch 391 Loss 5.0004 Learning Rate 0.00013696\n",
            "Batch 401 Loss 5.5745 Learning Rate 0.00014045\n",
            "Batch 411 Loss 5.2561 Learning Rate 0.00014395\n",
            "Batch 421 Loss 5.34 Learning Rate 0.00014744\n",
            "Batch 431 Loss 5.1555 Learning Rate 0.00015093\n",
            "Batch 441 Loss 4.7746 Learning Rate 0.00015443\n",
            "Batch 451 Loss 4.4097 Learning Rate 0.00015792\n",
            "Batch 461 Loss 4.5838 Learning Rate 0.00016142\n",
            "Batch 471 Loss 4.361 Learning Rate 0.00016491\n",
            "Batch 481 Loss 5.0108 Learning Rate 0.0001684\n",
            "Batch 491 Loss 5.1133 Learning Rate 0.0001719\n",
            "Batch 501 Loss 4.7405 Learning Rate 0.00017539\n",
            "Batch 511 Loss 4.3417 Learning Rate 0.00017889\n",
            "Batch 521 Loss 4.8725 Learning Rate 0.00018238\n",
            "Batch 531 Loss 4.6335 Learning Rate 0.00018587\n",
            "Batch 541 Loss 4.8158 Learning Rate 0.00018937\n",
            "Batch 551 Loss 5.2535 Learning Rate 0.00019286\n",
            "Batch 561 Loss 5.2908 Learning Rate 0.00019635\n",
            "Batch 571 Loss 4.6435 Learning Rate 0.00019985\n",
            "Batch 581 Loss 4.6783 Learning Rate 0.00020334\n",
            "Batch 591 Loss 4.6635 Learning Rate 0.00020684\n",
            "Batch 601 Loss 4.1876 Learning Rate 0.00021033\n",
            "Batch 611 Loss 4.6311 Learning Rate 0.00021382\n",
            "Batch 621 Loss 4.8446 Learning Rate 0.00021732\n",
            "Batch 631 Loss 4.581 Learning Rate 0.00022081\n",
            "Batch 641 Loss 4.6251 Learning Rate 0.00022431\n",
            "Batch 651 Loss 4.8755 Learning Rate 0.0002278\n",
            "Batch 661 Loss 4.4027 Learning Rate 0.00023129\n",
            "Batch 671 Loss 4.6933 Learning Rate 0.00023479\n",
            "Batch 681 Loss 4.912 Learning Rate 0.00023828\n",
            "Batch 691 Loss 4.375 Learning Rate 0.00024177\n",
            "Batch 701 Loss 4.5174 Learning Rate 0.00024527\n",
            "Batch 711 Loss 4.99 Learning Rate 0.00024876\n",
            "Batch 721 Loss 4.269 Learning Rate 0.00025226\n",
            "Batch 731 Loss 4.1872 Learning Rate 0.00025575\n",
            "Batch 741 Loss 3.2704 Learning Rate 0.00025924\n",
            "Batch 751 Loss 3.7725 Learning Rate 0.00026274\n",
            "Batch 761 Loss 4.6102 Learning Rate 0.00026623\n",
            "Batch 771 Loss 4.8728 Learning Rate 0.00026973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 0 validation loss: 3.7105350494384766\n",
            "Batch 1 validation loss: 4.268636703491211\n",
            "Batch 2 validation loss: 5.592027187347412\n",
            "Batch 3 validation loss: 4.039651393890381\n",
            "Batch 4 validation loss: 4.270091533660889\n",
            "Batch 5 validation loss: 4.324169635772705\n",
            "Batch 6 validation loss: 4.464666843414307\n",
            "Batch 7 validation loss: 4.521942138671875\n",
            "Batch 8 validation loss: 4.539643287658691\n",
            "Batch 9 validation loss: 4.68431282043457\n",
            "Batch 10 validation loss: 4.729336261749268\n",
            "Batch 11 validation loss: 5.159358978271484\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 1 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 3.9577 Learning Rate 0.00027287\n",
            "Batch 11 Loss 4.8538 Learning Rate 0.00027636\n",
            "Batch 21 Loss 4.1408 Learning Rate 0.00027986\n",
            "Batch 31 Loss 4.5991 Learning Rate 0.00028335\n",
            "Batch 41 Loss 4.1712 Learning Rate 0.00028685\n",
            "Batch 51 Loss 4.4945 Learning Rate 0.00029034\n",
            "Batch 61 Loss 4.7803 Learning Rate 0.00029383\n",
            "Batch 71 Loss 4.5315 Learning Rate 0.00029733\n",
            "Batch 81 Loss 3.5574 Learning Rate 0.00030082\n",
            "Batch 91 Loss 3.7748 Learning Rate 0.00030431\n",
            "Batch 101 Loss 3.9106 Learning Rate 0.00030781\n",
            "Batch 111 Loss 5.2214 Learning Rate 0.0003113\n",
            "Batch 121 Loss 4.3984 Learning Rate 0.0003148\n",
            "Batch 131 Loss 4.6445 Learning Rate 0.00031829\n",
            "Batch 141 Loss 4.0175 Learning Rate 0.00032178\n",
            "Batch 151 Loss 4.3125 Learning Rate 0.00032528\n",
            "Batch 161 Loss 5.0588 Learning Rate 0.00032877\n",
            "Batch 171 Loss 5.0181 Learning Rate 0.00033227\n",
            "Batch 181 Loss 4.0195 Learning Rate 0.00033576\n",
            "Batch 191 Loss 4.3352 Learning Rate 0.00033925\n",
            "Batch 201 Loss 4.8488 Learning Rate 0.00034275\n",
            "Batch 211 Loss 4.317 Learning Rate 0.00034624\n",
            "Batch 221 Loss 4.8932 Learning Rate 0.00034974\n",
            "Batch 231 Loss 4.4872 Learning Rate 0.00035323\n",
            "Batch 241 Loss 4.2332 Learning Rate 0.00035672\n",
            "Batch 251 Loss 4.7186 Learning Rate 0.00036022\n",
            "Batch 261 Loss 4.368 Learning Rate 0.00036371\n",
            "Batch 271 Loss 3.9106 Learning Rate 0.0003672\n",
            "Batch 281 Loss 4.1537 Learning Rate 0.0003707\n",
            "Batch 291 Loss 4.0637 Learning Rate 0.00037419\n",
            "Batch 301 Loss 4.4374 Learning Rate 0.00037769\n",
            "Batch 311 Loss 3.9776 Learning Rate 0.00038118\n",
            "Batch 321 Loss 4.9321 Learning Rate 0.00038467\n",
            "Batch 331 Loss 4.1454 Learning Rate 0.00038817\n",
            "Batch 341 Loss 4.7788 Learning Rate 0.00039166\n",
            "Batch 351 Loss 4.2911 Learning Rate 0.00039516\n",
            "Batch 361 Loss 5.2144 Learning Rate 0.00039865\n",
            "Batch 371 Loss 4.1455 Learning Rate 0.00040214\n",
            "Batch 381 Loss 4.201 Learning Rate 0.00040564\n",
            "Batch 391 Loss 4.16 Learning Rate 0.00040913\n",
            "Batch 401 Loss 4.575 Learning Rate 0.00041262\n",
            "Batch 411 Loss 4.6038 Learning Rate 0.00041612\n",
            "Batch 421 Loss 4.0082 Learning Rate 0.00041961\n",
            "Batch 431 Loss 2.7852 Learning Rate 0.00042311\n",
            "Batch 441 Loss 3.7687 Learning Rate 0.0004266\n",
            "Batch 451 Loss 4.1503 Learning Rate 0.00043009\n",
            "Batch 461 Loss 2.2105 Learning Rate 0.00043359\n",
            "Batch 471 Loss 3.6948 Learning Rate 0.00043708\n",
            "Batch 481 Loss 4.4885 Learning Rate 0.00044058\n",
            "Batch 491 Loss 3.6818 Learning Rate 0.00044407\n",
            "Batch 501 Loss 3.6602 Learning Rate 0.00044756\n",
            "Batch 511 Loss 3.8711 Learning Rate 0.00045106\n",
            "Batch 521 Loss 4.7106 Learning Rate 0.00045455\n",
            "Batch 531 Loss 3.8525 Learning Rate 0.00045804\n",
            "Batch 541 Loss 3.5976 Learning Rate 0.00046154\n",
            "Batch 551 Loss 3.0965 Learning Rate 0.00046503\n",
            "Batch 561 Loss 3.6517 Learning Rate 0.00046853\n",
            "Batch 571 Loss 4.1165 Learning Rate 0.00047202\n",
            "Batch 581 Loss 3.5023 Learning Rate 0.00047551\n",
            "Batch 591 Loss 3.338 Learning Rate 0.00047901\n",
            "Batch 601 Loss 4.0673 Learning Rate 0.0004825\n",
            "Batch 611 Loss 4.3706 Learning Rate 0.000486\n",
            "Batch 621 Loss 3.1075 Learning Rate 0.00048949\n",
            "Batch 631 Loss 4.0917 Learning Rate 0.00049298\n",
            "Batch 641 Loss 2.1374 Learning Rate 0.00049648\n",
            "Batch 651 Loss 4.0439 Learning Rate 0.00049997\n",
            "Batch 661 Loss 2.8451 Learning Rate 0.00050346\n",
            "Batch 671 Loss 3.4053 Learning Rate 0.00050696\n",
            "Batch 681 Loss 4.5584 Learning Rate 0.00051045\n",
            "Batch 691 Loss 3.48 Learning Rate 0.00051395\n",
            "Batch 701 Loss 3.4905 Learning Rate 0.00051744\n",
            "Batch 711 Loss 3.6127 Learning Rate 0.00052093\n",
            "Batch 721 Loss 3.6277 Learning Rate 0.00052443\n",
            "Batch 731 Loss 3.2622 Learning Rate 0.00052792\n",
            "Batch 741 Loss 3.3519 Learning Rate 0.00053142\n",
            "Batch 751 Loss 3.4062 Learning Rate 0.00053491\n",
            "Batch 761 Loss 3.5373 Learning Rate 0.0005384\n",
            "Batch 771 Loss 3.8564 Learning Rate 0.0005419\n",
            "Batch 0 validation loss: 2.532510995864868\n",
            "Batch 1 validation loss: 3.1414785385131836\n",
            "Batch 2 validation loss: 4.754210472106934\n",
            "Batch 3 validation loss: 3.0079684257507324\n",
            "Batch 4 validation loss: 3.28088116645813\n",
            "Batch 5 validation loss: 3.277064085006714\n",
            "Batch 6 validation loss: 3.4214565753936768\n",
            "Batch 7 validation loss: 3.4802322387695312\n",
            "Batch 8 validation loss: 3.600802421569824\n",
            "Batch 9 validation loss: 3.761362314224243\n",
            "Batch 10 validation loss: 3.810892105102539\n",
            "Batch 11 validation loss: 4.303008079528809\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 2 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 3.6227 Learning Rate 0.00054504\n",
            "Batch 11 Loss 3.5581 Learning Rate 0.00054854\n",
            "Batch 21 Loss 3.642 Learning Rate 0.00055203\n",
            "Batch 31 Loss 3.5616 Learning Rate 0.00055552\n",
            "Batch 41 Loss 3.7303 Learning Rate 0.00055902\n",
            "Batch 51 Loss 2.9087 Learning Rate 0.00056251\n",
            "Batch 61 Loss 4.0297 Learning Rate 0.000566\n",
            "Batch 71 Loss 3.4962 Learning Rate 0.0005695\n",
            "Batch 81 Loss 2.757 Learning Rate 0.00057299\n",
            "Batch 91 Loss 2.9364 Learning Rate 0.00057649\n",
            "Batch 101 Loss 4.4323 Learning Rate 0.00057998\n",
            "Batch 111 Loss 3.7784 Learning Rate 0.00058347\n",
            "Batch 121 Loss 2.9687 Learning Rate 0.00058697\n",
            "Batch 131 Loss 3.8421 Learning Rate 0.00059046\n",
            "Batch 141 Loss 3.3428 Learning Rate 0.00059396\n",
            "Batch 151 Loss 3.3012 Learning Rate 0.00059745\n",
            "Batch 161 Loss 2.8704 Learning Rate 0.00060094\n",
            "Batch 171 Loss 2.8038 Learning Rate 0.00060444\n",
            "Batch 181 Loss 2.7927 Learning Rate 0.00060793\n",
            "Batch 191 Loss 3.2133 Learning Rate 0.00061142\n",
            "Batch 201 Loss 3.5755 Learning Rate 0.00061492\n",
            "Batch 211 Loss 2.754 Learning Rate 0.00061841\n",
            "Batch 221 Loss 2.8177 Learning Rate 0.00062191\n",
            "Batch 231 Loss 3.514 Learning Rate 0.0006254\n",
            "Batch 241 Loss 3.5033 Learning Rate 0.00062889\n",
            "Batch 251 Loss 3.2543 Learning Rate 0.00063239\n",
            "Batch 261 Loss 3.653 Learning Rate 0.00063588\n",
            "Batch 271 Loss 3.5804 Learning Rate 0.00063938\n",
            "Batch 281 Loss 3.1278 Learning Rate 0.00064287\n",
            "Batch 291 Loss 3.1758 Learning Rate 0.00064636\n",
            "Batch 301 Loss 2.9063 Learning Rate 0.00064986\n",
            "Batch 311 Loss 3.2305 Learning Rate 0.00065335\n",
            "Batch 321 Loss 3.1172 Learning Rate 0.00065684\n",
            "Batch 331 Loss 3.776 Learning Rate 0.00066034\n",
            "Batch 341 Loss 2.2373 Learning Rate 0.00066383\n",
            "Batch 351 Loss 3.095 Learning Rate 0.00066733\n",
            "Batch 361 Loss 3.0958 Learning Rate 0.00067082\n",
            "Batch 371 Loss 3.4504 Learning Rate 0.00067431\n",
            "Batch 381 Loss 2.6454 Learning Rate 0.00067781\n",
            "Batch 391 Loss 3.7221 Learning Rate 0.0006813\n",
            "Batch 401 Loss 2.7488 Learning Rate 0.0006848\n",
            "Batch 411 Loss 3.3909 Learning Rate 0.00068829\n",
            "Batch 421 Loss 3.5327 Learning Rate 0.00069178\n",
            "Batch 431 Loss 3.8864 Learning Rate 0.00069528\n",
            "Batch 441 Loss 3.5259 Learning Rate 0.00069877\n",
            "Batch 451 Loss 3.011 Learning Rate 0.00070227\n",
            "Batch 461 Loss 2.8966 Learning Rate 0.00070576\n",
            "Batch 471 Loss 3.3939 Learning Rate 0.00070925\n",
            "Batch 481 Loss 3.5683 Learning Rate 0.00071275\n",
            "Batch 491 Loss 3.1549 Learning Rate 0.00071624\n",
            "Batch 501 Loss 3.1249 Learning Rate 0.00071973\n",
            "Batch 511 Loss 2.9 Learning Rate 0.00072323\n",
            "Batch 521 Loss 3.2569 Learning Rate 0.00072672\n",
            "Batch 531 Loss 3.2676 Learning Rate 0.00073022\n",
            "Batch 541 Loss 3.2817 Learning Rate 0.00073371\n",
            "Batch 551 Loss 3.4539 Learning Rate 0.0007372\n",
            "Batch 561 Loss 2.5739 Learning Rate 0.0007407\n",
            "Batch 571 Loss 2.5216 Learning Rate 0.00074419\n",
            "Batch 581 Loss 2.9775 Learning Rate 0.00074769\n",
            "Batch 591 Loss 3.142 Learning Rate 0.00075118\n",
            "Batch 601 Loss 2.528 Learning Rate 0.00075467\n",
            "Batch 611 Loss 2.834 Learning Rate 0.00075817\n",
            "Batch 621 Loss 2.794 Learning Rate 0.00076166\n",
            "Batch 631 Loss 3.0271 Learning Rate 0.00076515\n",
            "Batch 641 Loss 2.9242 Learning Rate 0.00076865\n",
            "Batch 651 Loss 3.2013 Learning Rate 0.00077214\n",
            "Batch 661 Loss 2.8795 Learning Rate 0.00077564\n",
            "Batch 671 Loss 2.6126 Learning Rate 0.00077913\n",
            "Batch 681 Loss 2.7531 Learning Rate 0.00078262\n",
            "Batch 691 Loss 3.6534 Learning Rate 0.00078612\n",
            "Batch 701 Loss 2.8446 Learning Rate 0.00078961\n",
            "Batch 711 Loss 3.0181 Learning Rate 0.00079311\n",
            "Batch 721 Loss 2.9617 Learning Rate 0.0007966\n",
            "Batch 731 Loss 2.8762 Learning Rate 0.00080009\n",
            "Batch 741 Loss 2.9231 Learning Rate 0.00080359\n",
            "Batch 751 Loss 3.0322 Learning Rate 0.00080708\n",
            "Batch 761 Loss 4.0216 Learning Rate 0.00081057\n",
            "Batch 771 Loss 2.4489 Learning Rate 0.00081407\n",
            "Batch 0 validation loss: 1.6156717538833618\n",
            "Batch 1 validation loss: 2.3870980739593506\n",
            "Batch 2 validation loss: 3.9101929664611816\n",
            "Batch 3 validation loss: 2.215123176574707\n",
            "Batch 4 validation loss: 2.436215877532959\n",
            "Batch 5 validation loss: 2.492912530899048\n",
            "Batch 6 validation loss: 2.6540815830230713\n",
            "Batch 7 validation loss: 2.746965169906616\n",
            "Batch 8 validation loss: 2.9557178020477295\n",
            "Batch 9 validation loss: 3.1323115825653076\n",
            "Batch 10 validation loss: 3.2064127922058105\n",
            "Batch 11 validation loss: 3.629269599914551\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 3 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 2.4471 Learning Rate 0.00081721\n",
            "Batch 11 Loss 2.6407 Learning Rate 0.00082071\n",
            "Batch 21 Loss 3.0719 Learning Rate 0.0008242\n",
            "Batch 31 Loss 2.5804 Learning Rate 0.00082769\n",
            "Batch 41 Loss 3.0116 Learning Rate 0.00083119\n",
            "Batch 51 Loss 2.1843 Learning Rate 0.00083468\n",
            "Batch 61 Loss 2.022 Learning Rate 0.00083818\n",
            "Batch 71 Loss 2.419 Learning Rate 0.00084167\n",
            "Batch 81 Loss 2.4134 Learning Rate 0.00084516\n",
            "Batch 91 Loss 2.7972 Learning Rate 0.00084866\n",
            "Batch 101 Loss 2.1027 Learning Rate 0.00085215\n",
            "Batch 111 Loss 2.7738 Learning Rate 0.00085565\n",
            "Batch 121 Loss 2.5097 Learning Rate 0.00085914\n",
            "Batch 131 Loss 2.3721 Learning Rate 0.00086263\n",
            "Batch 141 Loss 2.9518 Learning Rate 0.00086613\n",
            "Batch 151 Loss 2.2004 Learning Rate 0.00086962\n",
            "Batch 161 Loss 2.4972 Learning Rate 0.00087311\n",
            "Batch 171 Loss 3.1019 Learning Rate 0.00087661\n",
            "Batch 181 Loss 2.5677 Learning Rate 0.0008801\n",
            "Batch 191 Loss 2.5962 Learning Rate 0.0008836\n",
            "Batch 201 Loss 2.6263 Learning Rate 0.00088709\n",
            "Batch 211 Loss 2.8155 Learning Rate 0.00089058\n",
            "Batch 221 Loss 3.0231 Learning Rate 0.00089408\n",
            "Batch 231 Loss 3.0327 Learning Rate 0.00089757\n",
            "Batch 241 Loss 2.6377 Learning Rate 0.00090107\n",
            "Batch 251 Loss 2.7904 Learning Rate 0.00090456\n",
            "Batch 261 Loss 2.9255 Learning Rate 0.00090805\n",
            "Batch 271 Loss 2.9254 Learning Rate 0.00091155\n",
            "Batch 281 Loss 2.3305 Learning Rate 0.00091504\n",
            "Batch 291 Loss 2.2951 Learning Rate 0.00091853\n",
            "Batch 301 Loss 1.879 Learning Rate 0.00092203\n",
            "Batch 311 Loss 1.9101 Learning Rate 0.00092552\n",
            "Batch 321 Loss 3.1696 Learning Rate 0.00092902\n",
            "Batch 331 Loss 2.2731 Learning Rate 0.00093251\n",
            "Batch 341 Loss 2.4598 Learning Rate 0.000936\n",
            "Batch 351 Loss 2.0403 Learning Rate 0.0009395\n",
            "Batch 361 Loss 3.4397 Learning Rate 0.00094299\n",
            "Batch 371 Loss 3.1609 Learning Rate 0.00094649\n",
            "Batch 381 Loss 2.2945 Learning Rate 0.00094998\n",
            "Batch 391 Loss 2.4709 Learning Rate 0.00095347\n",
            "Batch 401 Loss 2.883 Learning Rate 0.00095697\n",
            "Batch 411 Loss 2.6254 Learning Rate 0.00096046\n",
            "Batch 421 Loss 2.261 Learning Rate 0.00096395\n",
            "Batch 431 Loss 2.6377 Learning Rate 0.00096745\n",
            "Batch 441 Loss 2.1512 Learning Rate 0.00097094\n",
            "Batch 451 Loss 2.3314 Learning Rate 0.00097444\n",
            "Batch 461 Loss 2.8797 Learning Rate 0.00097793\n",
            "Batch 471 Loss 2.0245 Learning Rate 0.00098142\n",
            "Batch 481 Loss 2.8147 Learning Rate 0.00098492\n",
            "Batch 491 Loss 2.2352 Learning Rate 0.00098841\n",
            "Batch 501 Loss 2.6125 Learning Rate 0.00099191\n",
            "Batch 511 Loss 2.2389 Learning Rate 0.0009954\n",
            "Batch 521 Loss 2.3701 Learning Rate 0.00099889\n",
            "Batch 531 Loss 2.0957 Learning Rate 0.00100239\n",
            "Batch 541 Loss 2.6104 Learning Rate 0.00100588\n",
            "Batch 551 Loss 2.9377 Learning Rate 0.00100938\n",
            "Batch 561 Loss 2.1357 Learning Rate 0.00101287\n",
            "Batch 571 Loss 2.5415 Learning Rate 0.00101636\n",
            "Batch 581 Loss 2.2362 Learning Rate 0.00101986\n",
            "Batch 591 Loss 1.9273 Learning Rate 0.00102335\n",
            "Batch 601 Loss 2.508 Learning Rate 0.00102684\n",
            "Batch 611 Loss 2.3939 Learning Rate 0.00103034\n",
            "Batch 621 Loss 1.7854 Learning Rate 0.00103383\n",
            "Batch 631 Loss 3.1216 Learning Rate 0.00103733\n",
            "Batch 641 Loss 1.789 Learning Rate 0.00104082\n",
            "Batch 651 Loss 1.8742 Learning Rate 0.00104431\n",
            "Batch 661 Loss 1.653 Learning Rate 0.00104781\n",
            "Batch 671 Loss 2.7108 Learning Rate 0.0010513\n",
            "Batch 681 Loss 1.131 Learning Rate 0.0010548\n",
            "Batch 691 Loss 1.8055 Learning Rate 0.00105829\n",
            "Batch 701 Loss 2.849 Learning Rate 0.00106178\n",
            "Batch 711 Loss 2.786 Learning Rate 0.00106528\n",
            "Batch 721 Loss 1.918 Learning Rate 0.00106877\n",
            "Batch 731 Loss 1.8155 Learning Rate 0.00107226\n",
            "Batch 741 Loss 2.1198 Learning Rate 0.00107576\n",
            "Batch 751 Loss 1.4298 Learning Rate 0.00107925\n",
            "Batch 761 Loss 3.2347 Learning Rate 0.00108275\n",
            "Batch 771 Loss 2.2914 Learning Rate 0.00108624\n",
            "Batch 0 validation loss: 1.1472764015197754\n",
            "Batch 1 validation loss: 1.735678791999817\n",
            "Batch 2 validation loss: 3.2317421436309814\n",
            "Batch 3 validation loss: 1.7128630876541138\n",
            "Batch 4 validation loss: 1.891152262687683\n",
            "Batch 5 validation loss: 1.946831226348877\n",
            "Batch 6 validation loss: 2.0886857509613037\n",
            "Batch 7 validation loss: 2.1744608879089355\n",
            "Batch 8 validation loss: 2.328662395477295\n",
            "Batch 9 validation loss: 2.4604618549346924\n",
            "Batch 10 validation loss: 2.592735767364502\n",
            "Batch 11 validation loss: 2.9295411109924316\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 4 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.814 Learning Rate 0.00108938\n",
            "Batch 11 Loss 2.5871 Learning Rate 0.00109288\n",
            "Batch 21 Loss 2.7259 Learning Rate 0.00109637\n",
            "Batch 31 Loss 1.6304 Learning Rate 0.00109987\n",
            "Batch 41 Loss 2.0566 Learning Rate 0.00110336\n",
            "Batch 51 Loss 1.7944 Learning Rate 0.00110685\n",
            "Batch 61 Loss 2.4433 Learning Rate 0.00111035\n",
            "Batch 71 Loss 2.2424 Learning Rate 0.00111384\n",
            "Batch 81 Loss 2.5825 Learning Rate 0.00111734\n",
            "Batch 91 Loss 2.5568 Learning Rate 0.00112083\n",
            "Batch 101 Loss 1.9679 Learning Rate 0.00112432\n",
            "Batch 111 Loss 2.4046 Learning Rate 0.00112782\n",
            "Batch 121 Loss 1.2333 Learning Rate 0.00113131\n",
            "Batch 131 Loss 2.3128 Learning Rate 0.0011348\n",
            "Batch 141 Loss 1.6998 Learning Rate 0.0011383\n",
            "Batch 151 Loss 1.352 Learning Rate 0.00114179\n",
            "Batch 161 Loss 2.6482 Learning Rate 0.00114529\n",
            "Batch 171 Loss 2.6105 Learning Rate 0.00114878\n",
            "Batch 181 Loss 1.557 Learning Rate 0.00115227\n",
            "Batch 191 Loss 2.0735 Learning Rate 0.00115577\n",
            "Batch 201 Loss 2.8596 Learning Rate 0.00115926\n",
            "Batch 211 Loss 1.5936 Learning Rate 0.00116276\n",
            "Batch 221 Loss 1.8124 Learning Rate 0.00116625\n",
            "Batch 231 Loss 2.2845 Learning Rate 0.00116974\n",
            "Batch 241 Loss 2.3939 Learning Rate 0.00117324\n",
            "Batch 251 Loss 2.492 Learning Rate 0.00117673\n",
            "Batch 261 Loss 1.6745 Learning Rate 0.00118022\n",
            "Batch 271 Loss 2.1457 Learning Rate 0.00118372\n",
            "Batch 281 Loss 1.7708 Learning Rate 0.00118721\n",
            "Batch 291 Loss 3.549 Learning Rate 0.00119071\n",
            "Batch 301 Loss 2.4256 Learning Rate 0.0011942\n",
            "Batch 311 Loss 2.2982 Learning Rate 0.00119769\n",
            "Batch 321 Loss 1.699 Learning Rate 0.00120119\n",
            "Batch 331 Loss 2.3846 Learning Rate 0.00120468\n",
            "Batch 341 Loss 1.8757 Learning Rate 0.00120818\n",
            "Batch 351 Loss 2.563 Learning Rate 0.00121167\n",
            "Batch 361 Loss 2.7293 Learning Rate 0.00121516\n",
            "Batch 371 Loss 1.6132 Learning Rate 0.00121866\n",
            "Batch 381 Loss 2.0924 Learning Rate 0.00122215\n",
            "Batch 391 Loss 3.0295 Learning Rate 0.00122564\n",
            "Batch 401 Loss 1.9437 Learning Rate 0.00122914\n",
            "Batch 411 Loss 2.8279 Learning Rate 0.00123263\n",
            "Batch 421 Loss 1.6648 Learning Rate 0.00123613\n",
            "Batch 431 Loss 2.599 Learning Rate 0.00123962\n",
            "Batch 441 Loss 1.2898 Learning Rate 0.00124311\n",
            "Batch 451 Loss 0.3047 Learning Rate 0.00124661\n",
            "Batch 461 Loss 3.1195 Learning Rate 0.0012501\n",
            "Batch 471 Loss 2.3197 Learning Rate 0.0012536\n",
            "Batch 481 Loss 1.9263 Learning Rate 0.00125709\n",
            "Batch 491 Loss 2.3582 Learning Rate 0.00126058\n",
            "Batch 501 Loss 1.6327 Learning Rate 0.00126408\n",
            "Batch 511 Loss 1.933 Learning Rate 0.00126757\n",
            "Batch 521 Loss 2.8398 Learning Rate 0.00127106\n",
            "Batch 531 Loss 2.5354 Learning Rate 0.00127456\n",
            "Batch 541 Loss 2.8364 Learning Rate 0.00127805\n",
            "Batch 551 Loss 2.3018 Learning Rate 0.00128155\n",
            "Batch 561 Loss 2.4327 Learning Rate 0.00128504\n",
            "Batch 571 Loss 2.8689 Learning Rate 0.00128853\n",
            "Batch 581 Loss 1.8398 Learning Rate 0.00129203\n",
            "Batch 591 Loss 1.5205 Learning Rate 0.00129552\n",
            "Batch 601 Loss 2.0905 Learning Rate 0.00129902\n",
            "Batch 611 Loss 1.8409 Learning Rate 0.00130251\n",
            "Batch 621 Loss 1.8216 Learning Rate 0.001306\n",
            "Batch 631 Loss 2.597 Learning Rate 0.0013095\n",
            "Batch 641 Loss 1.8232 Learning Rate 0.00131299\n",
            "Batch 651 Loss 1.5388 Learning Rate 0.00131649\n",
            "Batch 661 Loss 1.2954 Learning Rate 0.00131998\n",
            "Batch 671 Loss 2.0516 Learning Rate 0.00132347\n",
            "Batch 681 Loss 1.552 Learning Rate 0.00132697\n",
            "Batch 691 Loss 1.2208 Learning Rate 0.00133046\n",
            "Batch 701 Loss 0.998 Learning Rate 0.00133395\n",
            "Batch 711 Loss 2.0023 Learning Rate 0.00133745\n",
            "Batch 721 Loss 2.1223 Learning Rate 0.00134094\n",
            "Batch 731 Loss 2.6615 Learning Rate 0.00134444\n",
            "Batch 741 Loss 1.5932 Learning Rate 0.00134793\n",
            "Batch 751 Loss 1.6892 Learning Rate 0.00135142\n",
            "Batch 761 Loss 1.934 Learning Rate 0.00135492\n",
            "Batch 771 Loss 3.4757 Learning Rate 0.00135841\n",
            "Batch 0 validation loss: 0.8921085000038147\n",
            "Batch 1 validation loss: 1.363336205482483\n",
            "Batch 2 validation loss: 2.670403480529785\n",
            "Batch 3 validation loss: 1.389446496963501\n",
            "Batch 4 validation loss: 1.5765602588653564\n",
            "Batch 5 validation loss: 1.594912052154541\n",
            "Batch 6 validation loss: 1.6493275165557861\n",
            "Batch 7 validation loss: 1.7423667907714844\n",
            "Batch 8 validation loss: 1.9076526165008545\n",
            "Batch 9 validation loss: 1.9674623012542725\n",
            "Batch 10 validation loss: 2.04099440574646\n",
            "Batch 11 validation loss: 2.429009199142456\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 5 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.6218 Learning Rate 0.00136156\n",
            "Batch 11 Loss 2.1215 Learning Rate 0.00136505\n",
            "Batch 21 Loss 1.8642 Learning Rate 0.00136854\n",
            "Batch 31 Loss 1.7023 Learning Rate 0.00137204\n",
            "Batch 41 Loss 1.9705 Learning Rate 0.00137553\n",
            "Batch 51 Loss 1.2586 Learning Rate 0.00137903\n",
            "Batch 61 Loss 1.9387 Learning Rate 0.00138252\n",
            "Batch 71 Loss 2.3728 Learning Rate 0.00138601\n",
            "Batch 81 Loss 1.7529 Learning Rate 0.00138951\n",
            "Batch 91 Loss 1.525 Learning Rate 0.001393\n",
            "Batch 101 Loss 1.2642 Learning Rate 0.00139649\n",
            "Batch 111 Loss 1.4655 Learning Rate 0.00139632\n",
            "Batch 121 Loss 1.5709 Learning Rate 0.00139458\n",
            "Batch 131 Loss 2.0133 Learning Rate 0.00139285\n",
            "Batch 141 Loss 1.3659 Learning Rate 0.00139112\n",
            "Batch 151 Loss 0.3709 Learning Rate 0.0013894\n",
            "Batch 161 Loss 1.9906 Learning Rate 0.00138769\n",
            "Batch 171 Loss 1.6535 Learning Rate 0.00138598\n",
            "Batch 181 Loss 1.6536 Learning Rate 0.00138428\n",
            "Batch 191 Loss 2.2812 Learning Rate 0.00138259\n",
            "Batch 201 Loss 2.4546 Learning Rate 0.0013809\n",
            "Batch 211 Loss 1.9161 Learning Rate 0.00137922\n",
            "Batch 221 Loss 1.6072 Learning Rate 0.00137754\n",
            "Batch 231 Loss 1.8693 Learning Rate 0.00137587\n",
            "Batch 241 Loss 1.7043 Learning Rate 0.00137421\n",
            "Batch 251 Loss 1.4008 Learning Rate 0.00137255\n",
            "Batch 261 Loss 1.558 Learning Rate 0.0013709\n",
            "Batch 271 Loss 3.1389 Learning Rate 0.00136925\n",
            "Batch 281 Loss 2.0329 Learning Rate 0.00136761\n",
            "Batch 291 Loss 1.8884 Learning Rate 0.00136598\n",
            "Batch 301 Loss 1.5719 Learning Rate 0.00136435\n",
            "Batch 311 Loss 3.8143 Learning Rate 0.00136273\n",
            "Batch 321 Loss 1.5492 Learning Rate 0.00136111\n",
            "Batch 331 Loss 1.6162 Learning Rate 0.0013595\n",
            "Batch 341 Loss 1.9489 Learning Rate 0.00135789\n",
            "Batch 351 Loss 1.5666 Learning Rate 0.00135629\n",
            "Batch 361 Loss 1.5632 Learning Rate 0.0013547\n",
            "Batch 371 Loss 1.6665 Learning Rate 0.00135311\n",
            "Batch 381 Loss 1.8774 Learning Rate 0.00135153\n",
            "Batch 391 Loss 1.7013 Learning Rate 0.00134995\n",
            "Batch 401 Loss 1.3354 Learning Rate 0.00134838\n",
            "Batch 411 Loss 1.1102 Learning Rate 0.00134681\n",
            "Batch 421 Loss 1.7995 Learning Rate 0.00134525\n",
            "Batch 431 Loss 1.6442 Learning Rate 0.0013437\n",
            "Batch 441 Loss 1.9601 Learning Rate 0.00134215\n",
            "Batch 451 Loss 1.6854 Learning Rate 0.0013406\n",
            "Batch 461 Loss 1.7405 Learning Rate 0.00133906\n",
            "Batch 471 Loss 1.623 Learning Rate 0.00133753\n",
            "Batch 481 Loss 1.3887 Learning Rate 0.001336\n",
            "Batch 491 Loss 1.8789 Learning Rate 0.00133448\n",
            "Batch 501 Loss 1.5188 Learning Rate 0.00133296\n",
            "Batch 511 Loss 1.9352 Learning Rate 0.00133145\n",
            "Batch 521 Loss 1.4285 Learning Rate 0.00132994\n",
            "Batch 531 Loss 1.4607 Learning Rate 0.00132843\n",
            "Batch 541 Loss 1.9575 Learning Rate 0.00132694\n",
            "Batch 551 Loss 1.2958 Learning Rate 0.00132544\n",
            "Batch 561 Loss 1.9723 Learning Rate 0.00132396\n",
            "Batch 571 Loss 1.6023 Learning Rate 0.00132247\n",
            "Batch 581 Loss 1.4877 Learning Rate 0.001321\n",
            "Batch 591 Loss 1.3648 Learning Rate 0.00131952\n",
            "Batch 601 Loss 1.4085 Learning Rate 0.00131806\n",
            "Batch 611 Loss 0.5758 Learning Rate 0.00131659\n",
            "Batch 621 Loss 1.4597 Learning Rate 0.00131513\n",
            "Batch 631 Loss 2.9802 Learning Rate 0.00131368\n",
            "Batch 641 Loss 1.5465 Learning Rate 0.00131223\n",
            "Batch 651 Loss 1.4106 Learning Rate 0.00131079\n",
            "Batch 661 Loss 2.287 Learning Rate 0.00130935\n",
            "Batch 671 Loss 1.8807 Learning Rate 0.00130791\n",
            "Batch 681 Loss 1.4867 Learning Rate 0.00130649\n",
            "Batch 691 Loss 2.4672 Learning Rate 0.00130506\n",
            "Batch 701 Loss 1.4576 Learning Rate 0.00130364\n",
            "Batch 711 Loss 1.2877 Learning Rate 0.00130222\n",
            "Batch 721 Loss 1.4059 Learning Rate 0.00130081\n",
            "Batch 731 Loss 1.5575 Learning Rate 0.00129941\n",
            "Batch 741 Loss 1.8286 Learning Rate 0.00129801\n",
            "Batch 751 Loss 1.8069 Learning Rate 0.00129661\n",
            "Batch 761 Loss 2.1033 Learning Rate 0.00129522\n",
            "Batch 771 Loss 1.4751 Learning Rate 0.00129383\n",
            "Batch 0 validation loss: 0.771895170211792\n",
            "Batch 1 validation loss: 0.9579195380210876\n",
            "Batch 2 validation loss: 2.1608996391296387\n",
            "Batch 3 validation loss: 1.1484416723251343\n",
            "Batch 4 validation loss: 1.3220269680023193\n",
            "Batch 5 validation loss: 1.2726192474365234\n",
            "Batch 6 validation loss: 1.362770915031433\n",
            "Batch 7 validation loss: 1.4133344888687134\n",
            "Batch 8 validation loss: 1.5562883615493774\n",
            "Batch 9 validation loss: 1.5916187763214111\n",
            "Batch 10 validation loss: 1.6714141368865967\n",
            "Batch 11 validation loss: 1.9863847494125366\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 6 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.2042 Learning Rate 0.00129258\n",
            "Batch 11 Loss 0.997 Learning Rate 0.0012912\n",
            "Batch 21 Loss 1.2829 Learning Rate 0.00128983\n",
            "Batch 31 Loss 1.881 Learning Rate 0.00128845\n",
            "Batch 41 Loss 1.11 Learning Rate 0.00128709\n",
            "Batch 51 Loss 1.18 Learning Rate 0.00128573\n",
            "Batch 61 Loss 2.1686 Learning Rate 0.00128437\n",
            "Batch 71 Loss 1.7226 Learning Rate 0.00128301\n",
            "Batch 81 Loss 1.5873 Learning Rate 0.00128166\n",
            "Batch 91 Loss 1.4856 Learning Rate 0.00128032\n",
            "Batch 101 Loss 2.0408 Learning Rate 0.00127898\n",
            "Batch 111 Loss 1.453 Learning Rate 0.00127764\n",
            "Batch 121 Loss 2.1305 Learning Rate 0.00127631\n",
            "Batch 131 Loss 1.1077 Learning Rate 0.00127498\n",
            "Batch 141 Loss 1.3202 Learning Rate 0.00127365\n",
            "Batch 151 Loss 1.3422 Learning Rate 0.00127233\n",
            "Batch 161 Loss 1.3453 Learning Rate 0.00127102\n",
            "Batch 171 Loss 1.2377 Learning Rate 0.00126971\n",
            "Batch 181 Loss 1.1779 Learning Rate 0.0012684\n",
            "Batch 191 Loss 1.3769 Learning Rate 0.00126709\n",
            "Batch 201 Loss 1.3528 Learning Rate 0.00126579\n",
            "Batch 211 Loss 1.0389 Learning Rate 0.0012645\n",
            "Batch 221 Loss 1.2462 Learning Rate 0.00126321\n",
            "Batch 231 Loss 1.0978 Learning Rate 0.00126192\n",
            "Batch 241 Loss 1.1039 Learning Rate 0.00126063\n",
            "Batch 251 Loss 1.2977 Learning Rate 0.00125935\n",
            "Batch 261 Loss 1.2392 Learning Rate 0.00125808\n",
            "Batch 271 Loss 1.0002 Learning Rate 0.00125681\n",
            "Batch 281 Loss 1.3131 Learning Rate 0.00125554\n",
            "Batch 291 Loss 1.3196 Learning Rate 0.00125427\n",
            "Batch 301 Loss 1.1487 Learning Rate 0.00125301\n",
            "Batch 311 Loss 1.4084 Learning Rate 0.00125175\n",
            "Batch 321 Loss 1.4944 Learning Rate 0.0012505\n",
            "Batch 331 Loss 2.9533 Learning Rate 0.00124925\n",
            "Batch 341 Loss 1.3626 Learning Rate 0.001248\n",
            "Batch 351 Loss 1.4339 Learning Rate 0.00124676\n",
            "Batch 361 Loss 1.0616 Learning Rate 0.00124552\n",
            "Batch 371 Loss 1.1393 Learning Rate 0.00124429\n",
            "Batch 381 Loss 1.2433 Learning Rate 0.00124306\n",
            "Batch 391 Loss 1.2566 Learning Rate 0.00124183\n",
            "Batch 401 Loss 1.1504 Learning Rate 0.00124061\n",
            "Batch 411 Loss 1.3146 Learning Rate 0.00123939\n",
            "Batch 421 Loss 1.6741 Learning Rate 0.00123817\n",
            "Batch 431 Loss 1.5175 Learning Rate 0.00123696\n",
            "Batch 441 Loss 1.3749 Learning Rate 0.00123575\n",
            "Batch 451 Loss 0.9485 Learning Rate 0.00123454\n",
            "Batch 461 Loss 1.6385 Learning Rate 0.00123334\n",
            "Batch 471 Loss 1.9291 Learning Rate 0.00123214\n",
            "Batch 481 Loss 1.6404 Learning Rate 0.00123094\n",
            "Batch 491 Loss 1.4808 Learning Rate 0.00122975\n",
            "Batch 501 Loss 1.4129 Learning Rate 0.00122856\n",
            "Batch 511 Loss 1.59 Learning Rate 0.00122738\n",
            "Batch 521 Loss 1.418 Learning Rate 0.0012262\n",
            "Batch 531 Loss 1.1442 Learning Rate 0.00122502\n",
            "Batch 541 Loss 1.2874 Learning Rate 0.00122384\n",
            "Batch 551 Loss 1.7498 Learning Rate 0.00122267\n",
            "Batch 561 Loss 1.4609 Learning Rate 0.0012215\n",
            "Batch 571 Loss 1.5827 Learning Rate 0.00122034\n",
            "Batch 581 Loss 1.2512 Learning Rate 0.00121918\n",
            "Batch 591 Loss 1.4528 Learning Rate 0.00121802\n",
            "Batch 601 Loss 1.0073 Learning Rate 0.00121687\n",
            "Batch 611 Loss 1.1717 Learning Rate 0.00121571\n",
            "Batch 621 Loss 1.0785 Learning Rate 0.00121457\n",
            "Batch 631 Loss 1.401 Learning Rate 0.00121342\n",
            "Batch 641 Loss 1.1446 Learning Rate 0.00121228\n",
            "Batch 651 Loss 1.1801 Learning Rate 0.00121114\n",
            "Batch 661 Loss 2.3471 Learning Rate 0.00121\n",
            "Batch 671 Loss 1.3579 Learning Rate 0.00120887\n",
            "Batch 681 Loss 1.1801 Learning Rate 0.00120774\n",
            "Batch 691 Loss 1.5194 Learning Rate 0.00120662\n",
            "Batch 701 Loss 1.5909 Learning Rate 0.00120549\n",
            "Batch 711 Loss 1.151 Learning Rate 0.00120438\n",
            "Batch 721 Loss 1.283 Learning Rate 0.00120326\n",
            "Batch 731 Loss 1.3301 Learning Rate 0.00120215\n",
            "Batch 741 Loss 1.1289 Learning Rate 0.00120104\n",
            "Batch 751 Loss 1.4843 Learning Rate 0.00119993\n",
            "Batch 761 Loss 1.231 Learning Rate 0.00119882\n",
            "Batch 771 Loss 0.771 Learning Rate 0.00119772\n",
            "Batch 0 validation loss: 0.5957909822463989\n",
            "Batch 1 validation loss: 0.5849694609642029\n",
            "Batch 2 validation loss: 1.468867540359497\n",
            "Batch 3 validation loss: 0.878498911857605\n",
            "Batch 4 validation loss: 1.0219837427139282\n",
            "Batch 5 validation loss: 1.0299426317214966\n",
            "Batch 6 validation loss: 1.0869134664535522\n",
            "Batch 7 validation loss: 1.066518783569336\n",
            "Batch 8 validation loss: 1.1307533979415894\n",
            "Batch 9 validation loss: 1.2861406803131104\n",
            "Batch 10 validation loss: 1.4135735034942627\n",
            "Batch 11 validation loss: 1.6389715671539307\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 7 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.3348 Learning Rate 0.00119673\n",
            "Batch 11 Loss 1.3789 Learning Rate 0.00119564\n",
            "Batch 21 Loss 1.5047 Learning Rate 0.00119455\n",
            "Batch 31 Loss 0.7919 Learning Rate 0.00119346\n",
            "Batch 41 Loss 0.1567 Learning Rate 0.00119237\n",
            "Batch 51 Loss 1.1444 Learning Rate 0.00119129\n",
            "Batch 61 Loss 1.029 Learning Rate 0.00119021\n",
            "Batch 71 Loss 0.9958 Learning Rate 0.00118913\n",
            "Batch 81 Loss 0.8264 Learning Rate 0.00118805\n",
            "Batch 91 Loss 1.3539 Learning Rate 0.00118698\n",
            "Batch 101 Loss 1.0477 Learning Rate 0.00118591\n",
            "Batch 111 Loss 1.0957 Learning Rate 0.00118485\n",
            "Batch 121 Loss 1.1483 Learning Rate 0.00118378\n",
            "Batch 131 Loss 1.072 Learning Rate 0.00118272\n",
            "Batch 141 Loss 1.0149 Learning Rate 0.00118167\n",
            "Batch 151 Loss 0.948 Learning Rate 0.00118061\n",
            "Batch 161 Loss 1.0698 Learning Rate 0.00117956\n",
            "Batch 171 Loss 0.9812 Learning Rate 0.00117851\n",
            "Batch 181 Loss 1.1433 Learning Rate 0.00117747\n",
            "Batch 191 Loss 1.0624 Learning Rate 0.00117642\n",
            "Batch 201 Loss 0.9024 Learning Rate 0.00117538\n",
            "Batch 211 Loss 1.1218 Learning Rate 0.00117434\n",
            "Batch 221 Loss 1.1474 Learning Rate 0.00117331\n",
            "Batch 231 Loss 1.1963 Learning Rate 0.00117228\n",
            "Batch 241 Loss 1.2948 Learning Rate 0.00117125\n",
            "Batch 251 Loss 0.7759 Learning Rate 0.00117022\n",
            "Batch 261 Loss 1.2233 Learning Rate 0.00116919\n",
            "Batch 271 Loss 1.2139 Learning Rate 0.00116817\n",
            "Batch 281 Loss 0.8995 Learning Rate 0.00116715\n",
            "Batch 291 Loss 0.9132 Learning Rate 0.00116614\n",
            "Batch 301 Loss 1.4719 Learning Rate 0.00116512\n",
            "Batch 311 Loss 0.9571 Learning Rate 0.00116411\n",
            "Batch 321 Loss 1.0129 Learning Rate 0.00116311\n",
            "Batch 331 Loss 1.1634 Learning Rate 0.0011621\n",
            "Batch 341 Loss 1.046 Learning Rate 0.0011611\n",
            "Batch 351 Loss 0.8647 Learning Rate 0.0011601\n",
            "Batch 361 Loss 1.2113 Learning Rate 0.0011591\n",
            "Batch 371 Loss 1.0326 Learning Rate 0.0011581\n",
            "Batch 381 Loss 0.9868 Learning Rate 0.00115711\n",
            "Batch 391 Loss 0.9749 Learning Rate 0.00115612\n",
            "Batch 401 Loss 0.9328 Learning Rate 0.00115513\n",
            "Batch 411 Loss 1.0741 Learning Rate 0.00115415\n",
            "Batch 421 Loss 1.0501 Learning Rate 0.00115316\n",
            "Batch 431 Loss 1.0478 Learning Rate 0.00115218\n",
            "Batch 441 Loss 1.2588 Learning Rate 0.00115121\n",
            "Batch 451 Loss 1.1306 Learning Rate 0.00115023\n",
            "Batch 461 Loss 1.1708 Learning Rate 0.00114926\n",
            "Batch 471 Loss 1.2275 Learning Rate 0.00114829\n",
            "Batch 481 Loss 0.7674 Learning Rate 0.00114732\n",
            "Batch 491 Loss 1.378 Learning Rate 0.00114635\n",
            "Batch 501 Loss 1.0591 Learning Rate 0.00114539\n",
            "Batch 511 Loss 0.8448 Learning Rate 0.00114443\n",
            "Batch 521 Loss 1.0166 Learning Rate 0.00114347\n",
            "Batch 531 Loss 0.9575 Learning Rate 0.00114252\n",
            "Batch 541 Loss 0.8299 Learning Rate 0.00114156\n",
            "Batch 551 Loss 1.5634 Learning Rate 0.00114061\n",
            "Batch 561 Loss 1.0347 Learning Rate 0.00113966\n",
            "Batch 571 Loss 0.9128 Learning Rate 0.00113872\n",
            "Batch 581 Loss 1.0262 Learning Rate 0.00113777\n",
            "Batch 591 Loss 1.108 Learning Rate 0.00113683\n",
            "Batch 601 Loss 1.2754 Learning Rate 0.00113589\n",
            "Batch 611 Loss 1.0306 Learning Rate 0.00113496\n",
            "Batch 621 Loss 1.3356 Learning Rate 0.00113402\n",
            "Batch 631 Loss 1.155 Learning Rate 0.00113309\n",
            "Batch 641 Loss 1.4089 Learning Rate 0.00113216\n",
            "Batch 651 Loss 1.0128 Learning Rate 0.00113123\n",
            "Batch 661 Loss 1.0942 Learning Rate 0.00113031\n",
            "Batch 671 Loss 1.0805 Learning Rate 0.00112938\n",
            "Batch 681 Loss 1.2986 Learning Rate 0.00112846\n",
            "Batch 691 Loss 1.547 Learning Rate 0.00112755\n",
            "Batch 701 Loss 1.3759 Learning Rate 0.00112663\n",
            "Batch 711 Loss 1.1481 Learning Rate 0.00112572\n",
            "Batch 721 Loss 1.3408 Learning Rate 0.0011248\n",
            "Batch 731 Loss 0.887 Learning Rate 0.00112389\n",
            "Batch 741 Loss 1.0402 Learning Rate 0.00112299\n",
            "Batch 751 Loss 1.5297 Learning Rate 0.00112208\n",
            "Batch 761 Loss 1.2573 Learning Rate 0.00112118\n",
            "Batch 771 Loss 0.9933 Learning Rate 0.00112028\n",
            "Batch 0 validation loss: 0.49609193205833435\n",
            "Batch 1 validation loss: 0.45280373096466064\n",
            "Batch 2 validation loss: 1.5794904232025146\n",
            "Batch 3 validation loss: 0.7239348888397217\n",
            "Batch 4 validation loss: 0.793522834777832\n",
            "Batch 5 validation loss: 0.8205953240394592\n",
            "Batch 6 validation loss: 0.8883747458457947\n",
            "Batch 7 validation loss: 0.8438401222229004\n",
            "Batch 8 validation loss: 0.9048138856887817\n",
            "Batch 9 validation loss: 0.9451170563697815\n",
            "Batch 10 validation loss: 0.9925335645675659\n",
            "Batch 11 validation loss: 1.0980970859527588\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 8 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 2.1222 Learning Rate 0.00111947\n",
            "Batch 11 Loss 0.8483 Learning Rate 0.00111857\n",
            "Batch 21 Loss 1.1003 Learning Rate 0.00111768\n",
            "Batch 31 Loss 0.7721 Learning Rate 0.00111678\n",
            "Batch 41 Loss 0.9733 Learning Rate 0.00111589\n",
            "Batch 51 Loss 1.0648 Learning Rate 0.00111501\n",
            "Batch 61 Loss 0.9155 Learning Rate 0.00111412\n",
            "Batch 71 Loss 0.6605 Learning Rate 0.00111324\n",
            "Batch 81 Loss 0.769 Learning Rate 0.00111235\n",
            "Batch 91 Loss 0.9981 Learning Rate 0.00111147\n",
            "Batch 101 Loss 0.8458 Learning Rate 0.0011106\n",
            "Batch 111 Loss 0.7927 Learning Rate 0.00110972\n",
            "Batch 121 Loss 0.7721 Learning Rate 0.00110885\n",
            "Batch 131 Loss 0.9482 Learning Rate 0.00110797\n",
            "Batch 141 Loss 0.8482 Learning Rate 0.00110711\n",
            "Batch 151 Loss 0.7614 Learning Rate 0.00110624\n",
            "Batch 161 Loss 0.7232 Learning Rate 0.00110537\n",
            "Batch 171 Loss 0.9658 Learning Rate 0.00110451\n",
            "Batch 181 Loss 0.1568 Learning Rate 0.00110365\n",
            "Batch 191 Loss 0.8848 Learning Rate 0.00110279\n",
            "Batch 201 Loss 0.853 Learning Rate 0.00110193\n",
            "Batch 211 Loss 0.9697 Learning Rate 0.00110108\n",
            "Batch 221 Loss 0.851 Learning Rate 0.00110022\n",
            "Batch 231 Loss 0.2348 Learning Rate 0.00109937\n",
            "Batch 241 Loss 0.8283 Learning Rate 0.00109852\n",
            "Batch 251 Loss 0.9895 Learning Rate 0.00109767\n",
            "Batch 261 Loss 0.8409 Learning Rate 0.00109683\n",
            "Batch 271 Loss 0.8953 Learning Rate 0.00109599\n",
            "Batch 281 Loss 1.0782 Learning Rate 0.00109514\n",
            "Batch 291 Loss 0.8437 Learning Rate 0.0010943\n",
            "Batch 301 Loss 1.0188 Learning Rate 0.00109347\n",
            "Batch 311 Loss 0.8543 Learning Rate 0.00109263\n",
            "Batch 321 Loss 0.8347 Learning Rate 0.0010918\n",
            "Batch 331 Loss 0.8645 Learning Rate 0.00109096\n",
            "Batch 341 Loss 0.9471 Learning Rate 0.00109013\n",
            "Batch 351 Loss 0.7951 Learning Rate 0.00108931\n",
            "Batch 361 Loss 0.9319 Learning Rate 0.00108848\n",
            "Batch 371 Loss 1.0586 Learning Rate 0.00108766\n",
            "Batch 381 Loss 1.1796 Learning Rate 0.00108683\n",
            "Batch 391 Loss 0.969 Learning Rate 0.00108601\n",
            "Batch 401 Loss 1.073 Learning Rate 0.00108519\n",
            "Batch 411 Loss 0.8585 Learning Rate 0.00108438\n",
            "Batch 421 Loss 0.8452 Learning Rate 0.00108356\n",
            "Batch 431 Loss 0.8596 Learning Rate 0.00108275\n",
            "Batch 441 Loss 0.8803 Learning Rate 0.00108194\n",
            "Batch 451 Loss 0.7968 Learning Rate 0.00108113\n",
            "Batch 461 Loss 0.954 Learning Rate 0.00108032\n",
            "Batch 471 Loss 0.836 Learning Rate 0.00107951\n",
            "Batch 481 Loss 1.03 Learning Rate 0.00107871\n",
            "Batch 491 Loss 1.5999 Learning Rate 0.00107791\n",
            "Batch 501 Loss 1.0565 Learning Rate 0.00107711\n",
            "Batch 511 Loss 0.889 Learning Rate 0.00107631\n",
            "Batch 521 Loss 0.8727 Learning Rate 0.00107551\n",
            "Batch 531 Loss 0.722 Learning Rate 0.00107471\n",
            "Batch 541 Loss 1.0891 Learning Rate 0.00107392\n",
            "Batch 551 Loss 1.0671 Learning Rate 0.00107313\n",
            "Batch 561 Loss 0.7285 Learning Rate 0.00107234\n",
            "Batch 571 Loss 1.1274 Learning Rate 0.00107155\n",
            "Batch 581 Loss 1.0335 Learning Rate 0.00107076\n",
            "Batch 591 Loss 1.2312 Learning Rate 0.00106998\n",
            "Batch 601 Loss 0.9295 Learning Rate 0.0010692\n",
            "Batch 611 Loss 0.8443 Learning Rate 0.00106842\n",
            "Batch 621 Loss 0.9514 Learning Rate 0.00106764\n",
            "Batch 631 Loss 0.9631 Learning Rate 0.00106686\n",
            "Batch 641 Loss 0.8496 Learning Rate 0.00106608\n",
            "Batch 651 Loss 0.9851 Learning Rate 0.00106531\n",
            "Batch 661 Loss 1.0833 Learning Rate 0.00106453\n",
            "Batch 671 Loss 0.9816 Learning Rate 0.00106376\n",
            "Batch 681 Loss 0.9529 Learning Rate 0.00106299\n",
            "Batch 691 Loss 0.848 Learning Rate 0.00106222\n",
            "Batch 701 Loss 1.0671 Learning Rate 0.00106146\n",
            "Batch 711 Loss 0.7804 Learning Rate 0.00106069\n",
            "Batch 721 Loss 2.189 Learning Rate 0.00105993\n",
            "Batch 731 Loss 0.9313 Learning Rate 0.00105917\n",
            "Batch 741 Loss 0.9726 Learning Rate 0.00105841\n",
            "Batch 751 Loss 0.6978 Learning Rate 0.00105765\n",
            "Batch 761 Loss 1.0418 Learning Rate 0.0010569\n",
            "Batch 771 Loss 0.9673 Learning Rate 0.00105614\n",
            "Batch 0 validation loss: 0.49507075548171997\n",
            "Batch 1 validation loss: 0.33863165974617004\n",
            "Batch 2 validation loss: 1.149372935295105\n",
            "Batch 3 validation loss: 0.624294102191925\n",
            "Batch 4 validation loss: 0.6409701108932495\n",
            "Batch 5 validation loss: 0.6771601438522339\n",
            "Batch 6 validation loss: 0.7530499696731567\n",
            "Batch 7 validation loss: 0.6875689625740051\n",
            "Batch 8 validation loss: 0.7359377145767212\n",
            "Batch 9 validation loss: 0.7600632905960083\n",
            "Batch 10 validation loss: 0.8017950057983398\n",
            "Batch 11 validation loss: 0.9258954524993896\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 9 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.4556 Learning Rate 0.00105546\n",
            "Batch 11 Loss 0.7944 Learning Rate 0.00105471\n",
            "Batch 21 Loss 0.9916 Learning Rate 0.00105396\n",
            "Batch 31 Loss 0.606 Learning Rate 0.00105321\n",
            "Batch 41 Loss 1.0668 Learning Rate 0.00105247\n",
            "Batch 51 Loss 0.6275 Learning Rate 0.00105172\n",
            "Batch 61 Loss 0.8294 Learning Rate 0.00105098\n",
            "Batch 71 Loss 0.6066 Learning Rate 0.00105023\n",
            "Batch 81 Loss 0.8009 Learning Rate 0.00104949\n",
            "Batch 91 Loss 0.6477 Learning Rate 0.00104876\n",
            "Batch 101 Loss 0.8153 Learning Rate 0.00104802\n",
            "Batch 111 Loss 0.8675 Learning Rate 0.00104728\n",
            "Batch 121 Loss 0.7992 Learning Rate 0.00104655\n",
            "Batch 131 Loss 0.6803 Learning Rate 0.00104581\n",
            "Batch 141 Loss 0.6462 Learning Rate 0.00104508\n",
            "Batch 151 Loss 0.8141 Learning Rate 0.00104435\n",
            "Batch 161 Loss 0.7493 Learning Rate 0.00104363\n",
            "Batch 171 Loss 0.9211 Learning Rate 0.0010429\n",
            "Batch 181 Loss 0.8497 Learning Rate 0.00104217\n",
            "Batch 191 Loss 0.6833 Learning Rate 0.00104145\n",
            "Batch 201 Loss 0.8319 Learning Rate 0.00104073\n",
            "Batch 211 Loss 0.7682 Learning Rate 0.00104001\n",
            "Batch 221 Loss 0.7972 Learning Rate 0.00103929\n",
            "Batch 231 Loss 0.6839 Learning Rate 0.00103857\n",
            "Batch 241 Loss 0.6718 Learning Rate 0.00103785\n",
            "Batch 251 Loss 0.693 Learning Rate 0.00103714\n",
            "Batch 261 Loss 0.8399 Learning Rate 0.00103643\n",
            "Batch 271 Loss 0.2323 Learning Rate 0.00103571\n",
            "Batch 281 Loss 0.7284 Learning Rate 0.001035\n",
            "Batch 291 Loss 0.827 Learning Rate 0.00103429\n",
            "Batch 301 Loss 0.9382 Learning Rate 0.00103359\n",
            "Batch 311 Loss 0.8619 Learning Rate 0.00103288\n",
            "Batch 321 Loss 0.7763 Learning Rate 0.00103218\n",
            "Batch 331 Loss 0.7108 Learning Rate 0.00103147\n",
            "Batch 341 Loss 0.7027 Learning Rate 0.00103077\n",
            "Batch 351 Loss 0.747 Learning Rate 0.00103007\n",
            "Batch 361 Loss 1.4776 Learning Rate 0.00102937\n",
            "Batch 371 Loss 0.7413 Learning Rate 0.00102868\n",
            "Batch 381 Loss 0.8127 Learning Rate 0.00102798\n",
            "Batch 391 Loss 0.6648 Learning Rate 0.00102729\n",
            "Batch 401 Loss 0.8689 Learning Rate 0.00102659\n",
            "Batch 411 Loss 0.8797 Learning Rate 0.0010259\n",
            "Batch 421 Loss 0.7777 Learning Rate 0.00102521\n",
            "Batch 431 Loss 1.0681 Learning Rate 0.00102452\n",
            "Batch 441 Loss 0.7882 Learning Rate 0.00102383\n",
            "Batch 451 Loss 0.8552 Learning Rate 0.00102315\n",
            "Batch 461 Loss 0.8519 Learning Rate 0.00102246\n",
            "Batch 471 Loss 0.9438 Learning Rate 0.00102178\n",
            "Batch 481 Loss 0.7253 Learning Rate 0.0010211\n",
            "Batch 491 Loss 0.6537 Learning Rate 0.00102042\n",
            "Batch 501 Loss 1.0932 Learning Rate 0.00101974\n",
            "Batch 511 Loss 0.8686 Learning Rate 0.00101906\n",
            "Batch 521 Loss 0.7547 Learning Rate 0.00101838\n",
            "Batch 531 Loss 0.6643 Learning Rate 0.00101771\n",
            "Batch 541 Loss 1.0256 Learning Rate 0.00101703\n",
            "Batch 551 Loss 0.8832 Learning Rate 0.00101636\n",
            "Batch 561 Loss 0.8597 Learning Rate 0.00101569\n",
            "Batch 571 Loss 0.7423 Learning Rate 0.00101502\n",
            "Batch 581 Loss 0.8281 Learning Rate 0.00101435\n",
            "Batch 591 Loss 1.0029 Learning Rate 0.00101368\n",
            "Batch 601 Loss 0.9516 Learning Rate 0.00101302\n",
            "Batch 611 Loss 0.7362 Learning Rate 0.00101235\n",
            "Batch 621 Loss 0.7553 Learning Rate 0.00101169\n",
            "Batch 631 Loss 0.817 Learning Rate 0.00101103\n",
            "Batch 641 Loss 0.7004 Learning Rate 0.00101037\n",
            "Batch 651 Loss 0.6582 Learning Rate 0.00100971\n",
            "Batch 661 Loss 0.7179 Learning Rate 0.00100905\n",
            "Batch 671 Loss 0.7725 Learning Rate 0.00100839\n",
            "Batch 681 Loss 0.8215 Learning Rate 0.00100774\n",
            "Batch 691 Loss 0.6685 Learning Rate 0.00100708\n",
            "Batch 701 Loss 0.8122 Learning Rate 0.00100643\n",
            "Batch 711 Loss 0.8601 Learning Rate 0.00100578\n",
            "Batch 721 Loss 0.9945 Learning Rate 0.00100513\n",
            "Batch 731 Loss 0.7791 Learning Rate 0.00100448\n",
            "Batch 741 Loss 0.8007 Learning Rate 0.00100383\n",
            "Batch 751 Loss 0.7371 Learning Rate 0.00100318\n",
            "Batch 761 Loss 0.7084 Learning Rate 0.00100254\n",
            "Batch 771 Loss 1.0582 Learning Rate 0.00100189\n",
            "Batch 0 validation loss: 0.41167861223220825\n",
            "Batch 1 validation loss: 0.2983594834804535\n",
            "Batch 2 validation loss: 0.8374196887016296\n",
            "Batch 3 validation loss: 0.5323037505149841\n",
            "Batch 4 validation loss: 0.5935074687004089\n",
            "Batch 5 validation loss: 0.5565163493156433\n",
            "Batch 6 validation loss: 0.6035257577896118\n",
            "Batch 7 validation loss: 0.5838878154754639\n",
            "Batch 8 validation loss: 0.5804020166397095\n",
            "Batch 9 validation loss: 0.6301535367965698\n",
            "Batch 10 validation loss: 0.6226232051849365\n",
            "Batch 11 validation loss: 0.6997391581535339\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 10 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.5741 Learning Rate 0.00100131\n",
            "Batch 11 Loss 0.6159 Learning Rate 0.00100067\n",
            "Batch 21 Loss 0.6398 Learning Rate 0.00100003\n",
            "Batch 31 Loss 0.5382 Learning Rate 0.00099939\n",
            "Batch 41 Loss 0.5692 Learning Rate 0.00099875\n",
            "Batch 51 Loss 0.6936 Learning Rate 0.00099812\n",
            "Batch 61 Loss 0.465 Learning Rate 0.00099748\n",
            "Batch 71 Loss 0.5254 Learning Rate 0.00099685\n",
            "Batch 81 Loss 0.6358 Learning Rate 0.00099621\n",
            "Batch 91 Loss 0.6773 Learning Rate 0.00099558\n",
            "Batch 101 Loss 0.7266 Learning Rate 0.00099495\n",
            "Batch 111 Loss 0.714 Learning Rate 0.00099432\n",
            "Batch 121 Loss 0.6175 Learning Rate 0.00099369\n",
            "Batch 131 Loss 0.807 Learning Rate 0.00099306\n",
            "Batch 141 Loss 0.7048 Learning Rate 0.00099244\n",
            "Batch 151 Loss 0.7807 Learning Rate 0.00099181\n",
            "Batch 161 Loss 0.6031 Learning Rate 0.00099119\n",
            "Batch 171 Loss 0.5917 Learning Rate 0.00099057\n",
            "Batch 181 Loss 0.6766 Learning Rate 0.00098995\n",
            "Batch 191 Loss 0.65 Learning Rate 0.00098933\n",
            "Batch 201 Loss 0.5348 Learning Rate 0.00098871\n",
            "Batch 211 Loss 0.5945 Learning Rate 0.00098809\n",
            "Batch 221 Loss 0.7132 Learning Rate 0.00098747\n",
            "Batch 231 Loss 0.7844 Learning Rate 0.00098686\n",
            "Batch 241 Loss 0.7871 Learning Rate 0.00098624\n",
            "Batch 251 Loss 0.6675 Learning Rate 0.00098563\n",
            "Batch 261 Loss 0.6073 Learning Rate 0.00098502\n",
            "Batch 271 Loss 0.6106 Learning Rate 0.0009844\n",
            "Batch 281 Loss 0.6503 Learning Rate 0.00098379\n",
            "Batch 291 Loss 0.7137 Learning Rate 0.00098319\n",
            "Batch 301 Loss 0.5805 Learning Rate 0.00098258\n",
            "Batch 311 Loss 0.596 Learning Rate 0.00098197\n",
            "Batch 321 Loss 0.652 Learning Rate 0.00098137\n",
            "Batch 331 Loss 0.614 Learning Rate 0.00098076\n",
            "Batch 341 Loss 0.5458 Learning Rate 0.00098016\n",
            "Batch 351 Loss 0.6321 Learning Rate 0.00097956\n",
            "Batch 361 Loss 0.5981 Learning Rate 0.00097896\n",
            "Batch 371 Loss 0.9067 Learning Rate 0.00097836\n",
            "Batch 381 Loss 0.6997 Learning Rate 0.00097776\n",
            "Batch 391 Loss 0.6102 Learning Rate 0.00097716\n",
            "Batch 401 Loss 0.7123 Learning Rate 0.00097656\n",
            "Batch 411 Loss 0.8438 Learning Rate 0.00097597\n",
            "Batch 421 Loss 0.8437 Learning Rate 0.00097537\n",
            "Batch 431 Loss 0.5627 Learning Rate 0.00097478\n",
            "Batch 441 Loss 0.8096 Learning Rate 0.00097419\n",
            "Batch 451 Loss 0.7149 Learning Rate 0.0009736\n",
            "Batch 461 Loss 0.6059 Learning Rate 0.00097301\n",
            "Batch 471 Loss 0.6958 Learning Rate 0.00097242\n",
            "Batch 481 Loss 0.6774 Learning Rate 0.00097183\n",
            "Batch 491 Loss 0.6712 Learning Rate 0.00097124\n",
            "Batch 501 Loss 0.7024 Learning Rate 0.00097066\n",
            "Batch 511 Loss 0.7906 Learning Rate 0.00097007\n",
            "Batch 521 Loss 0.7281 Learning Rate 0.00096949\n",
            "Batch 531 Loss 0.7788 Learning Rate 0.0009689\n",
            "Batch 541 Loss 0.7724 Learning Rate 0.00096832\n",
            "Batch 551 Loss 0.2004 Learning Rate 0.00096774\n",
            "Batch 561 Loss 0.7336 Learning Rate 0.00096716\n",
            "Batch 571 Loss 0.6871 Learning Rate 0.00096658\n",
            "Batch 581 Loss 0.6664 Learning Rate 0.00096601\n",
            "Batch 591 Loss 2.1823 Learning Rate 0.00096543\n",
            "Batch 601 Loss 0.8439 Learning Rate 0.00096486\n",
            "Batch 611 Loss 0.6342 Learning Rate 0.00096428\n",
            "Batch 621 Loss 0.6157 Learning Rate 0.00096371\n",
            "Batch 631 Loss 0.6227 Learning Rate 0.00096314\n",
            "Batch 641 Loss 0.6706 Learning Rate 0.00096256\n",
            "Batch 651 Loss 0.885 Learning Rate 0.00096199\n",
            "Batch 661 Loss 0.7318 Learning Rate 0.00096142\n",
            "Batch 671 Loss 0.6615 Learning Rate 0.00096086\n",
            "Batch 681 Loss 0.8586 Learning Rate 0.00096029\n",
            "Batch 691 Loss 0.6575 Learning Rate 0.00095972\n",
            "Batch 701 Loss 0.8763 Learning Rate 0.00095916\n",
            "Batch 711 Loss 0.7389 Learning Rate 0.00095859\n",
            "Batch 721 Loss 0.6217 Learning Rate 0.00095803\n",
            "Batch 731 Loss 0.6847 Learning Rate 0.00095747\n",
            "Batch 741 Loss 1.409 Learning Rate 0.00095691\n",
            "Batch 751 Loss 0.6978 Learning Rate 0.00095635\n",
            "Batch 761 Loss 0.7053 Learning Rate 0.00095579\n",
            "Batch 771 Loss 0.9104 Learning Rate 0.00095523\n",
            "Batch 0 validation loss: 0.3783020079135895\n",
            "Batch 1 validation loss: 0.30779653787612915\n",
            "Batch 2 validation loss: 1.1663265228271484\n",
            "Batch 3 validation loss: 0.42135104537010193\n",
            "Batch 4 validation loss: 0.5088921785354614\n",
            "Batch 5 validation loss: 0.4800536036491394\n",
            "Batch 6 validation loss: 0.5218664407730103\n",
            "Batch 7 validation loss: 0.5070340037345886\n",
            "Batch 8 validation loss: 0.5456284880638123\n",
            "Batch 9 validation loss: 0.5203667283058167\n",
            "Batch 10 validation loss: 0.5268518924713135\n",
            "Batch 11 validation loss: 0.6890150308609009\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 11 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.5319 Learning Rate 0.00095473\n",
            "Batch 11 Loss 0.5214 Learning Rate 0.00095417\n",
            "Batch 21 Loss 0.6101 Learning Rate 0.00095362\n",
            "Batch 31 Loss 1.7648 Learning Rate 0.00095306\n",
            "Batch 41 Loss 0.9781 Learning Rate 0.00095251\n",
            "Batch 51 Loss 0.5588 Learning Rate 0.00095195\n",
            "Batch 61 Loss 0.4131 Learning Rate 0.0009514\n",
            "Batch 71 Loss 0.5223 Learning Rate 0.00095085\n",
            "Batch 81 Loss 0.4511 Learning Rate 0.0009503\n",
            "Batch 91 Loss 0.6148 Learning Rate 0.00094975\n",
            "Batch 101 Loss 0.5171 Learning Rate 0.00094921\n",
            "Batch 111 Loss 0.4969 Learning Rate 0.00094866\n",
            "Batch 121 Loss 0.5674 Learning Rate 0.00094811\n",
            "Batch 131 Loss 0.52 Learning Rate 0.00094757\n",
            "Batch 141 Loss 0.6174 Learning Rate 0.00094702\n",
            "Batch 151 Loss 0.4377 Learning Rate 0.00094648\n",
            "Batch 161 Loss 0.4805 Learning Rate 0.00094594\n",
            "Batch 171 Loss 0.6198 Learning Rate 0.0009454\n",
            "Batch 181 Loss 0.7075 Learning Rate 0.00094486\n",
            "Batch 191 Loss 0.619 Learning Rate 0.00094432\n",
            "Batch 201 Loss 1.1901 Learning Rate 0.00094378\n",
            "Batch 211 Loss 0.5828 Learning Rate 0.00094324\n",
            "Batch 221 Loss 0.5324 Learning Rate 0.00094271\n",
            "Batch 231 Loss 0.5544 Learning Rate 0.00094217\n",
            "Batch 241 Loss 0.6682 Learning Rate 0.00094163\n",
            "Batch 251 Loss 0.7275 Learning Rate 0.0009411\n",
            "Batch 261 Loss 0.572 Learning Rate 0.00094057\n",
            "Batch 271 Loss 0.7559 Learning Rate 0.00094004\n",
            "Batch 281 Loss 0.6264 Learning Rate 0.0009395\n",
            "Batch 291 Loss 0.5323 Learning Rate 0.00093897\n",
            "Batch 301 Loss 0.6606 Learning Rate 0.00093844\n",
            "Batch 311 Loss 0.478 Learning Rate 0.00093792\n",
            "Batch 321 Loss 0.5329 Learning Rate 0.00093739\n",
            "Batch 331 Loss 0.6786 Learning Rate 0.00093686\n",
            "Batch 341 Loss 0.6004 Learning Rate 0.00093634\n",
            "Batch 351 Loss 0.548 Learning Rate 0.00093581\n",
            "Batch 361 Loss 0.4892 Learning Rate 0.00093529\n",
            "Batch 371 Loss 0.5269 Learning Rate 0.00093476\n",
            "Batch 381 Loss 0.5415 Learning Rate 0.00093424\n",
            "Batch 391 Loss 0.6116 Learning Rate 0.00093372\n",
            "Batch 401 Loss 0.5648 Learning Rate 0.0009332\n",
            "Batch 411 Loss 0.6032 Learning Rate 0.00093268\n",
            "Batch 421 Loss 0.6673 Learning Rate 0.00093216\n",
            "Batch 431 Loss 0.6278 Learning Rate 0.00093164\n",
            "Batch 441 Loss 0.7103 Learning Rate 0.00093113\n",
            "Batch 451 Loss 0.8901 Learning Rate 0.00093061\n",
            "Batch 461 Loss 0.6056 Learning Rate 0.00093009\n",
            "Batch 471 Loss 0.6079 Learning Rate 0.00092958\n",
            "Batch 481 Loss 0.5817 Learning Rate 0.00092907\n",
            "Batch 491 Loss 0.5536 Learning Rate 0.00092855\n",
            "Batch 501 Loss 0.6058 Learning Rate 0.00092804\n",
            "Batch 511 Loss 0.594 Learning Rate 0.00092753\n",
            "Batch 521 Loss 0.6392 Learning Rate 0.00092702\n",
            "Batch 531 Loss 0.6956 Learning Rate 0.00092651\n",
            "Batch 541 Loss 0.5898 Learning Rate 0.000926\n",
            "Batch 551 Loss 0.5325 Learning Rate 0.00092549\n",
            "Batch 561 Loss 0.6466 Learning Rate 0.00092499\n",
            "Batch 571 Loss 1.0659 Learning Rate 0.00092448\n",
            "Batch 581 Loss 0.5911 Learning Rate 0.00092398\n",
            "Batch 591 Loss 0.5304 Learning Rate 0.00092347\n",
            "Batch 601 Loss 0.6713 Learning Rate 0.00092297\n",
            "Batch 611 Loss 0.6231 Learning Rate 0.00092247\n",
            "Batch 621 Loss 0.6436 Learning Rate 0.00092196\n",
            "Batch 631 Loss 0.6166 Learning Rate 0.00092146\n",
            "Batch 641 Loss 0.5661 Learning Rate 0.00092096\n",
            "Batch 651 Loss 0.5509 Learning Rate 0.00092046\n",
            "Batch 661 Loss 0.6609 Learning Rate 0.00091996\n",
            "Batch 671 Loss 0.6618 Learning Rate 0.00091947\n",
            "Batch 681 Loss 0.5995 Learning Rate 0.00091897\n",
            "Batch 691 Loss 0.6058 Learning Rate 0.00091847\n",
            "Batch 701 Loss 0.6492 Learning Rate 0.00091798\n",
            "Batch 711 Loss 0.6656 Learning Rate 0.00091748\n",
            "Batch 721 Loss 0.6934 Learning Rate 0.00091699\n",
            "Batch 731 Loss 0.614 Learning Rate 0.0009165\n",
            "Batch 741 Loss 0.6483 Learning Rate 0.000916\n",
            "Batch 751 Loss 0.6587 Learning Rate 0.00091551\n",
            "Batch 761 Loss 0.7192 Learning Rate 0.00091502\n",
            "Batch 771 Loss 0.6039 Learning Rate 0.00091453\n",
            "Batch 0 validation loss: 0.3540763258934021\n",
            "Batch 1 validation loss: 0.2609553039073944\n",
            "Batch 2 validation loss: 1.1096469163894653\n",
            "Batch 3 validation loss: 0.37335851788520813\n",
            "Batch 4 validation loss: 0.45186087489128113\n",
            "Batch 5 validation loss: 0.4250459372997284\n",
            "Batch 6 validation loss: 0.46988359093666077\n",
            "Batch 7 validation loss: 0.43405982851982117\n",
            "Batch 8 validation loss: 0.45413485169410706\n",
            "Batch 9 validation loss: 0.4897821545600891\n",
            "Batch 10 validation loss: 0.43778595328330994\n",
            "Batch 11 validation loss: 0.6731825470924377\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 12 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.4344 Learning Rate 0.00091409\n",
            "Batch 11 Loss 0.4655 Learning Rate 0.0009136\n",
            "Batch 21 Loss 0.4826 Learning Rate 0.00091311\n",
            "Batch 31 Loss 0.431 Learning Rate 0.00091263\n",
            "Batch 41 Loss 0.4422 Learning Rate 0.00091214\n",
            "Batch 51 Loss 0.4626 Learning Rate 0.00091166\n",
            "Batch 61 Loss 0.4416 Learning Rate 0.00091117\n",
            "Batch 71 Loss 0.4261 Learning Rate 0.00091069\n",
            "Batch 81 Loss 0.5173 Learning Rate 0.0009102\n",
            "Batch 91 Loss 0.5473 Learning Rate 0.00090972\n",
            "Batch 101 Loss 0.4142 Learning Rate 0.00090924\n",
            "Batch 111 Loss 0.4116 Learning Rate 0.00090876\n",
            "Batch 121 Loss 0.6637 Learning Rate 0.00090828\n",
            "Batch 131 Loss 0.526 Learning Rate 0.0009078\n",
            "Batch 141 Loss 0.4182 Learning Rate 0.00090732\n",
            "Batch 151 Loss 1.2528 Learning Rate 0.00090685\n",
            "Batch 161 Loss 0.4759 Learning Rate 0.00090637\n",
            "Batch 171 Loss 0.484 Learning Rate 0.00090589\n",
            "Batch 181 Loss 0.5578 Learning Rate 0.00090542\n",
            "Batch 191 Loss 0.4999 Learning Rate 0.00090494\n",
            "Batch 201 Loss 0.4137 Learning Rate 0.00090447\n",
            "Batch 211 Loss 0.4794 Learning Rate 0.000904\n",
            "Batch 221 Loss 0.6255 Learning Rate 0.00090352\n",
            "Batch 231 Loss 0.5101 Learning Rate 0.00090305\n",
            "Batch 241 Loss 0.6319 Learning Rate 0.00090258\n",
            "Batch 251 Loss 0.5387 Learning Rate 0.00090211\n",
            "Batch 261 Loss 0.5644 Learning Rate 0.00090164\n",
            "Batch 271 Loss 0.5669 Learning Rate 0.00090117\n",
            "Batch 281 Loss 0.6265 Learning Rate 0.0009007\n",
            "Batch 291 Loss 0.4599 Learning Rate 0.00090024\n",
            "Batch 301 Loss 0.5002 Learning Rate 0.00089977\n",
            "Batch 311 Loss 0.4214 Learning Rate 0.0008993\n",
            "Batch 321 Loss 0.572 Learning Rate 0.00089884\n",
            "Batch 331 Loss 0.6066 Learning Rate 0.00089837\n",
            "Batch 341 Loss 0.5486 Learning Rate 0.00089791\n",
            "Batch 351 Loss 0.513 Learning Rate 0.00089745\n",
            "Batch 361 Loss 0.5067 Learning Rate 0.00089699\n",
            "Batch 371 Loss 1.3236 Learning Rate 0.00089652\n",
            "Batch 381 Loss 0.4752 Learning Rate 0.00089606\n",
            "Batch 391 Loss 0.5373 Learning Rate 0.0008956\n",
            "Batch 401 Loss 0.478 Learning Rate 0.00089514\n",
            "Batch 411 Loss 0.5392 Learning Rate 0.00089468\n",
            "Batch 421 Loss 0.5261 Learning Rate 0.00089423\n",
            "Batch 431 Loss 0.5327 Learning Rate 0.00089377\n",
            "Batch 441 Loss 0.4782 Learning Rate 0.00089331\n",
            "Batch 451 Loss 0.5255 Learning Rate 0.00089286\n",
            "Batch 461 Loss 0.6298 Learning Rate 0.0008924\n",
            "Batch 471 Loss 0.5351 Learning Rate 0.00089195\n",
            "Batch 481 Loss 0.4672 Learning Rate 0.00089149\n",
            "Batch 491 Loss 0.5312 Learning Rate 0.00089104\n",
            "Batch 501 Loss 0.5889 Learning Rate 0.00089059\n",
            "Batch 511 Loss 0.5085 Learning Rate 0.00089014\n",
            "Batch 521 Loss 0.5028 Learning Rate 0.00088969\n",
            "Batch 531 Loss 0.7264 Learning Rate 0.00088923\n",
            "Batch 541 Loss 0.5463 Learning Rate 0.00088879\n",
            "Batch 551 Loss 0.7256 Learning Rate 0.00088834\n",
            "Batch 561 Loss 0.6917 Learning Rate 0.00088789\n",
            "Batch 571 Loss 0.6178 Learning Rate 0.00088744\n",
            "Batch 581 Loss 0.5283 Learning Rate 0.00088699\n",
            "Batch 591 Loss 0.5367 Learning Rate 0.00088655\n",
            "Batch 601 Loss 0.5031 Learning Rate 0.0008861\n",
            "Batch 611 Loss 0.5395 Learning Rate 0.00088566\n",
            "Batch 621 Loss 0.5009 Learning Rate 0.00088521\n",
            "Batch 631 Loss 0.5012 Learning Rate 0.00088477\n",
            "Batch 641 Loss 0.4617 Learning Rate 0.00088433\n",
            "Batch 651 Loss 0.5083 Learning Rate 0.00088388\n",
            "Batch 661 Loss 0.5838 Learning Rate 0.00088344\n",
            "Batch 671 Loss 0.546 Learning Rate 0.000883\n",
            "Batch 681 Loss 0.5159 Learning Rate 0.00088256\n",
            "Batch 691 Loss 0.5128 Learning Rate 0.00088212\n",
            "Batch 701 Loss 1.7404 Learning Rate 0.00088168\n",
            "Batch 711 Loss 0.5885 Learning Rate 0.00088124\n",
            "Batch 721 Loss 0.5378 Learning Rate 0.00088081\n",
            "Batch 731 Loss 0.5526 Learning Rate 0.00088037\n",
            "Batch 741 Loss 0.6786 Learning Rate 0.00087993\n",
            "Batch 751 Loss 0.5835 Learning Rate 0.0008795\n",
            "Batch 761 Loss 0.5662 Learning Rate 0.00087906\n",
            "Batch 771 Loss 0.6527 Learning Rate 0.00087863\n",
            "Batch 0 validation loss: 0.29122552275657654\n",
            "Batch 1 validation loss: 0.2657957375049591\n",
            "Batch 2 validation loss: 1.1513615846633911\n",
            "Batch 3 validation loss: 0.32995298504829407\n",
            "Batch 4 validation loss: 0.39602717757225037\n",
            "Batch 5 validation loss: 0.3585090935230255\n",
            "Batch 6 validation loss: 0.40658268332481384\n",
            "Batch 7 validation loss: 0.35784298181533813\n",
            "Batch 8 validation loss: 0.379916787147522\n",
            "Batch 9 validation loss: 0.3962557911872864\n",
            "Batch 10 validation loss: 0.3690938651561737\n",
            "Batch 11 validation loss: 0.48886924982070923\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 13 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.3379 Learning Rate 0.00087824\n",
            "Batch 11 Loss 0.5288 Learning Rate 0.0008778\n",
            "Batch 21 Loss 0.37 Learning Rate 0.00087737\n",
            "Batch 31 Loss 0.3859 Learning Rate 0.00087694\n",
            "Batch 41 Loss 0.4997 Learning Rate 0.00087651\n",
            "Batch 51 Loss 0.48 Learning Rate 0.00087608\n",
            "Batch 61 Loss 0.3666 Learning Rate 0.00087565\n",
            "Batch 71 Loss 0.3954 Learning Rate 0.00087522\n",
            "Batch 81 Loss 0.5184 Learning Rate 0.00087479\n",
            "Batch 91 Loss 0.4689 Learning Rate 0.00087436\n",
            "Batch 101 Loss 0.4428 Learning Rate 0.00087393\n",
            "Batch 111 Loss 0.4119 Learning Rate 0.00087351\n",
            "Batch 121 Loss 1.2907 Learning Rate 0.00087308\n",
            "Batch 131 Loss 0.4865 Learning Rate 0.00087265\n",
            "Batch 141 Loss 0.3878 Learning Rate 0.00087223\n",
            "Batch 151 Loss 0.4872 Learning Rate 0.00087181\n",
            "Batch 161 Loss 0.3964 Learning Rate 0.00087138\n",
            "Batch 171 Loss 0.4237 Learning Rate 0.00087096\n",
            "Batch 181 Loss 0.5246 Learning Rate 0.00087054\n",
            "Batch 191 Loss 0.3421 Learning Rate 0.00087011\n",
            "Batch 201 Loss 0.4189 Learning Rate 0.00086969\n",
            "Batch 211 Loss 0.4117 Learning Rate 0.00086927\n",
            "Batch 221 Loss 0.5291 Learning Rate 0.00086885\n",
            "Batch 231 Loss 0.4204 Learning Rate 0.00086843\n",
            "Batch 241 Loss 0.3866 Learning Rate 0.00086801\n",
            "Batch 251 Loss 0.412 Learning Rate 0.0008676\n",
            "Batch 261 Loss 0.3757 Learning Rate 0.00086718\n",
            "Batch 271 Loss 0.4815 Learning Rate 0.00086676\n",
            "Batch 281 Loss 0.456 Learning Rate 0.00086634\n",
            "Batch 291 Loss 0.4683 Learning Rate 0.00086593\n",
            "Batch 301 Loss 0.6771 Learning Rate 0.00086551\n",
            "Batch 311 Loss 0.4388 Learning Rate 0.0008651\n",
            "Batch 321 Loss 0.5861 Learning Rate 0.00086468\n",
            "Batch 331 Loss 0.1656 Learning Rate 0.00086427\n",
            "Batch 341 Loss 0.5795 Learning Rate 0.00086386\n",
            "Batch 351 Loss 0.5909 Learning Rate 0.00086345\n",
            "Batch 361 Loss 0.4644 Learning Rate 0.00086303\n",
            "Batch 371 Loss 0.4515 Learning Rate 0.00086262\n",
            "Batch 381 Loss 0.8724 Learning Rate 0.00086221\n",
            "Batch 391 Loss 0.5603 Learning Rate 0.0008618\n",
            "Batch 401 Loss 0.519 Learning Rate 0.00086139\n",
            "Batch 411 Loss 0.4542 Learning Rate 0.00086098\n",
            "Batch 421 Loss 0.3845 Learning Rate 0.00086058\n",
            "Batch 431 Loss 0.4814 Learning Rate 0.00086017\n",
            "Batch 441 Loss 0.6243 Learning Rate 0.00085976\n",
            "Batch 451 Loss 0.4757 Learning Rate 0.00085936\n",
            "Batch 461 Loss 0.5075 Learning Rate 0.00085895\n",
            "Batch 471 Loss 0.4674 Learning Rate 0.00085854\n",
            "Batch 481 Loss 0.4983 Learning Rate 0.00085814\n",
            "Batch 491 Loss 0.4503 Learning Rate 0.00085774\n",
            "Batch 501 Loss 0.5047 Learning Rate 0.00085733\n",
            "Batch 511 Loss 0.4913 Learning Rate 0.00085693\n",
            "Batch 521 Loss 0.5151 Learning Rate 0.00085653\n",
            "Batch 531 Loss 0.4795 Learning Rate 0.00085612\n",
            "Batch 541 Loss 0.5412 Learning Rate 0.00085572\n",
            "Batch 551 Loss 0.8867 Learning Rate 0.00085532\n",
            "Batch 561 Loss 0.5577 Learning Rate 0.00085492\n",
            "Batch 571 Loss 0.5185 Learning Rate 0.00085452\n",
            "Batch 581 Loss 0.4998 Learning Rate 0.00085412\n",
            "Batch 591 Loss 0.4889 Learning Rate 0.00085372\n",
            "Batch 601 Loss 0.5305 Learning Rate 0.00085333\n",
            "Batch 611 Loss 0.5688 Learning Rate 0.00085293\n",
            "Batch 621 Loss 0.486 Learning Rate 0.00085253\n",
            "Batch 631 Loss 0.5141 Learning Rate 0.00085214\n",
            "Batch 641 Loss 0.4491 Learning Rate 0.00085174\n",
            "Batch 651 Loss 0.476 Learning Rate 0.00085135\n",
            "Batch 661 Loss 0.4362 Learning Rate 0.00085095\n",
            "Batch 671 Loss 0.5273 Learning Rate 0.00085056\n",
            "Batch 681 Loss 0.5526 Learning Rate 0.00085016\n",
            "Batch 691 Loss 0.6041 Learning Rate 0.00084977\n",
            "Batch 701 Loss 0.5684 Learning Rate 0.00084938\n",
            "Batch 711 Loss 0.6107 Learning Rate 0.00084899\n",
            "Batch 721 Loss 0.4344 Learning Rate 0.00084859\n",
            "Batch 731 Loss 0.5317 Learning Rate 0.0008482\n",
            "Batch 741 Loss 0.5145 Learning Rate 0.00084781\n",
            "Batch 751 Loss 0.4833 Learning Rate 0.00084742\n",
            "Batch 761 Loss 0.5571 Learning Rate 0.00084703\n",
            "Batch 771 Loss 0.6083 Learning Rate 0.00084665\n",
            "Batch 0 validation loss: 0.28746020793914795\n",
            "Batch 1 validation loss: 0.23982249200344086\n",
            "Batch 2 validation loss: 0.7423505783081055\n",
            "Batch 3 validation loss: 0.3041073977947235\n",
            "Batch 4 validation loss: 0.37361064553260803\n",
            "Batch 5 validation loss: 0.31506797671318054\n",
            "Batch 6 validation loss: 0.35461360216140747\n",
            "Batch 7 validation loss: 0.35722294449806213\n",
            "Batch 8 validation loss: 0.34138044714927673\n",
            "Batch 9 validation loss: 0.35144853591918945\n",
            "Batch 10 validation loss: 0.34185948967933655\n",
            "Batch 11 validation loss: 0.3995687961578369\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 14 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.479 Learning Rate 0.0008463\n",
            "Batch 11 Loss 0.4235 Learning Rate 0.00084591\n",
            "Batch 21 Loss 0.3515 Learning Rate 0.00084552\n",
            "Batch 31 Loss 0.5023 Learning Rate 0.00084513\n",
            "Batch 41 Loss 0.4087 Learning Rate 0.00084475\n",
            "Batch 51 Loss 0.3869 Learning Rate 0.00084436\n",
            "Batch 61 Loss 0.5102 Learning Rate 0.00084398\n",
            "Batch 71 Loss 0.2978 Learning Rate 0.00084359\n",
            "Batch 81 Loss 0.3259 Learning Rate 0.00084321\n",
            "Batch 91 Loss 0.3799 Learning Rate 0.00084283\n",
            "Batch 101 Loss 0.3709 Learning Rate 0.00084244\n",
            "Batch 111 Loss 0.4045 Learning Rate 0.00084206\n",
            "Batch 121 Loss 0.3339 Learning Rate 0.00084168\n",
            "Batch 131 Loss 0.3328 Learning Rate 0.0008413\n",
            "Batch 141 Loss 0.3464 Learning Rate 0.00084092\n",
            "Batch 151 Loss 0.4526 Learning Rate 0.00084054\n",
            "Batch 161 Loss 0.3541 Learning Rate 0.00084016\n",
            "Batch 171 Loss 0.5304 Learning Rate 0.00083978\n",
            "Batch 181 Loss 0.4243 Learning Rate 0.0008394\n",
            "Batch 191 Loss 0.4773 Learning Rate 0.00083902\n",
            "Batch 201 Loss 0.4312 Learning Rate 0.00083864\n",
            "Batch 211 Loss 0.4107 Learning Rate 0.00083827\n",
            "Batch 221 Loss 0.3839 Learning Rate 0.00083789\n",
            "Batch 231 Loss 0.6548 Learning Rate 0.00083751\n",
            "Batch 241 Loss 0.4625 Learning Rate 0.00083714\n",
            "Batch 251 Loss 0.4502 Learning Rate 0.00083676\n",
            "Batch 261 Loss 0.4849 Learning Rate 0.00083639\n",
            "Batch 271 Loss 0.5786 Learning Rate 0.00083601\n",
            "Batch 281 Loss 0.3668 Learning Rate 0.00083564\n",
            "Batch 291 Loss 0.4381 Learning Rate 0.00083527\n",
            "Batch 301 Loss 0.4721 Learning Rate 0.00083489\n",
            "Batch 311 Loss 0.4131 Learning Rate 0.00083452\n",
            "Batch 321 Loss 0.523 Learning Rate 0.00083415\n",
            "Batch 331 Loss 0.1866 Learning Rate 0.00083378\n",
            "Batch 341 Loss 0.4421 Learning Rate 0.00083341\n",
            "Batch 351 Loss 0.4247 Learning Rate 0.00083304\n",
            "Batch 361 Loss 0.3996 Learning Rate 0.00083267\n",
            "Batch 371 Loss 0.3531 Learning Rate 0.0008323\n",
            "Batch 381 Loss 0.3737 Learning Rate 0.00083193\n",
            "Batch 391 Loss 0.4883 Learning Rate 0.00083156\n",
            "Batch 401 Loss 0.4521 Learning Rate 0.00083119\n",
            "Batch 411 Loss 0.4913 Learning Rate 0.00083083\n",
            "Batch 421 Loss 0.4025 Learning Rate 0.00083046\n",
            "Batch 431 Loss 0.429 Learning Rate 0.00083009\n",
            "Batch 441 Loss 0.4171 Learning Rate 0.00082973\n",
            "Batch 451 Loss 0.4756 Learning Rate 0.00082936\n",
            "Batch 461 Loss 0.4569 Learning Rate 0.000829\n",
            "Batch 471 Loss 0.4449 Learning Rate 0.00082863\n",
            "Batch 481 Loss 0.4443 Learning Rate 0.00082827\n",
            "Batch 491 Loss 0.5277 Learning Rate 0.00082791\n",
            "Batch 501 Loss 0.7725 Learning Rate 0.00082754\n",
            "Batch 511 Loss 0.6127 Learning Rate 0.00082718\n",
            "Batch 521 Loss 0.4752 Learning Rate 0.00082682\n",
            "Batch 531 Loss 0.4467 Learning Rate 0.00082646\n",
            "Batch 541 Loss 0.5211 Learning Rate 0.0008261\n",
            "Batch 551 Loss 0.4289 Learning Rate 0.00082573\n",
            "Batch 561 Loss 0.4669 Learning Rate 0.00082537\n",
            "Batch 571 Loss 0.5042 Learning Rate 0.00082502\n",
            "Batch 581 Loss 1.6824 Learning Rate 0.00082466\n",
            "Batch 591 Loss 0.4819 Learning Rate 0.0008243\n",
            "Batch 601 Loss 0.5197 Learning Rate 0.00082394\n",
            "Batch 611 Loss 0.5115 Learning Rate 0.00082358\n",
            "Batch 621 Loss 0.6731 Learning Rate 0.00082322\n",
            "Batch 631 Loss 0.5643 Learning Rate 0.00082287\n",
            "Batch 641 Loss 0.4391 Learning Rate 0.00082251\n",
            "Batch 651 Loss 0.4965 Learning Rate 0.00082215\n",
            "Batch 661 Loss 0.4299 Learning Rate 0.0008218\n",
            "Batch 671 Loss 0.4956 Learning Rate 0.00082144\n",
            "Batch 681 Loss 0.4377 Learning Rate 0.00082109\n",
            "Batch 691 Loss 0.4791 Learning Rate 0.00082074\n",
            "Batch 701 Loss 0.468 Learning Rate 0.00082038\n",
            "Batch 711 Loss 0.4579 Learning Rate 0.00082003\n",
            "Batch 721 Loss 0.5166 Learning Rate 0.00081968\n",
            "Batch 731 Loss 0.4545 Learning Rate 0.00081932\n",
            "Batch 741 Loss 0.4959 Learning Rate 0.00081897\n",
            "Batch 751 Loss 0.4488 Learning Rate 0.00081862\n",
            "Batch 761 Loss 0.4544 Learning Rate 0.00081827\n",
            "Batch 771 Loss 0.4759 Learning Rate 0.00081792\n",
            "Batch 0 validation loss: 0.26908737421035767\n",
            "Batch 1 validation loss: 0.18066900968551636\n",
            "Batch 2 validation loss: 0.5140706896781921\n",
            "Batch 3 validation loss: 0.2657991349697113\n",
            "Batch 4 validation loss: 0.3193909525871277\n",
            "Batch 5 validation loss: 0.29442712664604187\n",
            "Batch 6 validation loss: 0.3147536516189575\n",
            "Batch 7 validation loss: 0.3061350882053375\n",
            "Batch 8 validation loss: 0.30321887135505676\n",
            "Batch 9 validation loss: 0.31613194942474365\n",
            "Batch 10 validation loss: 0.28590139746665955\n",
            "Batch 11 validation loss: 0.38932350277900696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qzrHPlDfECgh",
        "colab_type": "code",
        "outputId": "b7820b47-3238-46a6-c321-86fc68da8e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "cell_type": "code",
      "source": [
        "# Predict a translation using greedy decoding for simplicity\n",
        "# You must use the jupyter EncoderDecoder class to run this!\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    \n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], \n",
        "                       dim=1)\n",
        "    return ys# Decode the model to produce translations (first sentence in validation set)\n",
        "\n",
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "print(TGT.vocab)\n",
        "\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    \n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    \n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    \n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.vocab.Vocab object at 0x7ffac8d86438>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Translation:\tAs daughters of God , you were born to lead . \n",
            "Target:\tAs daughters of God , you were born to lead . \n",
            "\n",
            "Translation:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "Target:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "\n",
            "Translation:\tThe other counselor was a prominent judge in the city . \n",
            "Target:\tThe other counselor was a prominent judge in the city . \n",
            "\n",
            "Translation:\tMy wife , Harriet was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "Target:\tMy wife , Harriet , was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "\n",
            "Translation:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "Target:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "\n",
            "Translation:\t“ And he that receiveth my Father receiveth my Father ’s kingdom ; therefore all that my Father hath shall be given unto him . ” \n",
            "Target:\t“ And he that receiveth my Father receiveth my Father ’s kingdom ; therefore all that my Father hath shall be given unto him . ” \n",
            "\n",
            "Translation:\tWhile it is good to pray and work for physical protection and healing during our mortal existence , our supreme focus should be on the spiritual miracles that are available to all of God ’s children . \n",
            "Target:\tWhile it is good to pray for and work for physical protection and healing during our mortal existence , our supreme focus should be on the spiritual miracles that are available to all of God ’s children . \n",
            "\n",
            "Translation:\tAnother regret people expressed was that they failed to become the person they felt they could and should have been . As they looked into their lives in the upon considering that they never had potential to bring up their potential , which too many songs would not be shared . \n",
            "Target:\tAnother regret people expressed was that they failed to become the person they felt they could and should have been . When they looked back on their lives , they realized that they never lived up to their potential , that too many songs remained unsung . \n",
            "\n",
            "Translation:\tThe Savior taught , “ This shall ye always do for all those who repent and are baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always \n",
            "Target:\tThe Savior taught : “ This shall ye always do to those who repent and are baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always remember me ye shall have my Spirit to be with you ” ( 3 Nephi 18:11 ) . \n",
            "\n",
            "Translation:\tAfter several years he returned to his hometown . However , the people refused to acknowledge his growth and improvement . To them , he was still just old “ so - and - so , ” and they treated him that way . Eventually , this good man faded away to a shadow of his former successful self \n",
            "Target:\tAfter several years he returned to his hometown . However , the people refused to acknowledge his growth and improvement . To them , he was still just old “ so - and - so , ” and they treated him that way . Eventually , this good man faded away to a shadow of his former successful self without being able to use his marvelously developed talents to bless those who derided and rejected him once again . What a loss , both for him and the community ! \n",
            "\n",
            "Translation:\tThe blessings you receive as you serve others are many . I have said , “ Oh , I ’ve been on my visits . ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I could see what a burden on what I could see as a burden rather \n",
            "Target:\tThe blessings you receive as you serve others are many . I have sometimes said , “ Oh , I ’ve got to get my visiting teaching done ! ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I was looking at it as a burden rather than a blessing . ) I can honestly say that when I went visiting teaching , I always felt better . I was lifted , loved , and blessed , usually much more than the sister I was visiting . My love increased . My desire to serve increased . And I could see what a beautiful way Heavenly Father has planned for us to watch over and care for one another . \n",
            "\n",
            "Translation:\tAnother journal entry reads : “ The … miracle for me occurred in the Family History office of Mel Olsen who presented me with a copy of all the converted ancestral line of the Ancestral File computerized records into the genealogical society . The majority of the records performed by the four generation of the Church program the four \n",
            "Target:\tAnother journal entry reads : “ The … miracle for me occurred in the Family History office of Mel Olsen who presented me with a printout of all my known ancestral pedigrees taken from the update of the Ancestral File computerized records sent into the genealogical society . They came mostly from the records of the four generation ’s program the Church called for many years ago . I had been overwhelmed with the thought of the huge task ahead of me to gather all my ancestors’ research records from family organizations to get them all in the computer for the first computerized distribution of the Ancestral File . And there they all were , beautiful , organized and laser printed and sitting there on the desk before me . I was so thrilled and so overwhelmed I just sat there stunned and then began to cry I was so happy . … For one who has <unk> , painstakingly researched for <unk> years , the <unk> of all these records is truly exciting . And when I think of the hundreds of thousands of people who are now or soon will be computerizing huge blocks of censuses and private research disks … I am so excited . It is truly the Lord ’s work and He is directing it . ” \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TrQSZM9hELCJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Two-layer training and results"
      ]
    },
    {
      "metadata": {
        "id": "DOSxEiN7EJTB",
        "colab_type": "code",
        "outputId": "a1ca0c05-72d4-46b1-8597-c84ba991c237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 30555
        }
      },
      "cell_type": "code",
      "source": [
        "# Set up the model \n",
        "model = make_model(n_src, n_tgt, N=2)\n",
        "model_opt = get_std_opt(model)\n",
        "model.cuda()\n",
        "\n",
        "# Set up the label smoothing to penalize overconfidence\n",
        "criterion = LabelSmoothing(size=n_tgt, padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "\n",
        "# Set up training and validation iterators\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True)\n",
        "\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "num_train_batches = 0\n",
        "for i, batch in enumerate(train_iter):\n",
        "    num_train_batches += 1\n",
        "num_valid_batches = 0\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    num_valid_batches += 1\n",
        "print(\"\\nNumber of training batches:\", num_train_batches)\n",
        "print(\"Number of validation batches:\", num_valid_batches, \"\\n\")\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    print(\"\\n\\n%%%%%%%%%% EPOCH \" + str(epoch) + \" %%%%%%%%%%\\n\")\n",
        "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, \n",
        "                model_opt)\n",
        "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of training batches: 779\n",
            "Number of validation batches: 12 \n",
            "\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 0 %%%%%%%%%%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 1 Loss 8.7888 Learning Rate 7e-07\n",
            "Batch 11 Loss 8.177 Learning Rate 4.19e-06\n",
            "Batch 21 Loss 8.3963 Learning Rate 7.69e-06\n",
            "Batch 31 Loss 9.2697 Learning Rate 1.118e-05\n",
            "Batch 41 Loss 8.9931 Learning Rate 1.467e-05\n",
            "Batch 51 Loss 8.3166 Learning Rate 1.817e-05\n",
            "Batch 61 Loss 8.6715 Learning Rate 2.166e-05\n",
            "Batch 71 Loss 8.1716 Learning Rate 2.516e-05\n",
            "Batch 81 Loss 6.8748 Learning Rate 2.865e-05\n",
            "Batch 91 Loss 8.1221 Learning Rate 3.214e-05\n",
            "Batch 101 Loss 7.4344 Learning Rate 3.564e-05\n",
            "Batch 111 Loss 7.3857 Learning Rate 3.913e-05\n",
            "Batch 121 Loss 7.3995 Learning Rate 4.263e-05\n",
            "Batch 131 Loss 7.0885 Learning Rate 4.612e-05\n",
            "Batch 141 Loss 6.7308 Learning Rate 4.961e-05\n",
            "Batch 151 Loss 6.9675 Learning Rate 5.311e-05\n",
            "Batch 161 Loss 6.4291 Learning Rate 5.66e-05\n",
            "Batch 171 Loss 6.2009 Learning Rate 6.009e-05\n",
            "Batch 181 Loss 6.5411 Learning Rate 6.359e-05\n",
            "Batch 191 Loss 5.6554 Learning Rate 6.708e-05\n",
            "Batch 201 Loss 6.0499 Learning Rate 7.058e-05\n",
            "Batch 211 Loss 5.7795 Learning Rate 7.407e-05\n",
            "Batch 221 Loss 5.8482 Learning Rate 7.756e-05\n",
            "Batch 231 Loss 5.0455 Learning Rate 8.106e-05\n",
            "Batch 241 Loss 5.3984 Learning Rate 8.455e-05\n",
            "Batch 251 Loss 5.8033 Learning Rate 8.805e-05\n",
            "Batch 261 Loss 5.3677 Learning Rate 9.154e-05\n",
            "Batch 271 Loss 5.5336 Learning Rate 9.503e-05\n",
            "Batch 281 Loss 5.3108 Learning Rate 9.853e-05\n",
            "Batch 291 Loss 4.714 Learning Rate 0.00010202\n",
            "Batch 301 Loss 5.8015 Learning Rate 0.00010551\n",
            "Batch 311 Loss 5.0882 Learning Rate 0.00010901\n",
            "Batch 321 Loss 5.5078 Learning Rate 0.0001125\n",
            "Batch 331 Loss 5.3935 Learning Rate 0.000116\n",
            "Batch 341 Loss 5.3839 Learning Rate 0.00011949\n",
            "Batch 351 Loss 5.2 Learning Rate 0.00012298\n",
            "Batch 361 Loss 5.6887 Learning Rate 0.00012648\n",
            "Batch 371 Loss 5.4045 Learning Rate 0.00012997\n",
            "Batch 381 Loss 4.9991 Learning Rate 0.00013347\n",
            "Batch 391 Loss 4.9268 Learning Rate 0.00013696\n",
            "Batch 401 Loss 5.355 Learning Rate 0.00014045\n",
            "Batch 411 Loss 5.1427 Learning Rate 0.00014395\n",
            "Batch 421 Loss 5.2044 Learning Rate 0.00014744\n",
            "Batch 431 Loss 5.0559 Learning Rate 0.00015093\n",
            "Batch 441 Loss 4.6787 Learning Rate 0.00015443\n",
            "Batch 451 Loss 4.5023 Learning Rate 0.00015792\n",
            "Batch 461 Loss 4.4948 Learning Rate 0.00016142\n",
            "Batch 471 Loss 4.2642 Learning Rate 0.00016491\n",
            "Batch 481 Loss 4.8937 Learning Rate 0.0001684\n",
            "Batch 491 Loss 4.9794 Learning Rate 0.0001719\n",
            "Batch 501 Loss 4.4334 Learning Rate 0.00017539\n",
            "Batch 511 Loss 4.1947 Learning Rate 0.00017889\n",
            "Batch 521 Loss 4.7273 Learning Rate 0.00018238\n",
            "Batch 531 Loss 4.4864 Learning Rate 0.00018587\n",
            "Batch 541 Loss 4.7965 Learning Rate 0.00018937\n",
            "Batch 551 Loss 5.1743 Learning Rate 0.00019286\n",
            "Batch 561 Loss 5.1772 Learning Rate 0.00019635\n",
            "Batch 571 Loss 4.5155 Learning Rate 0.00019985\n",
            "Batch 581 Loss 4.541 Learning Rate 0.00020334\n",
            "Batch 591 Loss 4.5206 Learning Rate 0.00020684\n",
            "Batch 601 Loss 4.5449 Learning Rate 0.00021033\n",
            "Batch 611 Loss 4.4653 Learning Rate 0.00021382\n",
            "Batch 621 Loss 4.7278 Learning Rate 0.00021732\n",
            "Batch 631 Loss 4.4317 Learning Rate 0.00022081\n",
            "Batch 641 Loss 4.5177 Learning Rate 0.00022431\n",
            "Batch 651 Loss 4.7426 Learning Rate 0.0002278\n",
            "Batch 661 Loss 4.1811 Learning Rate 0.00023129\n",
            "Batch 671 Loss 4.5328 Learning Rate 0.00023479\n",
            "Batch 681 Loss 4.7963 Learning Rate 0.00023828\n",
            "Batch 691 Loss 4.22 Learning Rate 0.00024177\n",
            "Batch 701 Loss 4.399 Learning Rate 0.00024527\n",
            "Batch 711 Loss 4.816 Learning Rate 0.00024876\n",
            "Batch 721 Loss 4.0924 Learning Rate 0.00025226\n",
            "Batch 731 Loss 3.9663 Learning Rate 0.00025575\n",
            "Batch 741 Loss 2.4063 Learning Rate 0.00025924\n",
            "Batch 751 Loss 3.416 Learning Rate 0.00026274\n",
            "Batch 761 Loss 4.4277 Learning Rate 0.00026623\n",
            "Batch 771 Loss 4.6864 Learning Rate 0.00026973\n",
            "Batch 0 validation loss: 3.2690951824188232\n",
            "Batch 1 validation loss: 3.9763059616088867\n",
            "Batch 2 validation loss: 5.470400333404541\n",
            "Batch 3 validation loss: 3.737321138381958\n",
            "Batch 4 validation loss: 3.9770538806915283\n",
            "Batch 5 validation loss: 4.043707847595215\n",
            "Batch 6 validation loss: 4.207093238830566\n",
            "Batch 7 validation loss: 4.278143405914307\n",
            "Batch 8 validation loss: 4.336517810821533\n",
            "Batch 9 validation loss: 4.494414806365967\n",
            "Batch 10 validation loss: 4.516138076782227\n",
            "Batch 11 validation loss: 4.937347888946533\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 1 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 3.4621 Learning Rate 0.00027287\n",
            "Batch 11 Loss 4.4973 Learning Rate 0.00027636\n",
            "Batch 21 Loss 3.8339 Learning Rate 0.00027986\n",
            "Batch 31 Loss 4.3756 Learning Rate 0.00028335\n",
            "Batch 41 Loss 3.8782 Learning Rate 0.00028685\n",
            "Batch 51 Loss 4.2711 Learning Rate 0.00029034\n",
            "Batch 61 Loss 4.5808 Learning Rate 0.00029383\n",
            "Batch 71 Loss 4.3261 Learning Rate 0.00029733\n",
            "Batch 81 Loss 3.2082 Learning Rate 0.00030082\n",
            "Batch 91 Loss 3.3589 Learning Rate 0.00030431\n",
            "Batch 101 Loss 3.5864 Learning Rate 0.00030781\n",
            "Batch 111 Loss 5.1278 Learning Rate 0.0003113\n",
            "Batch 121 Loss 4.09 Learning Rate 0.0003148\n",
            "Batch 131 Loss 4.3985 Learning Rate 0.00031829\n",
            "Batch 141 Loss 3.5469 Learning Rate 0.00032178\n",
            "Batch 151 Loss 3.8747 Learning Rate 0.00032528\n",
            "Batch 161 Loss 4.9157 Learning Rate 0.00032877\n",
            "Batch 171 Loss 4.1914 Learning Rate 0.00033227\n",
            "Batch 181 Loss 3.4757 Learning Rate 0.00033576\n",
            "Batch 191 Loss 3.9364 Learning Rate 0.00033925\n",
            "Batch 201 Loss 4.6122 Learning Rate 0.00034275\n",
            "Batch 211 Loss 3.8813 Learning Rate 0.00034624\n",
            "Batch 221 Loss 4.5439 Learning Rate 0.00034974\n",
            "Batch 231 Loss 4.1412 Learning Rate 0.00035323\n",
            "Batch 241 Loss 3.684 Learning Rate 0.00035672\n",
            "Batch 251 Loss 4.3696 Learning Rate 0.00036022\n",
            "Batch 261 Loss 3.8267 Learning Rate 0.00036371\n",
            "Batch 271 Loss 3.2917 Learning Rate 0.0003672\n",
            "Batch 281 Loss 3.6425 Learning Rate 0.0003707\n",
            "Batch 291 Loss 3.4293 Learning Rate 0.00037419\n",
            "Batch 301 Loss 3.9849 Learning Rate 0.00037769\n",
            "Batch 311 Loss 3.4132 Learning Rate 0.00038118\n",
            "Batch 321 Loss 4.6053 Learning Rate 0.00038467\n",
            "Batch 331 Loss 3.8062 Learning Rate 0.00038817\n",
            "Batch 341 Loss 4.5639 Learning Rate 0.00039166\n",
            "Batch 351 Loss 3.721 Learning Rate 0.00039516\n",
            "Batch 361 Loss 5.0126 Learning Rate 0.00039865\n",
            "Batch 371 Loss 3.6175 Learning Rate 0.00040214\n",
            "Batch 381 Loss 3.4712 Learning Rate 0.00040564\n",
            "Batch 391 Loss 3.4833 Learning Rate 0.00040913\n",
            "Batch 401 Loss 4.1675 Learning Rate 0.00041262\n",
            "Batch 411 Loss 4.2686 Learning Rate 0.00041612\n",
            "Batch 421 Loss 3.2196 Learning Rate 0.00041961\n",
            "Batch 431 Loss 1.91 Learning Rate 0.00042311\n",
            "Batch 441 Loss 3.1989 Learning Rate 0.0004266\n",
            "Batch 451 Loss 3.5187 Learning Rate 0.00043009\n",
            "Batch 461 Loss 2.8223 Learning Rate 0.00043359\n",
            "Batch 471 Loss 2.918 Learning Rate 0.00043708\n",
            "Batch 481 Loss 3.8376 Learning Rate 0.00044058\n",
            "Batch 491 Loss 3.2717 Learning Rate 0.00044407\n",
            "Batch 501 Loss 2.8867 Learning Rate 0.00044756\n",
            "Batch 511 Loss 2.9639 Learning Rate 0.00045106\n",
            "Batch 521 Loss 4.3206 Learning Rate 0.00045455\n",
            "Batch 531 Loss 3.0154 Learning Rate 0.00045804\n",
            "Batch 541 Loss 2.875 Learning Rate 0.00046154\n",
            "Batch 551 Loss 2.3517 Learning Rate 0.00046503\n",
            "Batch 561 Loss 2.8535 Learning Rate 0.00046853\n",
            "Batch 571 Loss 3.2699 Learning Rate 0.00047202\n",
            "Batch 581 Loss 2.6547 Learning Rate 0.00047551\n",
            "Batch 591 Loss 2.4228 Learning Rate 0.00047901\n",
            "Batch 601 Loss 3.4782 Learning Rate 0.0004825\n",
            "Batch 611 Loss 3.7945 Learning Rate 0.000486\n",
            "Batch 621 Loss 2.1386 Learning Rate 0.00048949\n",
            "Batch 631 Loss 3.3317 Learning Rate 0.00049298\n",
            "Batch 641 Loss 2.3109 Learning Rate 0.00049648\n",
            "Batch 651 Loss 3.3361 Learning Rate 0.00049997\n",
            "Batch 661 Loss 2.0334 Learning Rate 0.00050346\n",
            "Batch 671 Loss 2.4457 Learning Rate 0.00050696\n",
            "Batch 681 Loss 3.9658 Learning Rate 0.00051045\n",
            "Batch 691 Loss 2.6817 Learning Rate 0.00051395\n",
            "Batch 701 Loss 2.6467 Learning Rate 0.00051744\n",
            "Batch 711 Loss 2.6654 Learning Rate 0.00052093\n",
            "Batch 721 Loss 2.6648 Learning Rate 0.00052443\n",
            "Batch 731 Loss 2.4336 Learning Rate 0.00052792\n",
            "Batch 741 Loss 2.4234 Learning Rate 0.00053142\n",
            "Batch 751 Loss 2.3681 Learning Rate 0.00053491\n",
            "Batch 761 Loss 2.3833 Learning Rate 0.0005384\n",
            "Batch 771 Loss 2.6698 Learning Rate 0.0005419\n",
            "Batch 0 validation loss: 1.4847255945205688\n",
            "Batch 1 validation loss: 2.117258071899414\n",
            "Batch 2 validation loss: 4.436027526855469\n",
            "Batch 3 validation loss: 1.991815447807312\n",
            "Batch 4 validation loss: 2.215348243713379\n",
            "Batch 5 validation loss: 2.2011215686798096\n",
            "Batch 6 validation loss: 2.3108716011047363\n",
            "Batch 7 validation loss: 2.4292218685150146\n",
            "Batch 8 validation loss: 2.545295238494873\n",
            "Batch 9 validation loss: 2.661600351333618\n",
            "Batch 10 validation loss: 2.797564744949341\n",
            "Batch 11 validation loss: 3.349400281906128\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 2 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 2.5886 Learning Rate 0.00054504\n",
            "Batch 11 Loss 2.2959 Learning Rate 0.00054854\n",
            "Batch 21 Loss 2.4046 Learning Rate 0.00055203\n",
            "Batch 31 Loss 2.5579 Learning Rate 0.00055552\n",
            "Batch 41 Loss 2.7289 Learning Rate 0.00055902\n",
            "Batch 51 Loss 1.8932 Learning Rate 0.00056251\n",
            "Batch 61 Loss 3.0904 Learning Rate 0.000566\n",
            "Batch 71 Loss 2.4397 Learning Rate 0.0005695\n",
            "Batch 81 Loss 1.7647 Learning Rate 0.00057299\n",
            "Batch 91 Loss 1.9087 Learning Rate 0.00057649\n",
            "Batch 101 Loss 3.7153 Learning Rate 0.00057998\n",
            "Batch 111 Loss 2.6861 Learning Rate 0.00058347\n",
            "Batch 121 Loss 1.8159 Learning Rate 0.00058697\n",
            "Batch 131 Loss 2.5327 Learning Rate 0.00059046\n",
            "Batch 141 Loss 2.1093 Learning Rate 0.00059396\n",
            "Batch 151 Loss 2.0088 Learning Rate 0.00059745\n",
            "Batch 161 Loss 1.9007 Learning Rate 0.00060094\n",
            "Batch 171 Loss 1.7866 Learning Rate 0.00060444\n",
            "Batch 181 Loss 1.7799 Learning Rate 0.00060793\n",
            "Batch 191 Loss 2.159 Learning Rate 0.00061142\n",
            "Batch 201 Loss 2.4213 Learning Rate 0.00061492\n",
            "Batch 211 Loss 1.7355 Learning Rate 0.00061841\n",
            "Batch 221 Loss 1.8116 Learning Rate 0.00062191\n",
            "Batch 231 Loss 2.178 Learning Rate 0.0006254\n",
            "Batch 241 Loss 2.1022 Learning Rate 0.00062889\n",
            "Batch 251 Loss 2.0796 Learning Rate 0.00063239\n",
            "Batch 261 Loss 2.588 Learning Rate 0.00063588\n",
            "Batch 271 Loss 2.8357 Learning Rate 0.00063938\n",
            "Batch 281 Loss 1.908 Learning Rate 0.00064287\n",
            "Batch 291 Loss 2.4723 Learning Rate 0.00064636\n",
            "Batch 301 Loss 1.762 Learning Rate 0.00064986\n",
            "Batch 311 Loss 1.9424 Learning Rate 0.00065335\n",
            "Batch 321 Loss 1.9783 Learning Rate 0.00065684\n",
            "Batch 331 Loss 2.7882 Learning Rate 0.00066034\n",
            "Batch 341 Loss 1.5008 Learning Rate 0.00066383\n",
            "Batch 351 Loss 1.916 Learning Rate 0.00066733\n",
            "Batch 361 Loss 2.0982 Learning Rate 0.00067082\n",
            "Batch 371 Loss 2.3882 Learning Rate 0.00067431\n",
            "Batch 381 Loss 1.6974 Learning Rate 0.00067781\n",
            "Batch 391 Loss 2.3563 Learning Rate 0.0006813\n",
            "Batch 401 Loss 1.7696 Learning Rate 0.0006848\n",
            "Batch 411 Loss 2.0801 Learning Rate 0.00068829\n",
            "Batch 421 Loss 2.335 Learning Rate 0.00069178\n",
            "Batch 431 Loss 2.8332 Learning Rate 0.00069528\n",
            "Batch 441 Loss 2.4636 Learning Rate 0.00069877\n",
            "Batch 451 Loss 1.8978 Learning Rate 0.00070227\n",
            "Batch 461 Loss 1.8313 Learning Rate 0.00070576\n",
            "Batch 471 Loss 2.2478 Learning Rate 0.00070925\n",
            "Batch 481 Loss 2.3134 Learning Rate 0.00071275\n",
            "Batch 491 Loss 2.0639 Learning Rate 0.00071624\n",
            "Batch 501 Loss 1.9222 Learning Rate 0.00071973\n",
            "Batch 511 Loss 1.8916 Learning Rate 0.00072323\n",
            "Batch 521 Loss 2.2661 Learning Rate 0.00072672\n",
            "Batch 531 Loss 2.5629 Learning Rate 0.00073022\n",
            "Batch 541 Loss 2.0719 Learning Rate 0.00073371\n",
            "Batch 551 Loss 2.2497 Learning Rate 0.0007372\n",
            "Batch 561 Loss 1.6194 Learning Rate 0.0007407\n",
            "Batch 571 Loss 1.7093 Learning Rate 0.00074419\n",
            "Batch 581 Loss 1.8408 Learning Rate 0.00074769\n",
            "Batch 591 Loss 1.9222 Learning Rate 0.00075118\n",
            "Batch 601 Loss 1.6592 Learning Rate 0.00075467\n",
            "Batch 611 Loss 1.7308 Learning Rate 0.00075817\n",
            "Batch 621 Loss 1.8599 Learning Rate 0.00076166\n",
            "Batch 631 Loss 1.7488 Learning Rate 0.00076515\n",
            "Batch 641 Loss 1.8776 Learning Rate 0.00076865\n",
            "Batch 651 Loss 2.0061 Learning Rate 0.00077214\n",
            "Batch 661 Loss 1.6537 Learning Rate 0.00077564\n",
            "Batch 671 Loss 1.6229 Learning Rate 0.00077913\n",
            "Batch 681 Loss 1.7492 Learning Rate 0.00078262\n",
            "Batch 691 Loss 2.676 Learning Rate 0.00078612\n",
            "Batch 701 Loss 1.7732 Learning Rate 0.00078961\n",
            "Batch 711 Loss 1.9662 Learning Rate 0.00079311\n",
            "Batch 721 Loss 1.9807 Learning Rate 0.0007966\n",
            "Batch 731 Loss 1.7818 Learning Rate 0.00080009\n",
            "Batch 741 Loss 1.7239 Learning Rate 0.00080359\n",
            "Batch 751 Loss 1.7683 Learning Rate 0.00080708\n",
            "Batch 761 Loss 3.2062 Learning Rate 0.00081057\n",
            "Batch 771 Loss 1.5156 Learning Rate 0.00081407\n",
            "Batch 0 validation loss: 0.9065095782279968\n",
            "Batch 1 validation loss: 1.422868251800537\n",
            "Batch 2 validation loss: 3.2762575149536133\n",
            "Batch 3 validation loss: 1.402698040008545\n",
            "Batch 4 validation loss: 1.5128355026245117\n",
            "Batch 5 validation loss: 1.498350977897644\n",
            "Batch 6 validation loss: 1.6111712455749512\n",
            "Batch 7 validation loss: 1.624186396598816\n",
            "Batch 8 validation loss: 1.6982141733169556\n",
            "Batch 9 validation loss: 1.7680037021636963\n",
            "Batch 10 validation loss: 1.8998359441757202\n",
            "Batch 11 validation loss: 2.319052219390869\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 3 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.5531 Learning Rate 0.00081721\n",
            "Batch 11 Loss 1.5818 Learning Rate 0.00082071\n",
            "Batch 21 Loss 1.7845 Learning Rate 0.0008242\n",
            "Batch 31 Loss 1.451 Learning Rate 0.00082769\n",
            "Batch 41 Loss 1.6873 Learning Rate 0.00083119\n",
            "Batch 51 Loss 1.3459 Learning Rate 0.00083468\n",
            "Batch 61 Loss 1.2295 Learning Rate 0.00083818\n",
            "Batch 71 Loss 1.3964 Learning Rate 0.00084167\n",
            "Batch 81 Loss 1.642 Learning Rate 0.00084516\n",
            "Batch 91 Loss 1.5495 Learning Rate 0.00084866\n",
            "Batch 101 Loss 1.3042 Learning Rate 0.00085215\n",
            "Batch 111 Loss 1.693 Learning Rate 0.00085565\n",
            "Batch 121 Loss 1.4879 Learning Rate 0.00085914\n",
            "Batch 131 Loss 1.5358 Learning Rate 0.00086263\n",
            "Batch 141 Loss 1.6561 Learning Rate 0.00086613\n",
            "Batch 151 Loss 1.3417 Learning Rate 0.00086962\n",
            "Batch 161 Loss 1.5786 Learning Rate 0.00087311\n",
            "Batch 171 Loss 1.7992 Learning Rate 0.00087661\n",
            "Batch 181 Loss 1.9653 Learning Rate 0.0008801\n",
            "Batch 191 Loss 1.6617 Learning Rate 0.0008836\n",
            "Batch 201 Loss 1.5308 Learning Rate 0.00088709\n",
            "Batch 211 Loss 1.8897 Learning Rate 0.00089058\n",
            "Batch 221 Loss 1.6961 Learning Rate 0.00089408\n",
            "Batch 231 Loss 2.4279 Learning Rate 0.00089757\n",
            "Batch 241 Loss 1.5039 Learning Rate 0.00090107\n",
            "Batch 251 Loss 1.5421 Learning Rate 0.00090456\n",
            "Batch 261 Loss 1.7067 Learning Rate 0.00090805\n",
            "Batch 271 Loss 1.7339 Learning Rate 0.00091155\n",
            "Batch 281 Loss 1.554 Learning Rate 0.00091504\n",
            "Batch 291 Loss 1.3059 Learning Rate 0.00091853\n",
            "Batch 301 Loss 1.1758 Learning Rate 0.00092203\n",
            "Batch 311 Loss 1.1416 Learning Rate 0.00092552\n",
            "Batch 321 Loss 1.74 Learning Rate 0.00092902\n",
            "Batch 331 Loss 1.3394 Learning Rate 0.00093251\n",
            "Batch 341 Loss 1.3618 Learning Rate 0.000936\n",
            "Batch 351 Loss 1.2524 Learning Rate 0.0009395\n",
            "Batch 361 Loss 2.4629 Learning Rate 0.00094299\n",
            "Batch 371 Loss 1.8873 Learning Rate 0.00094649\n",
            "Batch 381 Loss 1.3569 Learning Rate 0.00094998\n",
            "Batch 391 Loss 1.3967 Learning Rate 0.00095347\n",
            "Batch 401 Loss 1.699 Learning Rate 0.00095697\n",
            "Batch 411 Loss 1.4692 Learning Rate 0.00096046\n",
            "Batch 421 Loss 1.3585 Learning Rate 0.00096395\n",
            "Batch 431 Loss 1.5438 Learning Rate 0.00096745\n",
            "Batch 441 Loss 1.3865 Learning Rate 0.00097094\n",
            "Batch 451 Loss 1.4408 Learning Rate 0.00097444\n",
            "Batch 461 Loss 1.5888 Learning Rate 0.00097793\n",
            "Batch 471 Loss 1.3024 Learning Rate 0.00098142\n",
            "Batch 481 Loss 1.6301 Learning Rate 0.00098492\n",
            "Batch 491 Loss 1.5194 Learning Rate 0.00098841\n",
            "Batch 501 Loss 1.6383 Learning Rate 0.00099191\n",
            "Batch 511 Loss 1.5661 Learning Rate 0.0009954\n",
            "Batch 521 Loss 1.4432 Learning Rate 0.00099889\n",
            "Batch 531 Loss 1.265 Learning Rate 0.00100239\n",
            "Batch 541 Loss 1.6224 Learning Rate 0.00100588\n",
            "Batch 551 Loss 1.6697 Learning Rate 0.00100938\n",
            "Batch 561 Loss 1.3968 Learning Rate 0.00101287\n",
            "Batch 571 Loss 1.5499 Learning Rate 0.00101636\n",
            "Batch 581 Loss 1.385 Learning Rate 0.00101986\n",
            "Batch 591 Loss 1.357 Learning Rate 0.00102335\n",
            "Batch 601 Loss 1.6317 Learning Rate 0.00102684\n",
            "Batch 611 Loss 1.7542 Learning Rate 0.00103034\n",
            "Batch 621 Loss 0.9298 Learning Rate 0.00103383\n",
            "Batch 631 Loss 1.9099 Learning Rate 0.00103733\n",
            "Batch 641 Loss 1.1747 Learning Rate 0.00104082\n",
            "Batch 651 Loss 1.2253 Learning Rate 0.00104431\n",
            "Batch 661 Loss 1.153 Learning Rate 0.00104781\n",
            "Batch 671 Loss 1.6599 Learning Rate 0.0010513\n",
            "Batch 681 Loss 0.8094 Learning Rate 0.0010548\n",
            "Batch 691 Loss 1.1438 Learning Rate 0.00105829\n",
            "Batch 701 Loss 1.4576 Learning Rate 0.00106178\n",
            "Batch 711 Loss 1.7339 Learning Rate 0.00106528\n",
            "Batch 721 Loss 1.2927 Learning Rate 0.00106877\n",
            "Batch 731 Loss 1.2319 Learning Rate 0.00107226\n",
            "Batch 741 Loss 1.32 Learning Rate 0.00107576\n",
            "Batch 751 Loss 1.0923 Learning Rate 0.00107925\n",
            "Batch 761 Loss 2.0049 Learning Rate 0.00108275\n",
            "Batch 771 Loss 1.4016 Learning Rate 0.00108624\n",
            "Batch 0 validation loss: 0.7819173336029053\n",
            "Batch 1 validation loss: 1.0376418828964233\n",
            "Batch 2 validation loss: 2.565135955810547\n",
            "Batch 3 validation loss: 1.0895403623580933\n",
            "Batch 4 validation loss: 1.1814943552017212\n",
            "Batch 5 validation loss: 1.1817004680633545\n",
            "Batch 6 validation loss: 1.2470636367797852\n",
            "Batch 7 validation loss: 1.2514640092849731\n",
            "Batch 8 validation loss: 1.3090959787368774\n",
            "Batch 9 validation loss: 1.346697211265564\n",
            "Batch 10 validation loss: 1.408844232559204\n",
            "Batch 11 validation loss: 1.695235013961792\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 4 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.1298 Learning Rate 0.00108938\n",
            "Batch 11 Loss 1.4078 Learning Rate 0.00109288\n",
            "Batch 21 Loss 1.5016 Learning Rate 0.00109637\n",
            "Batch 31 Loss 1.0116 Learning Rate 0.00109987\n",
            "Batch 41 Loss 1.2488 Learning Rate 0.00110336\n",
            "Batch 51 Loss 1.1236 Learning Rate 0.00110685\n",
            "Batch 61 Loss 1.4576 Learning Rate 0.00111035\n",
            "Batch 71 Loss 1.3445 Learning Rate 0.00111384\n",
            "Batch 81 Loss 1.5682 Learning Rate 0.00111734\n",
            "Batch 91 Loss 1.362 Learning Rate 0.00112083\n",
            "Batch 101 Loss 1.1944 Learning Rate 0.00112432\n",
            "Batch 111 Loss 1.5189 Learning Rate 0.00112782\n",
            "Batch 121 Loss 0.9083 Learning Rate 0.00113131\n",
            "Batch 131 Loss 1.3211 Learning Rate 0.0011348\n",
            "Batch 141 Loss 1.0514 Learning Rate 0.0011383\n",
            "Batch 151 Loss 0.9137 Learning Rate 0.00114179\n",
            "Batch 161 Loss 1.5132 Learning Rate 0.00114529\n",
            "Batch 171 Loss 1.526 Learning Rate 0.00114878\n",
            "Batch 181 Loss 1.0532 Learning Rate 0.00115227\n",
            "Batch 191 Loss 1.1787 Learning Rate 0.00115577\n",
            "Batch 201 Loss 1.7964 Learning Rate 0.00115926\n",
            "Batch 211 Loss 1.0636 Learning Rate 0.00116276\n",
            "Batch 221 Loss 1.133 Learning Rate 0.00116625\n",
            "Batch 231 Loss 1.4768 Learning Rate 0.00116974\n",
            "Batch 241 Loss 1.535 Learning Rate 0.00117324\n",
            "Batch 251 Loss 1.3874 Learning Rate 0.00117673\n",
            "Batch 261 Loss 1.2106 Learning Rate 0.00118022\n",
            "Batch 271 Loss 1.5016 Learning Rate 0.00118372\n",
            "Batch 281 Loss 1.1831 Learning Rate 0.00118721\n",
            "Batch 291 Loss 2.5685 Learning Rate 0.00119071\n",
            "Batch 301 Loss 1.9124 Learning Rate 0.0011942\n",
            "Batch 311 Loss 1.3457 Learning Rate 0.00119769\n",
            "Batch 321 Loss 1.1676 Learning Rate 0.00120119\n",
            "Batch 331 Loss 1.4933 Learning Rate 0.00120468\n",
            "Batch 341 Loss 1.2033 Learning Rate 0.00120818\n",
            "Batch 351 Loss 1.4521 Learning Rate 0.00121167\n",
            "Batch 361 Loss 1.6514 Learning Rate 0.00121516\n",
            "Batch 371 Loss 1.0594 Learning Rate 0.00121866\n",
            "Batch 381 Loss 1.2991 Learning Rate 0.00122215\n",
            "Batch 391 Loss 2.5777 Learning Rate 0.00122564\n",
            "Batch 401 Loss 1.1963 Learning Rate 0.00122914\n",
            "Batch 411 Loss 1.7457 Learning Rate 0.00123263\n",
            "Batch 421 Loss 1.1994 Learning Rate 0.00123613\n",
            "Batch 431 Loss 1.5585 Learning Rate 0.00123962\n",
            "Batch 441 Loss 1.0221 Learning Rate 0.00124311\n",
            "Batch 451 Loss 0.3009 Learning Rate 0.00124661\n",
            "Batch 461 Loss 2.2922 Learning Rate 0.0012501\n",
            "Batch 471 Loss 1.6196 Learning Rate 0.0012536\n",
            "Batch 481 Loss 1.3771 Learning Rate 0.00125709\n",
            "Batch 491 Loss 1.4634 Learning Rate 0.00126058\n",
            "Batch 501 Loss 1.1325 Learning Rate 0.00126408\n",
            "Batch 511 Loss 1.1958 Learning Rate 0.00126757\n",
            "Batch 521 Loss 1.7204 Learning Rate 0.00127106\n",
            "Batch 531 Loss 1.434 Learning Rate 0.00127456\n",
            "Batch 541 Loss 1.7336 Learning Rate 0.00127805\n",
            "Batch 551 Loss 1.3734 Learning Rate 0.00128155\n",
            "Batch 561 Loss 1.4551 Learning Rate 0.00128504\n",
            "Batch 571 Loss 1.7598 Learning Rate 0.00128853\n",
            "Batch 581 Loss 1.2749 Learning Rate 0.00129203\n",
            "Batch 591 Loss 1.1144 Learning Rate 0.00129552\n",
            "Batch 601 Loss 1.4144 Learning Rate 0.00129902\n",
            "Batch 611 Loss 1.2622 Learning Rate 0.00130251\n",
            "Batch 621 Loss 1.2573 Learning Rate 0.001306\n",
            "Batch 631 Loss 1.8372 Learning Rate 0.0013095\n",
            "Batch 641 Loss 1.4531 Learning Rate 0.00131299\n",
            "Batch 651 Loss 1.1235 Learning Rate 0.00131649\n",
            "Batch 661 Loss 0.9689 Learning Rate 0.00131998\n",
            "Batch 671 Loss 1.2441 Learning Rate 0.00132347\n",
            "Batch 681 Loss 1.1814 Learning Rate 0.00132697\n",
            "Batch 691 Loss 0.9462 Learning Rate 0.00133046\n",
            "Batch 701 Loss 0.7428 Learning Rate 0.00133395\n",
            "Batch 711 Loss 1.334 Learning Rate 0.00133745\n",
            "Batch 721 Loss 1.3431 Learning Rate 0.00134094\n",
            "Batch 731 Loss 1.6753 Learning Rate 0.00134444\n",
            "Batch 741 Loss 1.1468 Learning Rate 0.00134793\n",
            "Batch 751 Loss 1.173 Learning Rate 0.00135142\n",
            "Batch 761 Loss 1.38 Learning Rate 0.00135492\n",
            "Batch 771 Loss 2.5562 Learning Rate 0.00135841\n",
            "Batch 0 validation loss: 0.7421281933784485\n",
            "Batch 1 validation loss: 0.8544414639472961\n",
            "Batch 2 validation loss: 2.246772050857544\n",
            "Batch 3 validation loss: 0.9793022274971008\n",
            "Batch 4 validation loss: 1.0342847108840942\n",
            "Batch 5 validation loss: 1.0623961687088013\n",
            "Batch 6 validation loss: 1.0725072622299194\n",
            "Batch 7 validation loss: 1.0995666980743408\n",
            "Batch 8 validation loss: 1.1827986240386963\n",
            "Batch 9 validation loss: 1.1648423671722412\n",
            "Batch 10 validation loss: 1.1619141101837158\n",
            "Batch 11 validation loss: 1.4363676309585571\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 5 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.9861 Learning Rate 0.00136156\n",
            "Batch 11 Loss 1.7257 Learning Rate 0.00136505\n",
            "Batch 21 Loss 1.0453 Learning Rate 0.00136854\n",
            "Batch 31 Loss 1.1523 Learning Rate 0.00137204\n",
            "Batch 41 Loss 1.2148 Learning Rate 0.00137553\n",
            "Batch 51 Loss 0.8298 Learning Rate 0.00137903\n",
            "Batch 61 Loss 1.1433 Learning Rate 0.00138252\n",
            "Batch 71 Loss 1.4198 Learning Rate 0.00138601\n",
            "Batch 81 Loss 1.0999 Learning Rate 0.00138951\n",
            "Batch 91 Loss 1.0445 Learning Rate 0.001393\n",
            "Batch 101 Loss 0.8878 Learning Rate 0.00139649\n",
            "Batch 111 Loss 1.4862 Learning Rate 0.00139632\n",
            "Batch 121 Loss 1.0084 Learning Rate 0.00139458\n",
            "Batch 131 Loss 1.2269 Learning Rate 0.00139285\n",
            "Batch 141 Loss 0.9536 Learning Rate 0.00139112\n",
            "Batch 151 Loss 0.3964 Learning Rate 0.0013894\n",
            "Batch 161 Loss 1.152 Learning Rate 0.00138769\n",
            "Batch 171 Loss 1.16 Learning Rate 0.00138598\n",
            "Batch 181 Loss 1.0954 Learning Rate 0.00138428\n",
            "Batch 191 Loss 1.4425 Learning Rate 0.00138259\n",
            "Batch 201 Loss 1.5945 Learning Rate 0.0013809\n",
            "Batch 211 Loss 1.2378 Learning Rate 0.00137922\n",
            "Batch 221 Loss 1.1326 Learning Rate 0.00137754\n",
            "Batch 231 Loss 1.2535 Learning Rate 0.00137587\n",
            "Batch 241 Loss 1.1027 Learning Rate 0.00137421\n",
            "Batch 251 Loss 1.0224 Learning Rate 0.00137255\n",
            "Batch 261 Loss 1.0662 Learning Rate 0.0013709\n",
            "Batch 271 Loss 2.404 Learning Rate 0.00136925\n",
            "Batch 281 Loss 1.2249 Learning Rate 0.00136761\n",
            "Batch 291 Loss 1.1565 Learning Rate 0.00136598\n",
            "Batch 301 Loss 1.061 Learning Rate 0.00136435\n",
            "Batch 311 Loss 3.168 Learning Rate 0.00136273\n",
            "Batch 321 Loss 1.0357 Learning Rate 0.00136111\n",
            "Batch 331 Loss 1.1006 Learning Rate 0.0013595\n",
            "Batch 341 Loss 1.2562 Learning Rate 0.00135789\n",
            "Batch 351 Loss 1.1031 Learning Rate 0.00135629\n",
            "Batch 361 Loss 1.1073 Learning Rate 0.0013547\n",
            "Batch 371 Loss 1.1544 Learning Rate 0.00135311\n",
            "Batch 381 Loss 1.2411 Learning Rate 0.00135153\n",
            "Batch 391 Loss 1.2359 Learning Rate 0.00134995\n",
            "Batch 401 Loss 0.9554 Learning Rate 0.00134838\n",
            "Batch 411 Loss 0.8146 Learning Rate 0.00134681\n",
            "Batch 421 Loss 1.307 Learning Rate 0.00134525\n",
            "Batch 431 Loss 1.0444 Learning Rate 0.0013437\n",
            "Batch 441 Loss 1.3806 Learning Rate 0.00134215\n",
            "Batch 451 Loss 1.1404 Learning Rate 0.0013406\n",
            "Batch 461 Loss 1.1393 Learning Rate 0.00133906\n",
            "Batch 471 Loss 1.0288 Learning Rate 0.00133753\n",
            "Batch 481 Loss 0.992 Learning Rate 0.001336\n",
            "Batch 491 Loss 1.3131 Learning Rate 0.00133448\n",
            "Batch 501 Loss 1.0576 Learning Rate 0.00133296\n",
            "Batch 511 Loss 1.2591 Learning Rate 0.00133145\n",
            "Batch 521 Loss 1.0237 Learning Rate 0.00132994\n",
            "Batch 531 Loss 1.0773 Learning Rate 0.00132843\n",
            "Batch 541 Loss 1.2398 Learning Rate 0.00132694\n",
            "Batch 551 Loss 0.9727 Learning Rate 0.00132544\n",
            "Batch 561 Loss 1.2713 Learning Rate 0.00132396\n",
            "Batch 571 Loss 1.1326 Learning Rate 0.00132247\n",
            "Batch 581 Loss 1.0372 Learning Rate 0.001321\n",
            "Batch 591 Loss 0.987 Learning Rate 0.00131952\n",
            "Batch 601 Loss 1.0112 Learning Rate 0.00131806\n",
            "Batch 611 Loss 0.5005 Learning Rate 0.00131659\n",
            "Batch 621 Loss 1.072 Learning Rate 0.00131513\n",
            "Batch 631 Loss 2.1919 Learning Rate 0.00131368\n",
            "Batch 641 Loss 1.0564 Learning Rate 0.00131223\n",
            "Batch 651 Loss 1.0044 Learning Rate 0.00131079\n",
            "Batch 661 Loss 1.3925 Learning Rate 0.00130935\n",
            "Batch 671 Loss 1.1472 Learning Rate 0.00130791\n",
            "Batch 681 Loss 1.0809 Learning Rate 0.00130649\n",
            "Batch 691 Loss 1.6174 Learning Rate 0.00130506\n",
            "Batch 701 Loss 1.0391 Learning Rate 0.00130364\n",
            "Batch 711 Loss 0.9523 Learning Rate 0.00130222\n",
            "Batch 721 Loss 0.9456 Learning Rate 0.00130081\n",
            "Batch 731 Loss 1.092 Learning Rate 0.00129941\n",
            "Batch 741 Loss 1.1681 Learning Rate 0.00129801\n",
            "Batch 751 Loss 1.283 Learning Rate 0.00129661\n",
            "Batch 761 Loss 1.2304 Learning Rate 0.00129522\n",
            "Batch 771 Loss 1.0669 Learning Rate 0.00129383\n",
            "Batch 0 validation loss: 0.6136118769645691\n",
            "Batch 1 validation loss: 0.6620437502861023\n",
            "Batch 2 validation loss: 1.682948350906372\n",
            "Batch 3 validation loss: 0.804160475730896\n",
            "Batch 4 validation loss: 0.932482898235321\n",
            "Batch 5 validation loss: 0.8830187916755676\n",
            "Batch 6 validation loss: 0.9419034123420715\n",
            "Batch 7 validation loss: 0.955898642539978\n",
            "Batch 8 validation loss: 0.9899043440818787\n",
            "Batch 9 validation loss: 0.9761426448822021\n",
            "Batch 10 validation loss: 0.9457051753997803\n",
            "Batch 11 validation loss: 1.1976834535598755\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 6 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.8171 Learning Rate 0.00129258\n",
            "Batch 11 Loss 0.8254 Learning Rate 0.0012912\n",
            "Batch 21 Loss 0.9944 Learning Rate 0.00128983\n",
            "Batch 31 Loss 0.9674 Learning Rate 0.00128845\n",
            "Batch 41 Loss 0.725 Learning Rate 0.00128709\n",
            "Batch 51 Loss 0.8504 Learning Rate 0.00128573\n",
            "Batch 61 Loss 1.3835 Learning Rate 0.00128437\n",
            "Batch 71 Loss 1.0563 Learning Rate 0.00128301\n",
            "Batch 81 Loss 1.0325 Learning Rate 0.00128166\n",
            "Batch 91 Loss 1.0476 Learning Rate 0.00128032\n",
            "Batch 101 Loss 1.2851 Learning Rate 0.00127898\n",
            "Batch 111 Loss 1.0636 Learning Rate 0.00127764\n",
            "Batch 121 Loss 1.3309 Learning Rate 0.00127631\n",
            "Batch 131 Loss 0.7725 Learning Rate 0.00127498\n",
            "Batch 141 Loss 0.8504 Learning Rate 0.00127365\n",
            "Batch 151 Loss 0.7914 Learning Rate 0.00127233\n",
            "Batch 161 Loss 0.9222 Learning Rate 0.00127102\n",
            "Batch 171 Loss 0.8482 Learning Rate 0.00126971\n",
            "Batch 181 Loss 0.8426 Learning Rate 0.0012684\n",
            "Batch 191 Loss 0.7714 Learning Rate 0.00126709\n",
            "Batch 201 Loss 0.9308 Learning Rate 0.00126579\n",
            "Batch 211 Loss 0.8177 Learning Rate 0.0012645\n",
            "Batch 221 Loss 0.869 Learning Rate 0.00126321\n",
            "Batch 231 Loss 0.7889 Learning Rate 0.00126192\n",
            "Batch 241 Loss 0.831 Learning Rate 0.00126063\n",
            "Batch 251 Loss 0.8569 Learning Rate 0.00125935\n",
            "Batch 261 Loss 0.8235 Learning Rate 0.00125808\n",
            "Batch 271 Loss 0.73 Learning Rate 0.00125681\n",
            "Batch 281 Loss 0.9109 Learning Rate 0.00125554\n",
            "Batch 291 Loss 0.9236 Learning Rate 0.00125427\n",
            "Batch 301 Loss 0.7972 Learning Rate 0.00125301\n",
            "Batch 311 Loss 0.988 Learning Rate 0.00125175\n",
            "Batch 321 Loss 1.0063 Learning Rate 0.0012505\n",
            "Batch 331 Loss 2.2879 Learning Rate 0.00124925\n",
            "Batch 341 Loss 0.9363 Learning Rate 0.001248\n",
            "Batch 351 Loss 0.997 Learning Rate 0.00124676\n",
            "Batch 361 Loss 0.7612 Learning Rate 0.00124552\n",
            "Batch 371 Loss 0.8511 Learning Rate 0.00124429\n",
            "Batch 381 Loss 0.8025 Learning Rate 0.00124306\n",
            "Batch 391 Loss 0.9157 Learning Rate 0.00124183\n",
            "Batch 401 Loss 0.8272 Learning Rate 0.00124061\n",
            "Batch 411 Loss 0.8958 Learning Rate 0.00123939\n",
            "Batch 421 Loss 0.9724 Learning Rate 0.00123817\n",
            "Batch 431 Loss 0.9864 Learning Rate 0.00123696\n",
            "Batch 441 Loss 0.9059 Learning Rate 0.00123575\n",
            "Batch 451 Loss 0.7057 Learning Rate 0.00123454\n",
            "Batch 461 Loss 1.0674 Learning Rate 0.00123334\n",
            "Batch 471 Loss 1.0869 Learning Rate 0.00123214\n",
            "Batch 481 Loss 1.0688 Learning Rate 0.00123094\n",
            "Batch 491 Loss 1.0777 Learning Rate 0.00122975\n",
            "Batch 501 Loss 0.9497 Learning Rate 0.00122856\n",
            "Batch 511 Loss 1.0517 Learning Rate 0.00122738\n",
            "Batch 521 Loss 0.9496 Learning Rate 0.0012262\n",
            "Batch 531 Loss 0.8482 Learning Rate 0.00122502\n",
            "Batch 541 Loss 0.8794 Learning Rate 0.00122384\n",
            "Batch 551 Loss 1.0995 Learning Rate 0.00122267\n",
            "Batch 561 Loss 0.9942 Learning Rate 0.0012215\n",
            "Batch 571 Loss 1.1524 Learning Rate 0.00122034\n",
            "Batch 581 Loss 0.8937 Learning Rate 0.00121918\n",
            "Batch 591 Loss 0.9345 Learning Rate 0.00121802\n",
            "Batch 601 Loss 0.7317 Learning Rate 0.00121687\n",
            "Batch 611 Loss 0.8372 Learning Rate 0.00121571\n",
            "Batch 621 Loss 0.7459 Learning Rate 0.00121457\n",
            "Batch 631 Loss 0.8657 Learning Rate 0.00121342\n",
            "Batch 641 Loss 0.8712 Learning Rate 0.00121228\n",
            "Batch 651 Loss 0.872 Learning Rate 0.00121114\n",
            "Batch 661 Loss 1.5367 Learning Rate 0.00121\n",
            "Batch 671 Loss 0.9421 Learning Rate 0.00120887\n",
            "Batch 681 Loss 0.7767 Learning Rate 0.00120774\n",
            "Batch 691 Loss 0.9556 Learning Rate 0.00120662\n",
            "Batch 701 Loss 1.0193 Learning Rate 0.00120549\n",
            "Batch 711 Loss 0.8796 Learning Rate 0.00120438\n",
            "Batch 721 Loss 0.9323 Learning Rate 0.00120326\n",
            "Batch 731 Loss 0.9224 Learning Rate 0.00120215\n",
            "Batch 741 Loss 0.8521 Learning Rate 0.00120104\n",
            "Batch 751 Loss 1.0759 Learning Rate 0.00119993\n",
            "Batch 761 Loss 0.9068 Learning Rate 0.00119882\n",
            "Batch 771 Loss 0.6642 Learning Rate 0.00119772\n",
            "Batch 0 validation loss: 0.4665676951408386\n",
            "Batch 1 validation loss: 0.45079556107521057\n",
            "Batch 2 validation loss: 1.3214917182922363\n",
            "Batch 3 validation loss: 0.6405888795852661\n",
            "Batch 4 validation loss: 0.7170305252075195\n",
            "Batch 5 validation loss: 0.6961480975151062\n",
            "Batch 6 validation loss: 0.739130973815918\n",
            "Batch 7 validation loss: 0.716110348701477\n",
            "Batch 8 validation loss: 0.7118737697601318\n",
            "Batch 9 validation loss: 0.7560623288154602\n",
            "Batch 10 validation loss: 0.715940535068512\n",
            "Batch 11 validation loss: 0.9904666543006897\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 7 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.7538 Learning Rate 0.00119673\n",
            "Batch 11 Loss 0.8915 Learning Rate 0.00119564\n",
            "Batch 21 Loss 0.7922 Learning Rate 0.00119455\n",
            "Batch 31 Loss 0.5336 Learning Rate 0.00119346\n",
            "Batch 41 Loss 0.171 Learning Rate 0.00119237\n",
            "Batch 51 Loss 0.7394 Learning Rate 0.00119129\n",
            "Batch 61 Loss 0.6714 Learning Rate 0.00119021\n",
            "Batch 71 Loss 0.7159 Learning Rate 0.00118913\n",
            "Batch 81 Loss 0.5758 Learning Rate 0.00118805\n",
            "Batch 91 Loss 0.9304 Learning Rate 0.00118698\n",
            "Batch 101 Loss 0.7857 Learning Rate 0.00118591\n",
            "Batch 111 Loss 0.7171 Learning Rate 0.00118485\n",
            "Batch 121 Loss 0.7324 Learning Rate 0.00118378\n",
            "Batch 131 Loss 0.733 Learning Rate 0.00118272\n",
            "Batch 141 Loss 0.7016 Learning Rate 0.00118167\n",
            "Batch 151 Loss 0.6528 Learning Rate 0.00118061\n",
            "Batch 161 Loss 0.7713 Learning Rate 0.00117956\n",
            "Batch 171 Loss 0.7264 Learning Rate 0.00117851\n",
            "Batch 181 Loss 0.781 Learning Rate 0.00117747\n",
            "Batch 191 Loss 0.6932 Learning Rate 0.00117642\n",
            "Batch 201 Loss 0.6372 Learning Rate 0.00117538\n",
            "Batch 211 Loss 0.7783 Learning Rate 0.00117434\n",
            "Batch 221 Loss 0.715 Learning Rate 0.00117331\n",
            "Batch 231 Loss 0.7276 Learning Rate 0.00117228\n",
            "Batch 241 Loss 0.8036 Learning Rate 0.00117125\n",
            "Batch 251 Loss 0.6086 Learning Rate 0.00117022\n",
            "Batch 261 Loss 0.7925 Learning Rate 0.00116919\n",
            "Batch 271 Loss 0.7759 Learning Rate 0.00116817\n",
            "Batch 281 Loss 0.6372 Learning Rate 0.00116715\n",
            "Batch 291 Loss 0.6384 Learning Rate 0.00116614\n",
            "Batch 301 Loss 0.9054 Learning Rate 0.00116512\n",
            "Batch 311 Loss 0.7343 Learning Rate 0.00116411\n",
            "Batch 321 Loss 0.6939 Learning Rate 0.00116311\n",
            "Batch 331 Loss 0.7108 Learning Rate 0.0011621\n",
            "Batch 341 Loss 0.7792 Learning Rate 0.0011611\n",
            "Batch 351 Loss 0.8407 Learning Rate 0.0011601\n",
            "Batch 361 Loss 0.956 Learning Rate 0.0011591\n",
            "Batch 371 Loss 0.7226 Learning Rate 0.0011581\n",
            "Batch 381 Loss 0.7227 Learning Rate 0.00115711\n",
            "Batch 391 Loss 0.7065 Learning Rate 0.00115612\n",
            "Batch 401 Loss 0.6881 Learning Rate 0.00115513\n",
            "Batch 411 Loss 0.8165 Learning Rate 0.00115415\n",
            "Batch 421 Loss 0.7624 Learning Rate 0.00115316\n",
            "Batch 431 Loss 0.8081 Learning Rate 0.00115218\n",
            "Batch 441 Loss 0.7743 Learning Rate 0.00115121\n",
            "Batch 451 Loss 0.7806 Learning Rate 0.00115023\n",
            "Batch 461 Loss 0.8507 Learning Rate 0.00114926\n",
            "Batch 471 Loss 0.8256 Learning Rate 0.00114829\n",
            "Batch 481 Loss 0.6672 Learning Rate 0.00114732\n",
            "Batch 491 Loss 0.9034 Learning Rate 0.00114635\n",
            "Batch 501 Loss 0.7145 Learning Rate 0.00114539\n",
            "Batch 511 Loss 0.6187 Learning Rate 0.00114443\n",
            "Batch 521 Loss 0.7169 Learning Rate 0.00114347\n",
            "Batch 531 Loss 0.6325 Learning Rate 0.00114252\n",
            "Batch 541 Loss 0.6369 Learning Rate 0.00114156\n",
            "Batch 551 Loss 0.9709 Learning Rate 0.00114061\n",
            "Batch 561 Loss 0.7307 Learning Rate 0.00113966\n",
            "Batch 571 Loss 0.6771 Learning Rate 0.00113872\n",
            "Batch 581 Loss 0.7553 Learning Rate 0.00113777\n",
            "Batch 591 Loss 0.7629 Learning Rate 0.00113683\n",
            "Batch 601 Loss 0.8769 Learning Rate 0.00113589\n",
            "Batch 611 Loss 0.6918 Learning Rate 0.00113496\n",
            "Batch 621 Loss 0.7643 Learning Rate 0.00113402\n",
            "Batch 631 Loss 0.7762 Learning Rate 0.00113309\n",
            "Batch 641 Loss 0.9416 Learning Rate 0.00113216\n",
            "Batch 651 Loss 0.7158 Learning Rate 0.00113123\n",
            "Batch 661 Loss 0.7253 Learning Rate 0.00113031\n",
            "Batch 671 Loss 0.7409 Learning Rate 0.00112938\n",
            "Batch 681 Loss 0.8288 Learning Rate 0.00112846\n",
            "Batch 691 Loss 0.8999 Learning Rate 0.00112755\n",
            "Batch 701 Loss 0.82 Learning Rate 0.00112663\n",
            "Batch 711 Loss 0.7994 Learning Rate 0.00112572\n",
            "Batch 721 Loss 0.8388 Learning Rate 0.0011248\n",
            "Batch 731 Loss 0.6471 Learning Rate 0.00112389\n",
            "Batch 741 Loss 0.7608 Learning Rate 0.00112299\n",
            "Batch 751 Loss 0.9378 Learning Rate 0.00112208\n",
            "Batch 761 Loss 0.863 Learning Rate 0.00112118\n",
            "Batch 771 Loss 0.6969 Learning Rate 0.00112028\n",
            "Batch 0 validation loss: 0.3930329978466034\n",
            "Batch 1 validation loss: 0.36504048109054565\n",
            "Batch 2 validation loss: 1.1611366271972656\n",
            "Batch 3 validation loss: 0.5230439901351929\n",
            "Batch 4 validation loss: 0.5534433722496033\n",
            "Batch 5 validation loss: 0.5497424602508545\n",
            "Batch 6 validation loss: 0.5932779312133789\n",
            "Batch 7 validation loss: 0.5510127544403076\n",
            "Batch 8 validation loss: 0.5820673108100891\n",
            "Batch 9 validation loss: 0.5886112451553345\n",
            "Batch 10 validation loss: 0.5489278435707092\n",
            "Batch 11 validation loss: 0.6654669046401978\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 8 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.4967 Learning Rate 0.00111947\n",
            "Batch 11 Loss 0.6002 Learning Rate 0.00111857\n",
            "Batch 21 Loss 0.6754 Learning Rate 0.00111768\n",
            "Batch 31 Loss 0.5558 Learning Rate 0.00111678\n",
            "Batch 41 Loss 0.5981 Learning Rate 0.00111589\n",
            "Batch 51 Loss 0.6945 Learning Rate 0.00111501\n",
            "Batch 61 Loss 0.6448 Learning Rate 0.00111412\n",
            "Batch 71 Loss 0.4831 Learning Rate 0.00111324\n",
            "Batch 81 Loss 0.5239 Learning Rate 0.00111235\n",
            "Batch 91 Loss 0.659 Learning Rate 0.00111147\n",
            "Batch 101 Loss 0.6001 Learning Rate 0.0011106\n",
            "Batch 111 Loss 0.5379 Learning Rate 0.00110972\n",
            "Batch 121 Loss 0.4965 Learning Rate 0.00110885\n",
            "Batch 131 Loss 0.6429 Learning Rate 0.00110797\n",
            "Batch 141 Loss 0.618 Learning Rate 0.00110711\n",
            "Batch 151 Loss 0.5104 Learning Rate 0.00110624\n",
            "Batch 161 Loss 0.5412 Learning Rate 0.00110537\n",
            "Batch 171 Loss 0.6893 Learning Rate 0.00110451\n",
            "Batch 181 Loss 0.1795 Learning Rate 0.00110365\n",
            "Batch 191 Loss 0.6027 Learning Rate 0.00110279\n",
            "Batch 201 Loss 0.5779 Learning Rate 0.00110193\n",
            "Batch 211 Loss 0.6166 Learning Rate 0.00110108\n",
            "Batch 221 Loss 0.611 Learning Rate 0.00110022\n",
            "Batch 231 Loss 0.1946 Learning Rate 0.00109937\n",
            "Batch 241 Loss 0.5676 Learning Rate 0.00109852\n",
            "Batch 251 Loss 0.6576 Learning Rate 0.00109767\n",
            "Batch 261 Loss 0.6083 Learning Rate 0.00109683\n",
            "Batch 271 Loss 0.5916 Learning Rate 0.00109599\n",
            "Batch 281 Loss 0.7032 Learning Rate 0.00109514\n",
            "Batch 291 Loss 0.6315 Learning Rate 0.0010943\n",
            "Batch 301 Loss 0.707 Learning Rate 0.00109347\n",
            "Batch 311 Loss 0.5723 Learning Rate 0.00109263\n",
            "Batch 321 Loss 0.5919 Learning Rate 0.0010918\n",
            "Batch 331 Loss 0.5823 Learning Rate 0.00109096\n",
            "Batch 341 Loss 0.5979 Learning Rate 0.00109013\n",
            "Batch 351 Loss 0.5822 Learning Rate 0.00108931\n",
            "Batch 361 Loss 0.5909 Learning Rate 0.00108848\n",
            "Batch 371 Loss 0.8247 Learning Rate 0.00108766\n",
            "Batch 381 Loss 0.7275 Learning Rate 0.00108683\n",
            "Batch 391 Loss 0.6606 Learning Rate 0.00108601\n",
            "Batch 401 Loss 0.7158 Learning Rate 0.00108519\n",
            "Batch 411 Loss 0.6689 Learning Rate 0.00108438\n",
            "Batch 421 Loss 0.5744 Learning Rate 0.00108356\n",
            "Batch 431 Loss 0.6248 Learning Rate 0.00108275\n",
            "Batch 441 Loss 0.6782 Learning Rate 0.00108194\n",
            "Batch 451 Loss 0.601 Learning Rate 0.00108113\n",
            "Batch 461 Loss 0.6994 Learning Rate 0.00108032\n",
            "Batch 471 Loss 0.6008 Learning Rate 0.00107951\n",
            "Batch 481 Loss 0.7152 Learning Rate 0.00107871\n",
            "Batch 491 Loss 0.9382 Learning Rate 0.00107791\n",
            "Batch 501 Loss 0.6869 Learning Rate 0.00107711\n",
            "Batch 511 Loss 0.6443 Learning Rate 0.00107631\n",
            "Batch 521 Loss 0.6215 Learning Rate 0.00107551\n",
            "Batch 531 Loss 0.6212 Learning Rate 0.00107471\n",
            "Batch 541 Loss 0.7392 Learning Rate 0.00107392\n",
            "Batch 551 Loss 0.6691 Learning Rate 0.00107313\n",
            "Batch 561 Loss 0.5771 Learning Rate 0.00107234\n",
            "Batch 571 Loss 0.839 Learning Rate 0.00107155\n",
            "Batch 581 Loss 0.7998 Learning Rate 0.00107076\n",
            "Batch 591 Loss 0.7205 Learning Rate 0.00106998\n",
            "Batch 601 Loss 0.6522 Learning Rate 0.0010692\n",
            "Batch 611 Loss 0.6336 Learning Rate 0.00106842\n",
            "Batch 621 Loss 0.638 Learning Rate 0.00106764\n",
            "Batch 631 Loss 0.6563 Learning Rate 0.00106686\n",
            "Batch 641 Loss 0.621 Learning Rate 0.00106608\n",
            "Batch 651 Loss 0.6995 Learning Rate 0.00106531\n",
            "Batch 661 Loss 0.6499 Learning Rate 0.00106453\n",
            "Batch 671 Loss 0.6619 Learning Rate 0.00106376\n",
            "Batch 681 Loss 0.689 Learning Rate 0.00106299\n",
            "Batch 691 Loss 0.6085 Learning Rate 0.00106222\n",
            "Batch 701 Loss 0.7152 Learning Rate 0.00106146\n",
            "Batch 711 Loss 0.5699 Learning Rate 0.00106069\n",
            "Batch 721 Loss 1.6445 Learning Rate 0.00105993\n",
            "Batch 731 Loss 0.6678 Learning Rate 0.00105917\n",
            "Batch 741 Loss 0.6379 Learning Rate 0.00105841\n",
            "Batch 751 Loss 0.5661 Learning Rate 0.00105765\n",
            "Batch 761 Loss 0.8018 Learning Rate 0.0010569\n",
            "Batch 771 Loss 0.6571 Learning Rate 0.00105614\n",
            "Batch 0 validation loss: 0.4039313793182373\n",
            "Batch 1 validation loss: 0.29193130135536194\n",
            "Batch 2 validation loss: 0.9486050009727478\n",
            "Batch 3 validation loss: 0.4270579218864441\n",
            "Batch 4 validation loss: 0.4495542049407959\n",
            "Batch 5 validation loss: 0.4522691071033478\n",
            "Batch 6 validation loss: 0.49646058678627014\n",
            "Batch 7 validation loss: 0.48402562737464905\n",
            "Batch 8 validation loss: 0.4807291626930237\n",
            "Batch 9 validation loss: 0.4887714982032776\n",
            "Batch 10 validation loss: 0.44274988770484924\n",
            "Batch 11 validation loss: 0.5961697697639465\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 9 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.0031 Learning Rate 0.00105546\n",
            "Batch 11 Loss 0.6001 Learning Rate 0.00105471\n",
            "Batch 21 Loss 0.6435 Learning Rate 0.00105396\n",
            "Batch 31 Loss 0.4653 Learning Rate 0.00105321\n",
            "Batch 41 Loss 0.6003 Learning Rate 0.00105247\n",
            "Batch 51 Loss 0.4658 Learning Rate 0.00105172\n",
            "Batch 61 Loss 0.482 Learning Rate 0.00105098\n",
            "Batch 71 Loss 0.455 Learning Rate 0.00105023\n",
            "Batch 81 Loss 0.5276 Learning Rate 0.00104949\n",
            "Batch 91 Loss 0.4917 Learning Rate 0.00104876\n",
            "Batch 101 Loss 0.5616 Learning Rate 0.00104802\n",
            "Batch 111 Loss 0.6583 Learning Rate 0.00104728\n",
            "Batch 121 Loss 0.5341 Learning Rate 0.00104655\n",
            "Batch 131 Loss 0.4446 Learning Rate 0.00104581\n",
            "Batch 141 Loss 0.4592 Learning Rate 0.00104508\n",
            "Batch 151 Loss 0.5343 Learning Rate 0.00104435\n",
            "Batch 161 Loss 0.5133 Learning Rate 0.00104363\n",
            "Batch 171 Loss 0.6021 Learning Rate 0.0010429\n",
            "Batch 181 Loss 0.5355 Learning Rate 0.00104217\n",
            "Batch 191 Loss 0.4893 Learning Rate 0.00104145\n",
            "Batch 201 Loss 0.5218 Learning Rate 0.00104073\n",
            "Batch 211 Loss 0.5645 Learning Rate 0.00104001\n",
            "Batch 221 Loss 0.5786 Learning Rate 0.00103929\n",
            "Batch 231 Loss 0.4966 Learning Rate 0.00103857\n",
            "Batch 241 Loss 0.5012 Learning Rate 0.00103785\n",
            "Batch 251 Loss 0.4905 Learning Rate 0.00103714\n",
            "Batch 261 Loss 0.617 Learning Rate 0.00103643\n",
            "Batch 271 Loss 0.2205 Learning Rate 0.00103571\n",
            "Batch 281 Loss 0.5403 Learning Rate 0.001035\n",
            "Batch 291 Loss 0.5707 Learning Rate 0.00103429\n",
            "Batch 301 Loss 0.5809 Learning Rate 0.00103359\n",
            "Batch 311 Loss 0.5893 Learning Rate 0.00103288\n",
            "Batch 321 Loss 0.5728 Learning Rate 0.00103218\n",
            "Batch 331 Loss 0.5168 Learning Rate 0.00103147\n",
            "Batch 341 Loss 0.5119 Learning Rate 0.00103077\n",
            "Batch 351 Loss 0.5224 Learning Rate 0.00103007\n",
            "Batch 361 Loss 1.0298 Learning Rate 0.00102937\n",
            "Batch 371 Loss 0.5252 Learning Rate 0.00102868\n",
            "Batch 381 Loss 0.5315 Learning Rate 0.00102798\n",
            "Batch 391 Loss 0.4955 Learning Rate 0.00102729\n",
            "Batch 401 Loss 0.6409 Learning Rate 0.00102659\n",
            "Batch 411 Loss 0.5808 Learning Rate 0.0010259\n",
            "Batch 421 Loss 0.5905 Learning Rate 0.00102521\n",
            "Batch 431 Loss 0.6765 Learning Rate 0.00102452\n",
            "Batch 441 Loss 0.5312 Learning Rate 0.00102383\n",
            "Batch 451 Loss 0.58 Learning Rate 0.00102315\n",
            "Batch 461 Loss 0.6214 Learning Rate 0.00102246\n",
            "Batch 471 Loss 0.7683 Learning Rate 0.00102178\n",
            "Batch 481 Loss 0.5476 Learning Rate 0.0010211\n",
            "Batch 491 Loss 0.496 Learning Rate 0.00102042\n",
            "Batch 501 Loss 0.685 Learning Rate 0.00101974\n",
            "Batch 511 Loss 0.5815 Learning Rate 0.00101906\n",
            "Batch 521 Loss 0.6463 Learning Rate 0.00101838\n",
            "Batch 531 Loss 0.5055 Learning Rate 0.00101771\n",
            "Batch 541 Loss 0.8173 Learning Rate 0.00101703\n",
            "Batch 551 Loss 0.6197 Learning Rate 0.00101636\n",
            "Batch 561 Loss 0.5904 Learning Rate 0.00101569\n",
            "Batch 571 Loss 0.537 Learning Rate 0.00101502\n",
            "Batch 581 Loss 0.6113 Learning Rate 0.00101435\n",
            "Batch 591 Loss 0.8106 Learning Rate 0.00101368\n",
            "Batch 601 Loss 0.6941 Learning Rate 0.00101302\n",
            "Batch 611 Loss 0.5658 Learning Rate 0.00101235\n",
            "Batch 621 Loss 0.514 Learning Rate 0.00101169\n",
            "Batch 631 Loss 0.5953 Learning Rate 0.00101103\n",
            "Batch 641 Loss 0.5114 Learning Rate 0.00101037\n",
            "Batch 651 Loss 0.5166 Learning Rate 0.00100971\n",
            "Batch 661 Loss 0.5482 Learning Rate 0.00100905\n",
            "Batch 671 Loss 0.5834 Learning Rate 0.00100839\n",
            "Batch 681 Loss 0.6263 Learning Rate 0.00100774\n",
            "Batch 691 Loss 0.4988 Learning Rate 0.00100708\n",
            "Batch 701 Loss 0.5453 Learning Rate 0.00100643\n",
            "Batch 711 Loss 0.616 Learning Rate 0.00100578\n",
            "Batch 721 Loss 0.7096 Learning Rate 0.00100513\n",
            "Batch 731 Loss 0.5729 Learning Rate 0.00100448\n",
            "Batch 741 Loss 0.5979 Learning Rate 0.00100383\n",
            "Batch 751 Loss 0.5833 Learning Rate 0.00100318\n",
            "Batch 761 Loss 0.5581 Learning Rate 0.00100254\n",
            "Batch 771 Loss 0.8234 Learning Rate 0.00100189\n",
            "Batch 0 validation loss: 0.31478700041770935\n",
            "Batch 1 validation loss: 0.25234848260879517\n",
            "Batch 2 validation loss: 0.7351207733154297\n",
            "Batch 3 validation loss: 0.3693254292011261\n",
            "Batch 4 validation loss: 0.4265916049480438\n",
            "Batch 5 validation loss: 0.3914504051208496\n",
            "Batch 6 validation loss: 0.4159296154975891\n",
            "Batch 7 validation loss: 0.39437228441238403\n",
            "Batch 8 validation loss: 0.41132935881614685\n",
            "Batch 9 validation loss: 0.4004266858100891\n",
            "Batch 10 validation loss: 0.37732601165771484\n",
            "Batch 11 validation loss: 0.4698663651943207\n",
            "<torchtext.vocab.Vocab object at 0x7ffac8d86438>\n",
            "torch.Size([1, 60])\n",
            "torch.Size([13, 299])\n",
            "Translation:\tAs daughters of God , you were born to lead . \n",
            "Target:\ttensor(99, device='cuda:0')\n",
            "As tensor(570, device='cuda:0')\n",
            "daughters tensor(8, device='cuda:0')\n",
            "of tensor(35, device='cuda:0')\n",
            "God tensor(4, device='cuda:0')\n",
            ", tensor(20, device='cuda:0')\n",
            "you tensor(72, device='cuda:0')\n",
            "were tensor(629, device='cuda:0')\n",
            "born tensor(9, device='cuda:0')\n",
            "to tensor(508, device='cuda:0')\n",
            "lead tensor(6, device='cuda:0')\n",
            ". tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([16, 22])\n",
            "Translation:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "Target:\ttensor(365, device='cuda:0')\n",
            "Let tensor(28, device='cuda:0')\n",
            "not tensor(51, device='cuda:0')\n",
            "your tensor(171, device='cuda:0')\n",
            "heart tensor(21, device='cuda:0')\n",
            "be tensor(2123, device='cuda:0')\n",
            "troubled tensor(4, device='cuda:0')\n",
            ", tensor(1039, device='cuda:0')\n",
            "neither tensor(355, device='cuda:0')\n",
            "let tensor(31, device='cuda:0')\n",
            "it tensor(21, device='cuda:0')\n",
            "be tensor(1440, device='cuda:0')\n",
            "afraid tensor(6, device='cuda:0')\n",
            ". tensor(17, device='cuda:0')\n",
            "” tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([190, 21])\n",
            "Translation:\tThe other counselor was a prominent judge in the city . \n",
            "Target:\ttensor(30, device='cuda:0')\n",
            "The tensor(134, device='cuda:0')\n",
            "other tensor(1281, device='cuda:0')\n",
            "counselor tensor(26, device='cuda:0')\n",
            "was tensor(11, device='cuda:0')\n",
            "a tensor(3943, device='cuda:0')\n",
            "prominent tensor(1393, device='cuda:0')\n",
            "judge tensor(10, device='cuda:0')\n",
            "in tensor(5, device='cuda:0')\n",
            "the tensor(1137, device='cuda:0')\n",
            "city tensor(6, device='cuda:0')\n",
            ". tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([23, 171])\n",
            "Translation:\tMy wife , Harriet , was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "Target:\ttensor(174, device='cuda:0')\n",
            "My tensor(320, device='cuda:0')\n",
            "wife tensor(4, device='cuda:0')\n",
            ", tensor(3136, device='cuda:0')\n",
            "Harriet tensor(4, device='cuda:0')\n",
            ", tensor(26, device='cuda:0')\n",
            "was tensor(202, device='cuda:0')\n",
            "always tensor(5, device='cuda:0')\n",
            "the tensor(397, device='cuda:0')\n",
            "best tensor(73, device='cuda:0')\n",
            "at tensor(2003, device='cuda:0')\n",
            "finding tensor(349, device='cuda:0')\n",
            "something tensor(7463, device='cuda:0')\n",
            "inspirational tensor(4, device='cuda:0')\n",
            ", tensor(3769, device='cuda:0')\n",
            "uplifting tensor(4, device='cuda:0')\n",
            ", tensor(47, device='cuda:0')\n",
            "or tensor(8517, device='cuda:0')\n",
            "humorous tensor(9, device='cuda:0')\n",
            "to tensor(378, device='cuda:0')\n",
            "share tensor(6, device='cuda:0')\n",
            ". tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([26, 130])\n",
            "Translation:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "Target:\ttensor(14, device='cuda:0')\n",
            "“ tensor(104, device='cuda:0')\n",
            "And tensor(57, device='cuda:0')\n",
            "Jesus tensor(97, device='cuda:0')\n",
            "said tensor(123, device='cuda:0')\n",
            "unto tensor(62, device='cuda:0')\n",
            "them tensor(44, device='cuda:0')\n",
            ": tensor(2739, device='cuda:0')\n",
            "Pray tensor(43, device='cuda:0')\n",
            "on tensor(32, device='cuda:0')\n",
            "; tensor(2898, device='cuda:0')\n",
            "nevertheless tensor(53, device='cuda:0')\n",
            "they tensor(126, device='cuda:0')\n",
            "did tensor(28, device='cuda:0')\n",
            "not tensor(3526, device='cuda:0')\n",
            "cease tensor(9, device='cuda:0')\n",
            "to tensor(237, device='cuda:0')\n",
            "pray tensor(17, device='cuda:0')\n",
            "” tensor(55, device='cuda:0')\n",
            "( tensor(371, device='cuda:0')\n",
            "3 tensor(206, device='cuda:0')\n",
            "Nephi tensor(11165, device='cuda:0')\n",
            "19:26 tensor(56, device='cuda:0')\n",
            ") tensor(6, device='cuda:0')\n",
            ". tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([36, 113])\n",
            "Translation:\t“ And he that receiveth my Father ’s kingdom , and therefore all that my Father hath shall be given unto him . ” \n",
            "Target:\ttensor(14, device='cuda:0')\n",
            "“ tensor(104, device='cuda:0')\n",
            "And tensor(45, device='cuda:0')\n",
            "he tensor(12, device='cuda:0')\n",
            "that tensor(1666, device='cuda:0')\n",
            "receiveth tensor(39, device='cuda:0')\n",
            "my tensor(75, device='cuda:0')\n",
            "Father tensor(1666, device='cuda:0')\n",
            "receiveth tensor(39, device='cuda:0')\n",
            "my tensor(75, device='cuda:0')\n",
            "Father tensor(33, device='cuda:0')\n",
            "’s tensor(337, device='cuda:0')\n",
            "kingdom tensor(32, device='cuda:0')\n",
            "; tensor(977, device='cuda:0')\n",
            "therefore tensor(37, device='cuda:0')\n",
            "all tensor(12, device='cuda:0')\n",
            "that tensor(39, device='cuda:0')\n",
            "my tensor(75, device='cuda:0')\n",
            "Father tensor(664, device='cuda:0')\n",
            "hath tensor(135, device='cuda:0')\n",
            "shall tensor(21, device='cuda:0')\n",
            "be tensor(228, device='cuda:0')\n",
            "given tensor(123, device='cuda:0')\n",
            "unto tensor(95, device='cuda:0')\n",
            "him tensor(6, device='cuda:0')\n",
            ". tensor(17, device='cuda:0')\n",
            "” tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([43, 95])\n",
            "Translation:\tWhile it is good to pray and work for physical protection and healing during our mortal existence , our ultimate focus should be on the spiritual miracles that are available to all of God ’s children . \n",
            "Target:\ttensor(600, device='cuda:0')\n",
            "While tensor(31, device='cuda:0')\n",
            "it tensor(15, device='cuda:0')\n",
            "is tensor(154, device='cuda:0')\n",
            "good tensor(9, device='cuda:0')\n",
            "to tensor(237, device='cuda:0')\n",
            "pray tensor(19, device='cuda:0')\n",
            "for tensor(7, device='cuda:0')\n",
            "and tensor(124, device='cuda:0')\n",
            "work tensor(19, device='cuda:0')\n",
            "for tensor(606, device='cuda:0')\n",
            "physical tensor(1492, device='cuda:0')\n",
            "protection tensor(7, device='cuda:0')\n",
            "and tensor(782, device='cuda:0')\n",
            "healing tensor(423, device='cuda:0')\n",
            "during tensor(18, device='cuda:0')\n",
            "our tensor(437, device='cuda:0')\n",
            "mortal tensor(1802, device='cuda:0')\n",
            "existence tensor(4, device='cuda:0')\n",
            ", tensor(18, device='cuda:0')\n",
            "our tensor(5769, device='cuda:0')\n",
            "supreme tensor(858, device='cuda:0')\n",
            "focus tensor(175, device='cuda:0')\n",
            "should tensor(21, device='cuda:0')\n",
            "be tensor(43, device='cuda:0')\n",
            "on tensor(5, device='cuda:0')\n",
            "the tensor(183, device='cuda:0')\n",
            "spiritual tensor(1297, device='cuda:0')\n",
            "miracles tensor(12, device='cuda:0')\n",
            "that tensor(23, device='cuda:0')\n",
            "are tensor(607, device='cuda:0')\n",
            "available tensor(9, device='cuda:0')\n",
            "to tensor(37, device='cuda:0')\n",
            "all tensor(8, device='cuda:0')\n",
            "of tensor(35, device='cuda:0')\n",
            "God tensor(33, device='cuda:0')\n",
            "’s tensor(80, device='cuda:0')\n",
            "children tensor(6, device='cuda:0')\n",
            ". tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([54, 75])\n",
            "Translation:\tAnother regret people expressed was the little they felt they could not become the person they felt they could or should have been . As they looked back at their lives , they realized that they never lived up to their potential , many songs remained unsung . \n",
            "Target:\ttensor(1331, device='cuda:0')\n",
            "Another tensor(3754, device='cuda:0')\n",
            "regret tensor(119, device='cuda:0')\n",
            "people tensor(1188, device='cuda:0')\n",
            "expressed tensor(26, device='cuda:0')\n",
            "was tensor(12, device='cuda:0')\n",
            "that tensor(53, device='cuda:0')\n",
            "they tensor(3017, device='cuda:0')\n",
            "failed tensor(9, device='cuda:0')\n",
            "to tensor(168, device='cuda:0')\n",
            "become tensor(5, device='cuda:0')\n",
            "the tensor(497, device='cuda:0')\n",
            "person tensor(53, device='cuda:0')\n",
            "they tensor(231, device='cuda:0')\n",
            "felt tensor(53, device='cuda:0')\n",
            "they tensor(130, device='cuda:0')\n",
            "could tensor(7, device='cuda:0')\n",
            "and tensor(175, device='cuda:0')\n",
            "should tensor(27, device='cuda:0')\n",
            "have tensor(98, device='cuda:0')\n",
            "been tensor(6, device='cuda:0')\n",
            ". tensor(147, device='cuda:0')\n",
            "When tensor(53, device='cuda:0')\n",
            "they tensor(728, device='cuda:0')\n",
            "looked tensor(278, device='cuda:0')\n",
            "back tensor(43, device='cuda:0')\n",
            "on tensor(52, device='cuda:0')\n",
            "their tensor(144, device='cuda:0')\n",
            "lives tensor(4, device='cuda:0')\n",
            ", tensor(53, device='cuda:0')\n",
            "they tensor(949, device='cuda:0')\n",
            "realized tensor(12, device='cuda:0')\n",
            "that tensor(53, device='cuda:0')\n",
            "they tensor(196, device='cuda:0')\n",
            "never tensor(737, device='cuda:0')\n",
            "lived tensor(136, device='cuda:0')\n",
            "up tensor(9, device='cuda:0')\n",
            "to tensor(52, device='cuda:0')\n",
            "their tensor(894, device='cuda:0')\n",
            "potential tensor(4, device='cuda:0')\n",
            ", tensor(12, device='cuda:0')\n",
            "that tensor(376, device='cuda:0')\n",
            "too tensor(118, device='cuda:0')\n",
            "many tensor(2758, device='cuda:0')\n",
            "songs tensor(2329, device='cuda:0')\n",
            "remained tensor(19141, device='cuda:0')\n",
            "unsung tensor(6, device='cuda:0')\n",
            ". tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([78, 52])\n",
            "Translation:\tThe Savior taught , “ This shall ye always do to all who repent and are baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always remember \n",
            "Target:\ttensor(30, device='cuda:0')\n",
            "The tensor(78, device='cuda:0')\n",
            "Savior tensor(172, device='cuda:0')\n",
            "taught tensor(44, device='cuda:0')\n",
            ": tensor(14, device='cuda:0')\n",
            "“ tensor(140, device='cuda:0')\n",
            "This tensor(135, device='cuda:0')\n",
            "shall tensor(179, device='cuda:0')\n",
            "ye tensor(202, device='cuda:0')\n",
            "always tensor(54, device='cuda:0')\n",
            "do tensor(9, device='cuda:0')\n",
            "to tensor(81, device='cuda:0')\n",
            "those tensor(41, device='cuda:0')\n",
            "who tensor(526, device='cuda:0')\n",
            "repent tensor(7, device='cuda:0')\n",
            "and tensor(23, device='cuda:0')\n",
            "are tensor(548, device='cuda:0')\n",
            "baptized tensor(10, device='cuda:0')\n",
            "in tensor(39, device='cuda:0')\n",
            "my tensor(159, device='cuda:0')\n",
            "name tensor(32, device='cuda:0')\n",
            "; tensor(7, device='cuda:0')\n",
            "and tensor(179, device='cuda:0')\n",
            "ye tensor(135, device='cuda:0')\n",
            "shall tensor(54, device='cuda:0')\n",
            "do tensor(31, device='cuda:0')\n",
            "it tensor(10, device='cuda:0')\n",
            "in tensor(2467, device='cuda:0')\n",
            "remembrance tensor(8, device='cuda:0')\n",
            "of tensor(39, device='cuda:0')\n",
            "my tensor(1271, device='cuda:0')\n",
            "blood tensor(4, device='cuda:0')\n",
            ", tensor(84, device='cuda:0')\n",
            "which tensor(13, device='cuda:0')\n",
            "I tensor(27, device='cuda:0')\n",
            "have tensor(3264, device='cuda:0')\n",
            "shed tensor(19, device='cuda:0')\n",
            "for tensor(20, device='cuda:0')\n",
            "you tensor(4, device='cuda:0')\n",
            ", tensor(12, device='cuda:0')\n",
            "that tensor(179, device='cuda:0')\n",
            "ye tensor(91, device='cuda:0')\n",
            "may tensor(308, device='cuda:0')\n",
            "witness tensor(123, device='cuda:0')\n",
            "unto tensor(5, device='cuda:0')\n",
            "the tensor(75, device='cuda:0')\n",
            "Father tensor(12, device='cuda:0')\n",
            "that tensor(179, device='cuda:0')\n",
            "ye tensor(54, device='cuda:0')\n",
            "do tensor(202, device='cuda:0')\n",
            "always tensor(216, device='cuda:0')\n",
            "remember tensor(64, device='cuda:0')\n",
            "me tensor(6, device='cuda:0')\n",
            ". tensor(104, device='cuda:0')\n",
            "And tensor(106, device='cuda:0')\n",
            "if tensor(179, device='cuda:0')\n",
            "ye tensor(54, device='cuda:0')\n",
            "do tensor(202, device='cuda:0')\n",
            "always tensor(216, device='cuda:0')\n",
            "remember tensor(64, device='cuda:0')\n",
            "me tensor(179, device='cuda:0')\n",
            "ye tensor(135, device='cuda:0')\n",
            "shall tensor(27, device='cuda:0')\n",
            "have tensor(39, device='cuda:0')\n",
            "my tensor(162, device='cuda:0')\n",
            "Spirit tensor(9, device='cuda:0')\n",
            "to tensor(21, device='cuda:0')\n",
            "be tensor(22, device='cuda:0')\n",
            "with tensor(20, device='cuda:0')\n",
            "you tensor(17, device='cuda:0')\n",
            "” tensor(55, device='cuda:0')\n",
            "( tensor(371, device='cuda:0')\n",
            "3 tensor(206, device='cuda:0')\n",
            "Nephi tensor(7894, device='cuda:0')\n",
            "18:11 tensor(56, device='cuda:0')\n",
            ") tensor(6, device='cuda:0')\n",
            ". tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([112, 36])\n",
            "Translation:\tAfter several years he returned to his hometown . However , the people refused to acknowledge his growth and improvement . To them , he was still just old and so hard , ” and they treated him that way . Eventually , this good man faded away to a shadow of his former successful self without using his \n",
            "Target:\ttensor(386, device='cuda:0')\n",
            "After tensor(880, device='cuda:0')\n",
            "several tensor(157, device='cuda:0')\n",
            "years tensor(45, device='cuda:0')\n",
            "he tensor(831, device='cuda:0')\n",
            "returned tensor(9, device='cuda:0')\n",
            "to tensor(42, device='cuda:0')\n",
            "his tensor(8097, device='cuda:0')\n",
            "hometown tensor(6, device='cuda:0')\n",
            ". tensor(627, device='cuda:0')\n",
            "However tensor(4, device='cuda:0')\n",
            ", tensor(5, device='cuda:0')\n",
            "the tensor(119, device='cuda:0')\n",
            "people tensor(3753, device='cuda:0')\n",
            "refused tensor(9, device='cuda:0')\n",
            "to tensor(1673, device='cuda:0')\n",
            "acknowledge tensor(42, device='cuda:0')\n",
            "his tensor(1134, device='cuda:0')\n",
            "growth tensor(7, device='cuda:0')\n",
            "and tensor(3248, device='cuda:0')\n",
            "improvement tensor(6, device='cuda:0')\n",
            ". tensor(291, device='cuda:0')\n",
            "To tensor(62, device='cuda:0')\n",
            "them tensor(4, device='cuda:0')\n",
            ", tensor(45, device='cuda:0')\n",
            "he tensor(26, device='cuda:0')\n",
            "was tensor(393, device='cuda:0')\n",
            "still tensor(181, device='cuda:0')\n",
            "just tensor(329, device='cuda:0')\n",
            "old tensor(14, device='cuda:0')\n",
            "“ tensor(87, device='cuda:0')\n",
            "so tensor(61, device='cuda:0')\n",
            "- tensor(7, device='cuda:0')\n",
            "and tensor(61, device='cuda:0')\n",
            "- tensor(87, device='cuda:0')\n",
            "so tensor(4, device='cuda:0')\n",
            ", tensor(17, device='cuda:0')\n",
            "” tensor(7, device='cuda:0')\n",
            "and tensor(53, device='cuda:0')\n",
            "they tensor(3058, device='cuda:0')\n",
            "treated tensor(95, device='cuda:0')\n",
            "him tensor(12, device='cuda:0')\n",
            "that tensor(128, device='cuda:0')\n",
            "way tensor(6, device='cuda:0')\n",
            ". tensor(2513, device='cuda:0')\n",
            "Eventually tensor(4, device='cuda:0')\n",
            ", tensor(38, device='cuda:0')\n",
            "this tensor(154, device='cuda:0')\n",
            "good tensor(132, device='cuda:0')\n",
            "man tensor(10748, device='cuda:0')\n",
            "faded tensor(296, device='cuda:0')\n",
            "away tensor(9, device='cuda:0')\n",
            "to tensor(11, device='cuda:0')\n",
            "a tensor(7226, device='cuda:0')\n",
            "shadow tensor(8, device='cuda:0')\n",
            "of tensor(42, device='cuda:0')\n",
            "his tensor(2042, device='cuda:0')\n",
            "former tensor(1646, device='cuda:0')\n",
            "successful tensor(683, device='cuda:0')\n",
            "self tensor(279, device='cuda:0')\n",
            "without tensor(192, device='cuda:0')\n",
            "being tensor(385, device='cuda:0')\n",
            "able tensor(9, device='cuda:0')\n",
            "to tensor(448, device='cuda:0')\n",
            "use tensor(42, device='cuda:0')\n",
            "his tensor(14094, device='cuda:0')\n",
            "marvelously tensor(2085, device='cuda:0')\n",
            "developed tensor(1748, device='cuda:0')\n",
            "talents tensor(9, device='cuda:0')\n",
            "to tensor(455, device='cuda:0')\n",
            "bless tensor(81, device='cuda:0')\n",
            "those tensor(41, device='cuda:0')\n",
            "who tensor(15706, device='cuda:0')\n",
            "derided tensor(7, device='cuda:0')\n",
            "and tensor(2932, device='cuda:0')\n",
            "rejected tensor(95, device='cuda:0')\n",
            "him tensor(471, device='cuda:0')\n",
            "once tensor(269, device='cuda:0')\n",
            "again tensor(6, device='cuda:0')\n",
            ". tensor(244, device='cuda:0')\n",
            "What tensor(11, device='cuda:0')\n",
            "a tensor(1652, device='cuda:0')\n",
            "loss tensor(4, device='cuda:0')\n",
            ", tensor(339, device='cuda:0')\n",
            "both tensor(19, device='cuda:0')\n",
            "for tensor(95, device='cuda:0')\n",
            "him tensor(7, device='cuda:0')\n",
            "and tensor(5, device='cuda:0')\n",
            "the tensor(1867, device='cuda:0')\n",
            "community tensor(156, device='cuda:0')\n",
            "! tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([136, 30])\n",
            "Translation:\tThe blessings you receive as you serve others are many . I ’ve often said , “ Oh , I ’ve got to get my visiting teaching ! ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I could see it as a weight , more than a blessing \n",
            "Target:\ttensor(30, device='cuda:0')\n",
            "The tensor(204, device='cuda:0')\n",
            "blessings tensor(20, device='cuda:0')\n",
            "you tensor(199, device='cuda:0')\n",
            "receive tensor(24, device='cuda:0')\n",
            "as tensor(20, device='cuda:0')\n",
            "you tensor(182, device='cuda:0')\n",
            "serve tensor(145, device='cuda:0')\n",
            "others tensor(23, device='cuda:0')\n",
            "are tensor(118, device='cuda:0')\n",
            "many tensor(6, device='cuda:0')\n",
            ". tensor(13, device='cuda:0')\n",
            "I tensor(27, device='cuda:0')\n",
            "have tensor(609, device='cuda:0')\n",
            "sometimes tensor(97, device='cuda:0')\n",
            "said tensor(4, device='cuda:0')\n",
            ", tensor(14, device='cuda:0')\n",
            "“ tensor(1573, device='cuda:0')\n",
            "Oh tensor(4, device='cuda:0')\n",
            ", tensor(13, device='cuda:0')\n",
            "I tensor(1018, device='cuda:0')\n",
            "’ve tensor(1183, device='cuda:0')\n",
            "got tensor(9, device='cuda:0')\n",
            "to tensor(408, device='cuda:0')\n",
            "get tensor(39, device='cuda:0')\n",
            "my tensor(983, device='cuda:0')\n",
            "visiting tensor(374, device='cuda:0')\n",
            "teaching tensor(354, device='cuda:0')\n",
            "done tensor(156, device='cuda:0')\n",
            "! tensor(17, device='cuda:0')\n",
            "” tensor(55, device='cuda:0')\n",
            "( tensor(519, device='cuda:0')\n",
            "Those tensor(72, device='cuda:0')\n",
            "were tensor(5, device='cuda:0')\n",
            "the tensor(272, device='cuda:0')\n",
            "times tensor(13, device='cuda:0')\n",
            "I tensor(6677, device='cuda:0')\n",
            "forgot tensor(13, device='cuda:0')\n",
            "I tensor(26, device='cuda:0')\n",
            "was tensor(983, device='cuda:0')\n",
            "visiting tensor(7, device='cuda:0')\n",
            "and tensor(374, device='cuda:0')\n",
            "teaching tensor(211, device='cuda:0')\n",
            "women tensor(6, device='cuda:0')\n",
            ". tensor(519, device='cuda:0')\n",
            "Those tensor(72, device='cuda:0')\n",
            "were tensor(5, device='cuda:0')\n",
            "the tensor(272, device='cuda:0')\n",
            "times tensor(13, device='cuda:0')\n",
            "I tensor(26, device='cuda:0')\n",
            "was tensor(861, device='cuda:0')\n",
            "looking tensor(73, device='cuda:0')\n",
            "at tensor(31, device='cuda:0')\n",
            "it tensor(24, device='cuda:0')\n",
            "as tensor(11, device='cuda:0')\n",
            "a tensor(1564, device='cuda:0')\n",
            "burden tensor(758, device='cuda:0')\n",
            "rather tensor(148, device='cuda:0')\n",
            "than tensor(11, device='cuda:0')\n",
            "a tensor(323, device='cuda:0')\n",
            "blessing tensor(6, device='cuda:0')\n",
            ". tensor(56, device='cuda:0')\n",
            ") tensor(13, device='cuda:0')\n",
            "I tensor(48, device='cuda:0')\n",
            "can tensor(4051, device='cuda:0')\n",
            "honestly tensor(238, device='cuda:0')\n",
            "say tensor(12, device='cuda:0')\n",
            "that tensor(66, device='cuda:0')\n",
            "when tensor(13, device='cuda:0')\n",
            "I tensor(449, device='cuda:0')\n",
            "went tensor(983, device='cuda:0')\n",
            "visiting tensor(374, device='cuda:0')\n",
            "teaching tensor(4, device='cuda:0')\n",
            ", tensor(13, device='cuda:0')\n",
            "I tensor(202, device='cuda:0')\n",
            "always tensor(231, device='cuda:0')\n",
            "felt tensor(406, device='cuda:0')\n",
            "better tensor(6, device='cuda:0')\n",
            ". tensor(13, device='cuda:0')\n",
            "I tensor(26, device='cuda:0')\n",
            "was tensor(1342, device='cuda:0')\n",
            "lifted tensor(4, device='cuda:0')\n",
            ", tensor(468, device='cuda:0')\n",
            "loved tensor(4, device='cuda:0')\n",
            ", tensor(7, device='cuda:0')\n",
            "and tensor(315, device='cuda:0')\n",
            "blessed tensor(4, device='cuda:0')\n",
            ", tensor(2355, device='cuda:0')\n",
            "usually tensor(242, device='cuda:0')\n",
            "much tensor(86, device='cuda:0')\n",
            "more tensor(148, device='cuda:0')\n",
            "than tensor(5, device='cuda:0')\n",
            "the tensor(736, device='cuda:0')\n",
            "sister tensor(13, device='cuda:0')\n",
            "I tensor(26, device='cuda:0')\n",
            "was tensor(983, device='cuda:0')\n",
            "visiting tensor(6, device='cuda:0')\n",
            ". tensor(174, device='cuda:0')\n",
            "My tensor(77, device='cuda:0')\n",
            "love tensor(1157, device='cuda:0')\n",
            "increased tensor(6, device='cuda:0')\n",
            ". tensor(174, device='cuda:0')\n",
            "My tensor(310, device='cuda:0')\n",
            "desire tensor(9, device='cuda:0')\n",
            "to tensor(182, device='cuda:0')\n",
            "serve tensor(1157, device='cuda:0')\n",
            "increased tensor(6, device='cuda:0')\n",
            ". tensor(104, device='cuda:0')\n",
            "And tensor(13, device='cuda:0')\n",
            "I tensor(130, device='cuda:0')\n",
            "could tensor(88, device='cuda:0')\n",
            "see tensor(79, device='cuda:0')\n",
            "what tensor(11, device='cuda:0')\n",
            "a tensor(559, device='cuda:0')\n",
            "beautiful tensor(128, device='cuda:0')\n",
            "way tensor(149, device='cuda:0')\n",
            "Heavenly tensor(75, device='cuda:0')\n",
            "Father tensor(67, device='cuda:0')\n",
            "has tensor(3460, device='cuda:0')\n",
            "planned tensor(19, device='cuda:0')\n",
            "for tensor(29, device='cuda:0')\n",
            "us tensor(9, device='cuda:0')\n",
            "to tensor(992, device='cuda:0')\n",
            "watch tensor(205, device='cuda:0')\n",
            "over tensor(7, device='cuda:0')\n",
            "and tensor(578, device='cuda:0')\n",
            "care tensor(19, device='cuda:0')\n",
            "for tensor(70, device='cuda:0')\n",
            "one tensor(241, device='cuda:0')\n",
            "another tensor(6, device='cuda:0')\n",
            ". tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([224, 14])\n",
            "Translation:\tAnother journal entry reads : “ The miracle … occurred to me in the Family History office of Grandmother , who presented me with a Danish copy of all my known ancestral night that I could get myself back from the Ancestral File . Most of them coming from the Church ’s fabric , I said to the idea \n",
            "Target:\ttensor(1331, device='cuda:0')\n",
            "Another tensor(4539, device='cuda:0')\n",
            "journal tensor(8064, device='cuda:0')\n",
            "entry tensor(2753, device='cuda:0')\n",
            "reads tensor(44, device='cuda:0')\n",
            ": tensor(14, device='cuda:0')\n",
            "“ tensor(30, device='cuda:0')\n",
            "The tensor(100, device='cuda:0')\n",
            "… tensor(1066, device='cuda:0')\n",
            "miracle tensor(19, device='cuda:0')\n",
            "for tensor(64, device='cuda:0')\n",
            "me tensor(2034, device='cuda:0')\n",
            "occurred tensor(10, device='cuda:0')\n",
            "in tensor(5, device='cuda:0')\n",
            "the tensor(865, device='cuda:0')\n",
            "Family tensor(1015, device='cuda:0')\n",
            "History tensor(1190, device='cuda:0')\n",
            "office tensor(8, device='cuda:0')\n",
            "of tensor(13533, device='cuda:0')\n",
            "Mel tensor(9863, device='cuda:0')\n",
            "Olsen tensor(41, device='cuda:0')\n",
            "who tensor(2111, device='cuda:0')\n",
            "presented tensor(64, device='cuda:0')\n",
            "me tensor(22, device='cuda:0')\n",
            "with tensor(11, device='cuda:0')\n",
            "a tensor(31225, device='cuda:0')\n",
            "printout tensor(8, device='cuda:0')\n",
            "of tensor(37, device='cuda:0')\n",
            "all tensor(39, device='cuda:0')\n",
            "my tensor(720, device='cuda:0')\n",
            "known tensor(12403, device='cuda:0')\n",
            "ancestral tensor(31130, device='cuda:0')\n",
            "pedigrees tensor(906, device='cuda:0')\n",
            "taken tensor(50, device='cuda:0')\n",
            "from tensor(5, device='cuda:0')\n",
            "the tensor(14386, device='cuda:0')\n",
            "update tensor(8, device='cuda:0')\n",
            "of tensor(5, device='cuda:0')\n",
            "the tensor(20400, device='cuda:0')\n",
            "Ancestral tensor(20986, device='cuda:0')\n",
            "File tensor(23106, device='cuda:0')\n",
            "computerized tensor(1758, device='cuda:0')\n",
            "records tensor(671, device='cuda:0')\n",
            "sent tensor(137, device='cuda:0')\n",
            "into tensor(5, device='cuda:0')\n",
            "the tensor(15889, device='cuda:0')\n",
            "genealogical tensor(1145, device='cuda:0')\n",
            "society tensor(6, device='cuda:0')\n",
            ". tensor(120, device='cuda:0')\n",
            "They tensor(249, device='cuda:0')\n",
            "came tensor(4000, device='cuda:0')\n",
            "mostly tensor(50, device='cuda:0')\n",
            "from tensor(5, device='cuda:0')\n",
            "the tensor(1758, device='cuda:0')\n",
            "records tensor(8, device='cuda:0')\n",
            "of tensor(5, device='cuda:0')\n",
            "the tensor(634, device='cuda:0')\n",
            "four tensor(893, device='cuda:0')\n",
            "generation tensor(33, device='cuda:0')\n",
            "’s tensor(1193, device='cuda:0')\n",
            "program tensor(5, device='cuda:0')\n",
            "the tensor(59, device='cuda:0')\n",
            "Church tensor(224, device='cuda:0')\n",
            "called tensor(19, device='cuda:0')\n",
            "for tensor(118, device='cuda:0')\n",
            "many tensor(157, device='cuda:0')\n",
            "years tensor(341, device='cuda:0')\n",
            "ago tensor(6, device='cuda:0')\n",
            ". tensor(13, device='cuda:0')\n",
            "I tensor(60, device='cuda:0')\n",
            "had tensor(98, device='cuda:0')\n",
            "been tensor(2162, device='cuda:0')\n",
            "overwhelmed tensor(22, device='cuda:0')\n",
            "with tensor(5, device='cuda:0')\n",
            "the tensor(447, device='cuda:0')\n",
            "thought tensor(8, device='cuda:0')\n",
            "of tensor(5, device='cuda:0')\n",
            "the tensor(4432, device='cuda:0')\n",
            "huge tensor(2453, device='cuda:0')\n",
            "task tensor(1133, device='cuda:0')\n",
            "ahead tensor(8, device='cuda:0')\n",
            "of tensor(64, device='cuda:0')\n",
            "me tensor(9, device='cuda:0')\n",
            "to tensor(1693, device='cuda:0')\n",
            "gather tensor(37, device='cuda:0')\n",
            "all tensor(39, device='cuda:0')\n",
            "my tensor(11466, device='cuda:0')\n",
            "ancestors’ tensor(3028, device='cuda:0')\n",
            "research tensor(1758, device='cuda:0')\n",
            "records tensor(50, device='cuda:0')\n",
            "from tensor(90, device='cuda:0')\n",
            "family tensor(2614, device='cuda:0')\n",
            "organizations tensor(9, device='cuda:0')\n",
            "to tensor(408, device='cuda:0')\n",
            "get tensor(62, device='cuda:0')\n",
            "them tensor(37, device='cuda:0')\n",
            "all tensor(10, device='cuda:0')\n",
            "in tensor(5, device='cuda:0')\n",
            "the tensor(2545, device='cuda:0')\n",
            "computer tensor(19, device='cuda:0')\n",
            "for tensor(5, device='cuda:0')\n",
            "the tensor(190, device='cuda:0')\n",
            "first tensor(23106, device='cuda:0')\n",
            "computerized tensor(5270, device='cuda:0')\n",
            "distribution tensor(8, device='cuda:0')\n",
            "of tensor(5, device='cuda:0')\n",
            "the tensor(20400, device='cuda:0')\n",
            "Ancestral tensor(20986, device='cuda:0')\n",
            "File tensor(6, device='cuda:0')\n",
            ". tensor(104, device='cuda:0')\n",
            "And tensor(114, device='cuda:0')\n",
            "there tensor(53, device='cuda:0')\n",
            "they tensor(37, device='cuda:0')\n",
            "all tensor(72, device='cuda:0')\n",
            "were tensor(4, device='cuda:0')\n",
            ", tensor(559, device='cuda:0')\n",
            "beautiful tensor(4, device='cuda:0')\n",
            ", tensor(1410, device='cuda:0')\n",
            "organized tensor(7, device='cuda:0')\n",
            "and tensor(18621, device='cuda:0')\n",
            "laser tensor(4223, device='cuda:0')\n",
            "printed tensor(7, device='cuda:0')\n",
            "and tensor(1985, device='cuda:0')\n",
            "sitting tensor(114, device='cuda:0')\n",
            "there tensor(43, device='cuda:0')\n",
            "on tensor(5, device='cuda:0')\n",
            "the tensor(8923, device='cuda:0')\n",
            "desk tensor(188, device='cuda:0')\n",
            "before tensor(64, device='cuda:0')\n",
            "me tensor(6, device='cuda:0')\n",
            ". tensor(13, device='cuda:0')\n",
            "I tensor(26, device='cuda:0')\n",
            "was tensor(87, device='cuda:0')\n",
            "so tensor(4466, device='cuda:0')\n",
            "thrilled tensor(7, device='cuda:0')\n",
            "and tensor(87, device='cuda:0')\n",
            "so tensor(2162, device='cuda:0')\n",
            "overwhelmed tensor(13, device='cuda:0')\n",
            "I tensor(181, device='cuda:0')\n",
            "just tensor(1604, device='cuda:0')\n",
            "sat tensor(114, device='cuda:0')\n",
            "there tensor(5471, device='cuda:0')\n",
            "stunned tensor(7, device='cuda:0')\n",
            "and tensor(153, device='cuda:0')\n",
            "then tensor(389, device='cuda:0')\n",
            "began tensor(9, device='cuda:0')\n",
            "to tensor(1710, device='cuda:0')\n",
            "cry tensor(13, device='cuda:0')\n",
            "I tensor(26, device='cuda:0')\n",
            "was tensor(87, device='cuda:0')\n",
            "so tensor(734, device='cuda:0')\n",
            "happy tensor(6, device='cuda:0')\n",
            ". tensor(100, device='cuda:0')\n",
            "… tensor(217, device='cuda:0')\n",
            "For tensor(70, device='cuda:0')\n",
            "one tensor(41, device='cuda:0')\n",
            "who tensor(67, device='cuda:0')\n",
            "has tensor(0, device='cuda:0')\n",
            "<unk> tensor(4, device='cuda:0')\n",
            ", tensor(16246, device='cuda:0')\n",
            "painstakingly tensor(14232, device='cuda:0')\n",
            "researched tensor(19, device='cuda:0')\n",
            "for tensor(0, device='cuda:0')\n",
            "<unk> tensor(157, device='cuda:0')\n",
            "years tensor(4, device='cuda:0')\n",
            ", tensor(5, device='cuda:0')\n",
            "the tensor(0, device='cuda:0')\n",
            "<unk> tensor(8, device='cuda:0')\n",
            "of tensor(37, device='cuda:0')\n",
            "all tensor(115, device='cuda:0')\n",
            "these tensor(1758, device='cuda:0')\n",
            "records tensor(15, device='cuda:0')\n",
            "is tensor(644, device='cuda:0')\n",
            "truly tensor(3868, device='cuda:0')\n",
            "exciting tensor(6, device='cuda:0')\n",
            ". tensor(104, device='cuda:0')\n",
            "And tensor(66, device='cuda:0')\n",
            "when tensor(13, device='cuda:0')\n",
            "I tensor(336, device='cuda:0')\n",
            "think tensor(8, device='cuda:0')\n",
            "of tensor(5, device='cuda:0')\n",
            "the tensor(1832, device='cuda:0')\n",
            "hundreds tensor(8, device='cuda:0')\n",
            "of tensor(1222, device='cuda:0')\n",
            "thousands tensor(8, device='cuda:0')\n",
            "of tensor(119, device='cuda:0')\n",
            "people tensor(41, device='cuda:0')\n",
            "who tensor(23, device='cuda:0')\n",
            "are tensor(191, device='cuda:0')\n",
            "now tensor(47, device='cuda:0')\n",
            "or tensor(1053, device='cuda:0')\n",
            "soon tensor(25, device='cuda:0')\n",
            "will tensor(21, device='cuda:0')\n",
            "be tensor(30152, device='cuda:0')\n",
            "computerizing tensor(4432, device='cuda:0')\n",
            "huge tensor(3972, device='cuda:0')\n",
            "blocks tensor(8, device='cuda:0')\n",
            "of tensor(30070, device='cuda:0')\n",
            "censuses tensor(7, device='cuda:0')\n",
            "and tensor(2350, device='cuda:0')\n",
            "private tensor(3028, device='cuda:0')\n",
            "research tensor(30287, device='cuda:0')\n",
            "disks tensor(100, device='cuda:0')\n",
            "… tensor(13, device='cuda:0')\n",
            "I tensor(203, device='cuda:0')\n",
            "am tensor(87, device='cuda:0')\n",
            "so tensor(3631, device='cuda:0')\n",
            "excited tensor(6, device='cuda:0')\n",
            ". tensor(74, device='cuda:0')\n",
            "It tensor(15, device='cuda:0')\n",
            "is tensor(644, device='cuda:0')\n",
            "truly tensor(5, device='cuda:0')\n",
            "the tensor(46, device='cuda:0')\n",
            "Lord tensor(33, device='cuda:0')\n",
            "’s tensor(124, device='cuda:0')\n",
            "work tensor(7, device='cuda:0')\n",
            "and tensor(34, device='cuda:0')\n",
            "He tensor(15, device='cuda:0')\n",
            "is tensor(5557, device='cuda:0')\n",
            "directing tensor(31, device='cuda:0')\n",
            "it tensor(6, device='cuda:0')\n",
            ". tensor(17, device='cuda:0')\n",
            "” tensor(3, device='cuda:0')\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vZJlFo7sPAjY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Accidentally had extra print statements in the results printer, have to print them again"
      ]
    },
    {
      "metadata": {
        "id": "4j3FYvaGwB_U",
        "colab_type": "code",
        "outputId": "c8741ab2-94d1-422d-f00e-e4c0d99279a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "cell_type": "code",
      "source": [
        "# Decode the model to produce translations (first sentence in validation set)\n",
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "for i, batch in enumerate(valid_iter):\n",
        "    \n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    \n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    \n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Translation:\tAs daughters of God , you were born to lead . \n",
            "Target:\tAs daughters of God , you were born to lead . \n",
            "\n",
            "Translation:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "Target:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "\n",
            "Translation:\tThe other counselor was a prominent judge in the city . \n",
            "Target:\tThe other counselor was a prominent judge in the city . \n",
            "\n",
            "Translation:\tMy wife , Harriet , was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "Target:\tMy wife , Harriet , was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "\n",
            "Translation:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "Target:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "\n",
            "Translation:\t“ And he that receiveth my Father ’s kingdom , and therefore all that my Father hath shall be given unto him . ” \n",
            "Target:\t“ And he that receiveth my Father receiveth my Father ’s kingdom ; therefore all that my Father hath shall be given unto him . ” \n",
            "\n",
            "Translation:\tWhile it is good to pray and work for physical protection and healing during our mortal existence , our ultimate focus should be on the spiritual miracles that are available to all of God ’s children . \n",
            "Target:\tWhile it is good to pray for and work for physical protection and healing during our mortal existence , our supreme focus should be on the spiritual miracles that are available to all of God ’s children . \n",
            "\n",
            "Translation:\tAnother regret people expressed was the little they felt they could not become the person they felt they could or should have been . As they looked back at their lives , they realized that they never lived up to their potential , many songs remained unsung . \n",
            "Target:\tAnother regret people expressed was that they failed to become the person they felt they could and should have been . When they looked back on their lives , they realized that they never lived up to their potential , that too many songs remained unsung . \n",
            "\n",
            "Translation:\tThe Savior taught , “ This shall ye always do to all who repent and are baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always remember \n",
            "Target:\tThe Savior taught : “ This shall ye always do to those who repent and are baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always remember me ye shall have my Spirit to be with you ” ( 3 Nephi 18:11 ) . \n",
            "\n",
            "Translation:\tAfter several years he returned to his hometown . However , the people refused to acknowledge his growth and improvement . To them , he was still just old and so hard , ” and they treated him that way . Eventually , this good man faded away to a shadow of his former successful self without using his \n",
            "Target:\tAfter several years he returned to his hometown . However , the people refused to acknowledge his growth and improvement . To them , he was still just old “ so - and - so , ” and they treated him that way . Eventually , this good man faded away to a shadow of his former successful self without being able to use his marvelously developed talents to bless those who derided and rejected him once again . What a loss , both for him and the community ! \n",
            "\n",
            "Translation:\tThe blessings you receive as you serve others are many . I ’ve often said , “ Oh , I ’ve got to get my visiting teaching ! ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I could see it as a weight , more than a blessing \n",
            "Target:\tThe blessings you receive as you serve others are many . I have sometimes said , “ Oh , I ’ve got to get my visiting teaching done ! ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I was looking at it as a burden rather than a blessing . ) I can honestly say that when I went visiting teaching , I always felt better . I was lifted , loved , and blessed , usually much more than the sister I was visiting . My love increased . My desire to serve increased . And I could see what a beautiful way Heavenly Father has planned for us to watch over and care for one another . \n",
            "\n",
            "Translation:\tAnother journal entry reads : “ The miracle … occurred to me in the Family History office of Grandmother , who presented me with a Danish copy of all my known ancestral night that I could get myself back from the Ancestral File . Most of them coming from the Church ’s fabric , I said to the idea \n",
            "Target:\tAnother journal entry reads : “ The … miracle for me occurred in the Family History office of Mel Olsen who presented me with a printout of all my known ancestral pedigrees taken from the update of the Ancestral File computerized records sent into the genealogical society . They came mostly from the records of the four generation ’s program the Church called for many years ago . I had been overwhelmed with the thought of the huge task ahead of me to gather all my ancestors’ research records from family organizations to get them all in the computer for the first computerized distribution of the Ancestral File . And there they all were , beautiful , organized and laser printed and sitting there on the desk before me . I was so thrilled and so overwhelmed I just sat there stunned and then began to cry I was so happy . … For one who has <unk> , painstakingly researched for <unk> years , the <unk> of all these records is truly exciting . And when I think of the hundreds of thousands of people who are now or soon will be computerizing huge blocks of censuses and private research disks … I am so excited . It is truly the Lord ’s work and He is directing it . ” \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F1BGCirhwAyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# One-layer training and results (+summary paragraph)"
      ]
    },
    {
      "metadata": {
        "id": "c1v7P1iGEmNq",
        "colab_type": "code",
        "outputId": "89df09de-9a09-4ce1-9538-d7a701743640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17408
        }
      },
      "cell_type": "code",
      "source": [
        "# Set up the model \n",
        "model = make_model(n_src, n_tgt, N=1)\n",
        "model_opt = get_std_opt(model)\n",
        "model.cuda()\n",
        "\n",
        "# Set up the label smoothing to penalize overconfidence\n",
        "criterion = LabelSmoothing(size=n_tgt, padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "\n",
        "# Set up training and validation iterators\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True)\n",
        "\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "num_train_batches = 0\n",
        "for i, batch in enumerate(train_iter):\n",
        "    num_train_batches += 1\n",
        "num_valid_batches = 0\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    num_valid_batches += 1\n",
        "print(\"\\nNumber of training batches:\", num_train_batches)\n",
        "print(\"Number of validation batches:\", num_valid_batches, \"\\n\")\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    print(\"\\n\\n%%%%%%%%%% EPOCH \" + str(epoch) + \" %%%%%%%%%%\\n\")\n",
        "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, \n",
        "                model_opt)\n",
        "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of training batches: 779\n",
            "Number of validation batches: 12 \n",
            "\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 0 %%%%%%%%%%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 1 Loss 8.7637 Learning Rate 7e-07\n",
            "Batch 11 Loss 8.1585 Learning Rate 4.19e-06\n",
            "Batch 21 Loss 8.3962 Learning Rate 7.69e-06\n",
            "Batch 31 Loss 9.3083 Learning Rate 1.118e-05\n",
            "Batch 41 Loss 9.0539 Learning Rate 1.467e-05\n",
            "Batch 51 Loss 8.3758 Learning Rate 1.817e-05\n",
            "Batch 61 Loss 8.7287 Learning Rate 2.166e-05\n",
            "Batch 71 Loss 8.2275 Learning Rate 2.516e-05\n",
            "Batch 81 Loss 6.9796 Learning Rate 2.865e-05\n",
            "Batch 91 Loss 8.167 Learning Rate 3.214e-05\n",
            "Batch 101 Loss 7.4875 Learning Rate 3.564e-05\n",
            "Batch 111 Loss 7.4344 Learning Rate 3.913e-05\n",
            "Batch 121 Loss 7.4456 Learning Rate 4.263e-05\n",
            "Batch 131 Loss 7.1431 Learning Rate 4.612e-05\n",
            "Batch 141 Loss 6.7854 Learning Rate 4.961e-05\n",
            "Batch 151 Loss 7.0102 Learning Rate 5.311e-05\n",
            "Batch 161 Loss 6.4727 Learning Rate 5.66e-05\n",
            "Batch 171 Loss 6.2427 Learning Rate 6.009e-05\n",
            "Batch 181 Loss 6.5768 Learning Rate 6.359e-05\n",
            "Batch 191 Loss 5.6957 Learning Rate 6.708e-05\n",
            "Batch 201 Loss 6.0747 Learning Rate 7.058e-05\n",
            "Batch 211 Loss 5.8043 Learning Rate 7.407e-05\n",
            "Batch 221 Loss 5.8619 Learning Rate 7.756e-05\n",
            "Batch 231 Loss 5.0797 Learning Rate 8.106e-05\n",
            "Batch 241 Loss 5.4424 Learning Rate 8.455e-05\n",
            "Batch 251 Loss 5.8065 Learning Rate 8.805e-05\n",
            "Batch 261 Loss 5.3732 Learning Rate 9.154e-05\n",
            "Batch 271 Loss 5.525 Learning Rate 9.503e-05\n",
            "Batch 281 Loss 5.3213 Learning Rate 9.853e-05\n",
            "Batch 291 Loss 4.7249 Learning Rate 0.00010202\n",
            "Batch 301 Loss 5.7936 Learning Rate 0.00010551\n",
            "Batch 311 Loss 5.0962 Learning Rate 0.00010901\n",
            "Batch 321 Loss 5.5113 Learning Rate 0.0001125\n",
            "Batch 331 Loss 5.4081 Learning Rate 0.000116\n",
            "Batch 341 Loss 5.4015 Learning Rate 0.00011949\n",
            "Batch 351 Loss 5.1957 Learning Rate 0.00012298\n",
            "Batch 361 Loss 5.699 Learning Rate 0.00012648\n",
            "Batch 371 Loss 5.4282 Learning Rate 0.00012997\n",
            "Batch 381 Loss 5.0148 Learning Rate 0.00013347\n",
            "Batch 391 Loss 4.9228 Learning Rate 0.00013696\n",
            "Batch 401 Loss 5.304 Learning Rate 0.00014045\n",
            "Batch 411 Loss 5.1005 Learning Rate 0.00014395\n",
            "Batch 421 Loss 5.1623 Learning Rate 0.00014744\n",
            "Batch 431 Loss 5.0215 Learning Rate 0.00015093\n",
            "Batch 441 Loss 4.6115 Learning Rate 0.00015443\n",
            "Batch 451 Loss 4.4194 Learning Rate 0.00015792\n",
            "Batch 461 Loss 4.4454 Learning Rate 0.00016142\n",
            "Batch 471 Loss 4.1778 Learning Rate 0.00016491\n",
            "Batch 481 Loss 4.8376 Learning Rate 0.0001684\n",
            "Batch 491 Loss 4.9143 Learning Rate 0.0001719\n",
            "Batch 501 Loss 4.3197 Learning Rate 0.00017539\n",
            "Batch 511 Loss 4.1455 Learning Rate 0.00017889\n",
            "Batch 521 Loss 4.6628 Learning Rate 0.00018238\n",
            "Batch 531 Loss 4.4365 Learning Rate 0.00018587\n",
            "Batch 541 Loss 4.6136 Learning Rate 0.00018937\n",
            "Batch 551 Loss 5.0875 Learning Rate 0.00019286\n",
            "Batch 561 Loss 5.1268 Learning Rate 0.00019635\n",
            "Batch 571 Loss 4.4478 Learning Rate 0.00019985\n",
            "Batch 581 Loss 4.4728 Learning Rate 0.00020334\n",
            "Batch 591 Loss 4.4216 Learning Rate 0.00020684\n",
            "Batch 601 Loss 4.4662 Learning Rate 0.00021033\n",
            "Batch 611 Loss 4.3734 Learning Rate 0.00021382\n",
            "Batch 621 Loss 4.6672 Learning Rate 0.00021732\n",
            "Batch 631 Loss 4.3262 Learning Rate 0.00022081\n",
            "Batch 641 Loss 4.3788 Learning Rate 0.00022431\n",
            "Batch 651 Loss 4.6559 Learning Rate 0.0002278\n",
            "Batch 661 Loss 4.0703 Learning Rate 0.00023129\n",
            "Batch 671 Loss 4.4407 Learning Rate 0.00023479\n",
            "Batch 681 Loss 4.6581 Learning Rate 0.00023828\n",
            "Batch 691 Loss 4.137 Learning Rate 0.00024177\n",
            "Batch 701 Loss 4.2162 Learning Rate 0.00024527\n",
            "Batch 711 Loss 4.6677 Learning Rate 0.00024876\n",
            "Batch 721 Loss 3.913 Learning Rate 0.00025226\n",
            "Batch 731 Loss 3.7509 Learning Rate 0.00025575\n",
            "Batch 741 Loss 2.3761 Learning Rate 0.00025924\n",
            "Batch 751 Loss 3.1805 Learning Rate 0.00026274\n",
            "Batch 761 Loss 4.3303 Learning Rate 0.00026623\n",
            "Batch 771 Loss 4.5174 Learning Rate 0.00026973\n",
            "Batch 0 validation loss: 2.784275531768799\n",
            "Batch 1 validation loss: 3.721275806427002\n",
            "Batch 2 validation loss: 5.449455261230469\n",
            "Batch 3 validation loss: 3.4835562705993652\n",
            "Batch 4 validation loss: 3.7188467979431152\n",
            "Batch 5 validation loss: 3.7815043926239014\n",
            "Batch 6 validation loss: 3.9479029178619385\n",
            "Batch 7 validation loss: 4.022021293640137\n",
            "Batch 8 validation loss: 4.096297264099121\n",
            "Batch 9 validation loss: 4.286916255950928\n",
            "Batch 10 validation loss: 4.36007833480835\n",
            "Batch 11 validation loss: 4.853403091430664\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 1 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 3.1702 Learning Rate 0.00027287\n",
            "Batch 11 Loss 4.4595 Learning Rate 0.00027636\n",
            "Batch 21 Loss 3.5722 Learning Rate 0.00027986\n",
            "Batch 31 Loss 4.2481 Learning Rate 0.00028335\n",
            "Batch 41 Loss 3.6348 Learning Rate 0.00028685\n",
            "Batch 51 Loss 4.0504 Learning Rate 0.00029034\n",
            "Batch 61 Loss 4.3431 Learning Rate 0.00029383\n",
            "Batch 71 Loss 4.033 Learning Rate 0.00029733\n",
            "Batch 81 Loss 2.8951 Learning Rate 0.00030082\n",
            "Batch 91 Loss 3.0697 Learning Rate 0.00030431\n",
            "Batch 101 Loss 3.3944 Learning Rate 0.00030781\n",
            "Batch 111 Loss 5.0232 Learning Rate 0.0003113\n",
            "Batch 121 Loss 3.7291 Learning Rate 0.0003148\n",
            "Batch 131 Loss 4.1451 Learning Rate 0.00031829\n",
            "Batch 141 Loss 3.1973 Learning Rate 0.00032178\n",
            "Batch 151 Loss 3.5113 Learning Rate 0.00032528\n",
            "Batch 161 Loss 4.7379 Learning Rate 0.00032877\n",
            "Batch 171 Loss 3.9119 Learning Rate 0.00033227\n",
            "Batch 181 Loss 3.1151 Learning Rate 0.00033576\n",
            "Batch 191 Loss 3.5439 Learning Rate 0.00033925\n",
            "Batch 201 Loss 4.4118 Learning Rate 0.00034275\n",
            "Batch 211 Loss 3.5878 Learning Rate 0.00034624\n",
            "Batch 221 Loss 4.2442 Learning Rate 0.00034974\n",
            "Batch 231 Loss 3.8284 Learning Rate 0.00035323\n",
            "Batch 241 Loss 3.323 Learning Rate 0.00035672\n",
            "Batch 251 Loss 4.0774 Learning Rate 0.00036022\n",
            "Batch 261 Loss 3.4489 Learning Rate 0.00036371\n",
            "Batch 271 Loss 2.8581 Learning Rate 0.0003672\n",
            "Batch 281 Loss 3.3339 Learning Rate 0.0003707\n",
            "Batch 291 Loss 3.087 Learning Rate 0.00037419\n",
            "Batch 301 Loss 3.7414 Learning Rate 0.00037769\n",
            "Batch 311 Loss 3.0451 Learning Rate 0.00038118\n",
            "Batch 321 Loss 4.4204 Learning Rate 0.00038467\n",
            "Batch 331 Loss 3.6234 Learning Rate 0.00038817\n",
            "Batch 341 Loss 4.3126 Learning Rate 0.00039166\n",
            "Batch 351 Loss 3.3061 Learning Rate 0.00039516\n",
            "Batch 361 Loss 4.8845 Learning Rate 0.00039865\n",
            "Batch 371 Loss 3.3697 Learning Rate 0.00040214\n",
            "Batch 381 Loss 3.1883 Learning Rate 0.00040564\n",
            "Batch 391 Loss 3.0892 Learning Rate 0.00040913\n",
            "Batch 401 Loss 3.8197 Learning Rate 0.00041262\n",
            "Batch 411 Loss 3.9351 Learning Rate 0.00041612\n",
            "Batch 421 Loss 2.8253 Learning Rate 0.00041961\n",
            "Batch 431 Loss 1.7302 Learning Rate 0.00042311\n",
            "Batch 441 Loss 2.9002 Learning Rate 0.0004266\n",
            "Batch 451 Loss 3.2027 Learning Rate 0.00043009\n",
            "Batch 461 Loss 2.8999 Learning Rate 0.00043359\n",
            "Batch 471 Loss 2.5461 Learning Rate 0.00043708\n",
            "Batch 481 Loss 3.5188 Learning Rate 0.00044058\n",
            "Batch 491 Loss 3.021 Learning Rate 0.00044407\n",
            "Batch 501 Loss 2.5876 Learning Rate 0.00044756\n",
            "Batch 511 Loss 2.5618 Learning Rate 0.00045106\n",
            "Batch 521 Loss 4.0964 Learning Rate 0.00045455\n",
            "Batch 531 Loss 2.5923 Learning Rate 0.00045804\n",
            "Batch 541 Loss 2.5792 Learning Rate 0.00046154\n",
            "Batch 551 Loss 2.0753 Learning Rate 0.00046503\n",
            "Batch 561 Loss 2.5833 Learning Rate 0.00046853\n",
            "Batch 571 Loss 2.8686 Learning Rate 0.00047202\n",
            "Batch 581 Loss 2.3827 Learning Rate 0.00047551\n",
            "Batch 591 Loss 2.1641 Learning Rate 0.00047901\n",
            "Batch 601 Loss 3.0295 Learning Rate 0.0004825\n",
            "Batch 611 Loss 3.3804 Learning Rate 0.000486\n",
            "Batch 621 Loss 1.8849 Learning Rate 0.00048949\n",
            "Batch 631 Loss 3.0207 Learning Rate 0.00049298\n",
            "Batch 641 Loss 2.6009 Learning Rate 0.00049648\n",
            "Batch 651 Loss 3.0485 Learning Rate 0.00049997\n",
            "Batch 661 Loss 1.8424 Learning Rate 0.00050346\n",
            "Batch 671 Loss 2.1616 Learning Rate 0.00050696\n",
            "Batch 681 Loss 3.6814 Learning Rate 0.00051045\n",
            "Batch 691 Loss 2.4409 Learning Rate 0.00051395\n",
            "Batch 701 Loss 2.3939 Learning Rate 0.00051744\n",
            "Batch 711 Loss 2.4238 Learning Rate 0.00052093\n",
            "Batch 721 Loss 2.3859 Learning Rate 0.00052443\n",
            "Batch 731 Loss 2.1965 Learning Rate 0.00052792\n",
            "Batch 741 Loss 2.1771 Learning Rate 0.00053142\n",
            "Batch 751 Loss 2.1418 Learning Rate 0.00053491\n",
            "Batch 761 Loss 2.1499 Learning Rate 0.0005384\n",
            "Batch 771 Loss 2.3329 Learning Rate 0.0005419\n",
            "Batch 0 validation loss: 1.288232445716858\n",
            "Batch 1 validation loss: 1.986663818359375\n",
            "Batch 2 validation loss: 4.151251792907715\n",
            "Batch 3 validation loss: 1.7953202724456787\n",
            "Batch 4 validation loss: 1.9843965768814087\n",
            "Batch 5 validation loss: 1.9656755924224854\n",
            "Batch 6 validation loss: 2.056154727935791\n",
            "Batch 7 validation loss: 2.1704862117767334\n",
            "Batch 8 validation loss: 2.247936487197876\n",
            "Batch 9 validation loss: 2.30826473236084\n",
            "Batch 10 validation loss: 2.4300355911254883\n",
            "Batch 11 validation loss: 2.932319164276123\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 2 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 2.3088 Learning Rate 0.00054504\n",
            "Batch 11 Loss 2.0276 Learning Rate 0.00054854\n",
            "Batch 21 Loss 2.1911 Learning Rate 0.00055203\n",
            "Batch 31 Loss 2.3504 Learning Rate 0.00055552\n",
            "Batch 41 Loss 2.4635 Learning Rate 0.00055902\n",
            "Batch 51 Loss 1.7088 Learning Rate 0.00056251\n",
            "Batch 61 Loss 2.7249 Learning Rate 0.000566\n",
            "Batch 71 Loss 2.2238 Learning Rate 0.0005695\n",
            "Batch 81 Loss 1.6223 Learning Rate 0.00057299\n",
            "Batch 91 Loss 1.7211 Learning Rate 0.00057649\n",
            "Batch 101 Loss 3.3912 Learning Rate 0.00057998\n",
            "Batch 111 Loss 2.4862 Learning Rate 0.00058347\n",
            "Batch 121 Loss 1.6375 Learning Rate 0.00058697\n",
            "Batch 131 Loss 2.2581 Learning Rate 0.00059046\n",
            "Batch 141 Loss 1.8115 Learning Rate 0.00059396\n",
            "Batch 151 Loss 1.7826 Learning Rate 0.00059745\n",
            "Batch 161 Loss 1.7382 Learning Rate 0.00060094\n",
            "Batch 171 Loss 1.6149 Learning Rate 0.00060444\n",
            "Batch 181 Loss 1.6263 Learning Rate 0.00060793\n",
            "Batch 191 Loss 1.9792 Learning Rate 0.00061142\n",
            "Batch 201 Loss 2.1907 Learning Rate 0.00061492\n",
            "Batch 211 Loss 1.5555 Learning Rate 0.00061841\n",
            "Batch 221 Loss 1.6185 Learning Rate 0.00062191\n",
            "Batch 231 Loss 1.9149 Learning Rate 0.0006254\n",
            "Batch 241 Loss 1.9107 Learning Rate 0.00062889\n",
            "Batch 251 Loss 1.9119 Learning Rate 0.00063239\n",
            "Batch 261 Loss 2.3654 Learning Rate 0.00063588\n",
            "Batch 271 Loss 2.6232 Learning Rate 0.00063938\n",
            "Batch 281 Loss 1.7345 Learning Rate 0.00064287\n",
            "Batch 291 Loss 2.339 Learning Rate 0.00064636\n",
            "Batch 301 Loss 1.6023 Learning Rate 0.00064986\n",
            "Batch 311 Loss 1.7473 Learning Rate 0.00065335\n",
            "Batch 321 Loss 1.8103 Learning Rate 0.00065684\n",
            "Batch 331 Loss 2.5286 Learning Rate 0.00066034\n",
            "Batch 341 Loss 1.4147 Learning Rate 0.00066383\n",
            "Batch 351 Loss 1.7575 Learning Rate 0.00066733\n",
            "Batch 361 Loss 1.9381 Learning Rate 0.00067082\n",
            "Batch 371 Loss 2.2461 Learning Rate 0.00067431\n",
            "Batch 381 Loss 1.6201 Learning Rate 0.00067781\n",
            "Batch 391 Loss 2.2029 Learning Rate 0.0006813\n",
            "Batch 401 Loss 1.6755 Learning Rate 0.0006848\n",
            "Batch 411 Loss 1.98 Learning Rate 0.00068829\n",
            "Batch 421 Loss 2.0477 Learning Rate 0.00069178\n",
            "Batch 431 Loss 2.6171 Learning Rate 0.00069528\n",
            "Batch 441 Loss 2.2816 Learning Rate 0.00069877\n",
            "Batch 451 Loss 1.7658 Learning Rate 0.00070227\n",
            "Batch 461 Loss 1.7246 Learning Rate 0.00070576\n",
            "Batch 471 Loss 2.1156 Learning Rate 0.00070925\n",
            "Batch 481 Loss 2.2505 Learning Rate 0.00071275\n",
            "Batch 491 Loss 1.9558 Learning Rate 0.00071624\n",
            "Batch 501 Loss 1.7709 Learning Rate 0.00071973\n",
            "Batch 511 Loss 1.799 Learning Rate 0.00072323\n",
            "Batch 521 Loss 2.189 Learning Rate 0.00072672\n",
            "Batch 531 Loss 2.4307 Learning Rate 0.00073022\n",
            "Batch 541 Loss 1.942 Learning Rate 0.00073371\n",
            "Batch 551 Loss 2.0995 Learning Rate 0.0007372\n",
            "Batch 561 Loss 1.5118 Learning Rate 0.0007407\n",
            "Batch 571 Loss 1.607 Learning Rate 0.00074419\n",
            "Batch 581 Loss 1.695 Learning Rate 0.00074769\n",
            "Batch 591 Loss 1.8143 Learning Rate 0.00075118\n",
            "Batch 601 Loss 1.5814 Learning Rate 0.00075467\n",
            "Batch 611 Loss 1.5898 Learning Rate 0.00075817\n",
            "Batch 621 Loss 1.7773 Learning Rate 0.00076166\n",
            "Batch 631 Loss 1.658 Learning Rate 0.00076515\n",
            "Batch 641 Loss 1.7689 Learning Rate 0.00076865\n",
            "Batch 651 Loss 1.8553 Learning Rate 0.00077214\n",
            "Batch 661 Loss 1.5613 Learning Rate 0.00077564\n",
            "Batch 671 Loss 1.5355 Learning Rate 0.00077913\n",
            "Batch 681 Loss 1.601 Learning Rate 0.00078262\n",
            "Batch 691 Loss 2.4544 Learning Rate 0.00078612\n",
            "Batch 701 Loss 1.6442 Learning Rate 0.00078961\n",
            "Batch 711 Loss 1.8562 Learning Rate 0.00079311\n",
            "Batch 721 Loss 1.88 Learning Rate 0.0007966\n",
            "Batch 731 Loss 1.7457 Learning Rate 0.00080009\n",
            "Batch 741 Loss 1.6824 Learning Rate 0.00080359\n",
            "Batch 751 Loss 1.7306 Learning Rate 0.00080708\n",
            "Batch 761 Loss 3.0478 Learning Rate 0.00081057\n",
            "Batch 771 Loss 1.4223 Learning Rate 0.00081407\n",
            "Batch 0 validation loss: 0.8491809368133545\n",
            "Batch 1 validation loss: 1.3848421573638916\n",
            "Batch 2 validation loss: 3.0568344593048096\n",
            "Batch 3 validation loss: 1.293033242225647\n",
            "Batch 4 validation loss: 1.3946142196655273\n",
            "Batch 5 validation loss: 1.3925014734268188\n",
            "Batch 6 validation loss: 1.488036870956421\n",
            "Batch 7 validation loss: 1.483061671257019\n",
            "Batch 8 validation loss: 1.5778977870941162\n",
            "Batch 9 validation loss: 1.6697909832000732\n",
            "Batch 10 validation loss: 1.7509506940841675\n",
            "Batch 11 validation loss: 2.0723087787628174\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 3 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.4494 Learning Rate 0.00081721\n",
            "Batch 11 Loss 1.4797 Learning Rate 0.00082071\n",
            "Batch 21 Loss 1.6713 Learning Rate 0.0008242\n",
            "Batch 31 Loss 1.324 Learning Rate 0.00082769\n",
            "Batch 41 Loss 1.5768 Learning Rate 0.00083119\n",
            "Batch 51 Loss 1.2838 Learning Rate 0.00083468\n",
            "Batch 61 Loss 1.1093 Learning Rate 0.00083818\n",
            "Batch 71 Loss 1.3041 Learning Rate 0.00084167\n",
            "Batch 81 Loss 1.5281 Learning Rate 0.00084516\n",
            "Batch 91 Loss 1.3967 Learning Rate 0.00084866\n",
            "Batch 101 Loss 1.1969 Learning Rate 0.00085215\n",
            "Batch 111 Loss 1.568 Learning Rate 0.00085565\n",
            "Batch 121 Loss 1.409 Learning Rate 0.00085914\n",
            "Batch 131 Loss 1.4409 Learning Rate 0.00086263\n",
            "Batch 141 Loss 1.4825 Learning Rate 0.00086613\n",
            "Batch 151 Loss 1.2572 Learning Rate 0.00086962\n",
            "Batch 161 Loss 1.5317 Learning Rate 0.00087311\n",
            "Batch 171 Loss 1.7163 Learning Rate 0.00087661\n",
            "Batch 181 Loss 1.8614 Learning Rate 0.0008801\n",
            "Batch 191 Loss 1.5556 Learning Rate 0.0008836\n",
            "Batch 201 Loss 1.4635 Learning Rate 0.00088709\n",
            "Batch 211 Loss 1.7133 Learning Rate 0.00089058\n",
            "Batch 221 Loss 1.6015 Learning Rate 0.00089408\n",
            "Batch 231 Loss 2.3713 Learning Rate 0.00089757\n",
            "Batch 241 Loss 1.4763 Learning Rate 0.00090107\n",
            "Batch 251 Loss 1.4044 Learning Rate 0.00090456\n",
            "Batch 261 Loss 1.5805 Learning Rate 0.00090805\n",
            "Batch 271 Loss 1.6892 Learning Rate 0.00091155\n",
            "Batch 281 Loss 1.4739 Learning Rate 0.00091504\n",
            "Batch 291 Loss 1.2045 Learning Rate 0.00091853\n",
            "Batch 301 Loss 1.0791 Learning Rate 0.00092203\n",
            "Batch 311 Loss 1.0482 Learning Rate 0.00092552\n",
            "Batch 321 Loss 1.609 Learning Rate 0.00092902\n",
            "Batch 331 Loss 1.2749 Learning Rate 0.00093251\n",
            "Batch 341 Loss 1.2908 Learning Rate 0.000936\n",
            "Batch 351 Loss 1.1532 Learning Rate 0.0009395\n",
            "Batch 361 Loss 2.2329 Learning Rate 0.00094299\n",
            "Batch 371 Loss 1.7786 Learning Rate 0.00094649\n",
            "Batch 381 Loss 1.302 Learning Rate 0.00094998\n",
            "Batch 391 Loss 1.2715 Learning Rate 0.00095347\n",
            "Batch 401 Loss 1.6649 Learning Rate 0.00095697\n",
            "Batch 411 Loss 1.3335 Learning Rate 0.00096046\n",
            "Batch 421 Loss 1.3009 Learning Rate 0.00096395\n",
            "Batch 431 Loss 1.4857 Learning Rate 0.00096745\n",
            "Batch 441 Loss 1.316 Learning Rate 0.00097094\n",
            "Batch 451 Loss 1.3805 Learning Rate 0.00097444\n",
            "Batch 461 Loss 1.4978 Learning Rate 0.00097793\n",
            "Batch 471 Loss 1.226 Learning Rate 0.00098142\n",
            "Batch 481 Loss 1.4934 Learning Rate 0.00098492\n",
            "Batch 491 Loss 1.4503 Learning Rate 0.00098841\n",
            "Batch 501 Loss 1.5569 Learning Rate 0.00099191\n",
            "Batch 511 Loss 1.4972 Learning Rate 0.0009954\n",
            "Batch 521 Loss 1.3817 Learning Rate 0.00099889\n",
            "Batch 531 Loss 1.2235 Learning Rate 0.00100239\n",
            "Batch 541 Loss 1.6058 Learning Rate 0.00100588\n",
            "Batch 551 Loss 1.5386 Learning Rate 0.00100938\n",
            "Batch 561 Loss 1.3208 Learning Rate 0.00101287\n",
            "Batch 571 Loss 1.4743 Learning Rate 0.00101636\n",
            "Batch 581 Loss 1.3292 Learning Rate 0.00101986\n",
            "Batch 591 Loss 1.2579 Learning Rate 0.00102335\n",
            "Batch 601 Loss 1.5025 Learning Rate 0.00102684\n",
            "Batch 611 Loss 1.6695 Learning Rate 0.00103034\n",
            "Batch 621 Loss 0.96 Learning Rate 0.00103383\n",
            "Batch 631 Loss 1.7718 Learning Rate 0.00103733\n",
            "Batch 641 Loss 1.1488 Learning Rate 0.00104082\n",
            "Batch 651 Loss 1.1631 Learning Rate 0.00104431\n",
            "Batch 661 Loss 1.094 Learning Rate 0.00104781\n",
            "Batch 671 Loss 1.6381 Learning Rate 0.0010513\n",
            "Batch 681 Loss 0.7726 Learning Rate 0.0010548\n",
            "Batch 691 Loss 1.0693 Learning Rate 0.00105829\n",
            "Batch 701 Loss 1.3775 Learning Rate 0.00106178\n",
            "Batch 711 Loss 1.6586 Learning Rate 0.00106528\n",
            "Batch 721 Loss 1.2224 Learning Rate 0.00106877\n",
            "Batch 731 Loss 1.1699 Learning Rate 0.00107226\n",
            "Batch 741 Loss 1.2543 Learning Rate 0.00107576\n",
            "Batch 751 Loss 0.987 Learning Rate 0.00107925\n",
            "Batch 761 Loss 1.8914 Learning Rate 0.00108275\n",
            "Batch 771 Loss 1.3141 Learning Rate 0.00108624\n",
            "Batch 0 validation loss: 0.7738357186317444\n",
            "Batch 1 validation loss: 0.940669596195221\n",
            "Batch 2 validation loss: 2.3958723545074463\n",
            "Batch 3 validation loss: 1.0013459920883179\n",
            "Batch 4 validation loss: 1.1109416484832764\n",
            "Batch 5 validation loss: 1.1048378944396973\n",
            "Batch 6 validation loss: 1.1641770601272583\n",
            "Batch 7 validation loss: 1.1351432800292969\n",
            "Batch 8 validation loss: 1.2178914546966553\n",
            "Batch 9 validation loss: 1.2468067407608032\n",
            "Batch 10 validation loss: 1.3014476299285889\n",
            "Batch 11 validation loss: 1.5646226406097412\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 4 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.0603 Learning Rate 0.00108938\n",
            "Batch 11 Loss 1.3377 Learning Rate 0.00109288\n",
            "Batch 21 Loss 1.3934 Learning Rate 0.00109637\n",
            "Batch 31 Loss 0.9357 Learning Rate 0.00109987\n",
            "Batch 41 Loss 1.1928 Learning Rate 0.00110336\n",
            "Batch 51 Loss 1.0617 Learning Rate 0.00110685\n",
            "Batch 61 Loss 1.3703 Learning Rate 0.00111035\n",
            "Batch 71 Loss 1.2766 Learning Rate 0.00111384\n",
            "Batch 81 Loss 1.5106 Learning Rate 0.00111734\n",
            "Batch 91 Loss 1.2756 Learning Rate 0.00112083\n",
            "Batch 101 Loss 1.1068 Learning Rate 0.00112432\n",
            "Batch 111 Loss 1.3836 Learning Rate 0.00112782\n",
            "Batch 121 Loss 0.8864 Learning Rate 0.00113131\n",
            "Batch 131 Loss 1.2711 Learning Rate 0.0011348\n",
            "Batch 141 Loss 0.9862 Learning Rate 0.0011383\n",
            "Batch 151 Loss 0.9349 Learning Rate 0.00114179\n",
            "Batch 161 Loss 1.3511 Learning Rate 0.00114529\n",
            "Batch 171 Loss 1.4205 Learning Rate 0.00114878\n",
            "Batch 181 Loss 0.9548 Learning Rate 0.00115227\n",
            "Batch 191 Loss 1.0672 Learning Rate 0.00115577\n",
            "Batch 201 Loss 1.6682 Learning Rate 0.00115926\n",
            "Batch 211 Loss 0.9872 Learning Rate 0.00116276\n",
            "Batch 221 Loss 1.0577 Learning Rate 0.00116625\n",
            "Batch 231 Loss 1.4017 Learning Rate 0.00116974\n",
            "Batch 241 Loss 1.5152 Learning Rate 0.00117324\n",
            "Batch 251 Loss 1.3594 Learning Rate 0.00117673\n",
            "Batch 261 Loss 1.1755 Learning Rate 0.00118022\n",
            "Batch 271 Loss 1.4382 Learning Rate 0.00118372\n",
            "Batch 281 Loss 1.1259 Learning Rate 0.00118721\n",
            "Batch 291 Loss 2.3318 Learning Rate 0.00119071\n",
            "Batch 301 Loss 1.8628 Learning Rate 0.0011942\n",
            "Batch 311 Loss 1.2728 Learning Rate 0.00119769\n",
            "Batch 321 Loss 1.0876 Learning Rate 0.00120119\n",
            "Batch 331 Loss 1.4446 Learning Rate 0.00120468\n",
            "Batch 341 Loss 1.1207 Learning Rate 0.00120818\n",
            "Batch 351 Loss 1.3674 Learning Rate 0.00121167\n",
            "Batch 361 Loss 1.5309 Learning Rate 0.00121516\n",
            "Batch 371 Loss 1.0191 Learning Rate 0.00121866\n",
            "Batch 381 Loss 1.271 Learning Rate 0.00122215\n",
            "Batch 391 Loss 2.3702 Learning Rate 0.00122564\n",
            "Batch 401 Loss 1.1001 Learning Rate 0.00122914\n",
            "Batch 411 Loss 1.6464 Learning Rate 0.00123263\n",
            "Batch 421 Loss 1.1586 Learning Rate 0.00123613\n",
            "Batch 431 Loss 1.4204 Learning Rate 0.00123962\n",
            "Batch 441 Loss 0.9916 Learning Rate 0.00124311\n",
            "Batch 451 Loss 0.2001 Learning Rate 0.00124661\n",
            "Batch 461 Loss 2.0883 Learning Rate 0.0012501\n",
            "Batch 471 Loss 1.47 Learning Rate 0.0012536\n",
            "Batch 481 Loss 1.3416 Learning Rate 0.00125709\n",
            "Batch 491 Loss 1.4162 Learning Rate 0.00126058\n",
            "Batch 501 Loss 1.0569 Learning Rate 0.00126408\n",
            "Batch 511 Loss 1.1757 Learning Rate 0.00126757\n",
            "Batch 521 Loss 1.5833 Learning Rate 0.00127106\n",
            "Batch 531 Loss 1.3088 Learning Rate 0.00127456\n",
            "Batch 541 Loss 1.5481 Learning Rate 0.00127805\n",
            "Batch 551 Loss 1.3267 Learning Rate 0.00128155\n",
            "Batch 561 Loss 1.3792 Learning Rate 0.00128504\n",
            "Batch 571 Loss 1.6375 Learning Rate 0.00128853\n",
            "Batch 581 Loss 1.209 Learning Rate 0.00129203\n",
            "Batch 591 Loss 1.0292 Learning Rate 0.00129552\n",
            "Batch 601 Loss 1.3493 Learning Rate 0.00129902\n",
            "Batch 611 Loss 1.1634 Learning Rate 0.00130251\n",
            "Batch 621 Loss 1.1872 Learning Rate 0.001306\n",
            "Batch 631 Loss 1.781 Learning Rate 0.0013095\n",
            "Batch 641 Loss 1.3658 Learning Rate 0.00131299\n",
            "Batch 651 Loss 1.0736 Learning Rate 0.00131649\n",
            "Batch 661 Loss 0.9454 Learning Rate 0.00131998\n",
            "Batch 671 Loss 1.1725 Learning Rate 0.00132347\n",
            "Batch 681 Loss 1.139 Learning Rate 0.00132697\n",
            "Batch 691 Loss 0.9076 Learning Rate 0.00133046\n",
            "Batch 701 Loss 0.7512 Learning Rate 0.00133395\n",
            "Batch 711 Loss 1.2876 Learning Rate 0.00133745\n",
            "Batch 721 Loss 1.2678 Learning Rate 0.00134094\n",
            "Batch 731 Loss 1.6583 Learning Rate 0.00134444\n",
            "Batch 741 Loss 1.108 Learning Rate 0.00134793\n",
            "Batch 751 Loss 1.1191 Learning Rate 0.00135142\n",
            "Batch 761 Loss 1.3128 Learning Rate 0.00135492\n",
            "Batch 771 Loss 2.4233 Learning Rate 0.00135841\n",
            "Batch 0 validation loss: 0.6184248924255371\n",
            "Batch 1 validation loss: 0.8966490030288696\n",
            "Batch 2 validation loss: 2.1816442012786865\n",
            "Batch 3 validation loss: 0.8797017335891724\n",
            "Batch 4 validation loss: 1.031783938407898\n",
            "Batch 5 validation loss: 0.9880714416503906\n",
            "Batch 6 validation loss: 1.0228790044784546\n",
            "Batch 7 validation loss: 1.0336668491363525\n",
            "Batch 8 validation loss: 1.1004040241241455\n",
            "Batch 9 validation loss: 1.101301670074463\n",
            "Batch 10 validation loss: 1.0727052688598633\n",
            "Batch 11 validation loss: 1.2863324880599976\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 5 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.8586 Learning Rate 0.00136156\n",
            "Batch 11 Loss 1.5384 Learning Rate 0.00136505\n",
            "Batch 21 Loss 0.9641 Learning Rate 0.00136854\n",
            "Batch 31 Loss 1.0376 Learning Rate 0.00137204\n",
            "Batch 41 Loss 1.1639 Learning Rate 0.00137553\n",
            "Batch 51 Loss 0.7524 Learning Rate 0.00137903\n",
            "Batch 61 Loss 1.0753 Learning Rate 0.00138252\n",
            "Batch 71 Loss 1.3632 Learning Rate 0.00138601\n",
            "Batch 81 Loss 1.0172 Learning Rate 0.00138951\n",
            "Batch 91 Loss 0.9456 Learning Rate 0.001393\n",
            "Batch 101 Loss 0.8073 Learning Rate 0.00139649\n",
            "Batch 111 Loss 1.1467 Learning Rate 0.00139632\n",
            "Batch 121 Loss 0.9002 Learning Rate 0.00139458\n",
            "Batch 131 Loss 1.1866 Learning Rate 0.00139285\n",
            "Batch 141 Loss 0.9506 Learning Rate 0.00139112\n",
            "Batch 151 Loss 0.9768 Learning Rate 0.0013894\n",
            "Batch 161 Loss 1.141 Learning Rate 0.00138769\n",
            "Batch 171 Loss 1.0892 Learning Rate 0.00138598\n",
            "Batch 181 Loss 1.0529 Learning Rate 0.00138428\n",
            "Batch 191 Loss 1.2951 Learning Rate 0.00138259\n",
            "Batch 201 Loss 1.4331 Learning Rate 0.0013809\n",
            "Batch 211 Loss 1.1713 Learning Rate 0.00137922\n",
            "Batch 221 Loss 1.0546 Learning Rate 0.00137754\n",
            "Batch 231 Loss 1.1757 Learning Rate 0.00137587\n",
            "Batch 241 Loss 1.0784 Learning Rate 0.00137421\n",
            "Batch 251 Loss 0.9669 Learning Rate 0.00137255\n",
            "Batch 261 Loss 1.0051 Learning Rate 0.0013709\n",
            "Batch 271 Loss 2.1777 Learning Rate 0.00136925\n",
            "Batch 281 Loss 1.2324 Learning Rate 0.00136761\n",
            "Batch 291 Loss 1.0715 Learning Rate 0.00136598\n",
            "Batch 301 Loss 0.9936 Learning Rate 0.00136435\n",
            "Batch 311 Loss 2.8416 Learning Rate 0.00136273\n",
            "Batch 321 Loss 0.991 Learning Rate 0.00136111\n",
            "Batch 331 Loss 1.007 Learning Rate 0.0013595\n",
            "Batch 341 Loss 1.251 Learning Rate 0.00135789\n",
            "Batch 351 Loss 1.0384 Learning Rate 0.00135629\n",
            "Batch 361 Loss 1.0496 Learning Rate 0.0013547\n",
            "Batch 371 Loss 1.1322 Learning Rate 0.00135311\n",
            "Batch 381 Loss 1.1751 Learning Rate 0.00135153\n",
            "Batch 391 Loss 1.1548 Learning Rate 0.00134995\n",
            "Batch 401 Loss 0.9188 Learning Rate 0.00134838\n",
            "Batch 411 Loss 0.8072 Learning Rate 0.00134681\n",
            "Batch 421 Loss 1.2196 Learning Rate 0.00134525\n",
            "Batch 431 Loss 0.9858 Learning Rate 0.0013437\n",
            "Batch 441 Loss 1.3109 Learning Rate 0.00134215\n",
            "Batch 451 Loss 1.0423 Learning Rate 0.0013406\n",
            "Batch 461 Loss 1.0516 Learning Rate 0.00133906\n",
            "Batch 471 Loss 0.9874 Learning Rate 0.00133753\n",
            "Batch 481 Loss 0.971 Learning Rate 0.001336\n",
            "Batch 491 Loss 1.2723 Learning Rate 0.00133448\n",
            "Batch 501 Loss 0.9768 Learning Rate 0.00133296\n",
            "Batch 511 Loss 1.2231 Learning Rate 0.00133145\n",
            "Batch 521 Loss 0.9604 Learning Rate 0.00132994\n",
            "Batch 531 Loss 0.9973 Learning Rate 0.00132843\n",
            "Batch 541 Loss 1.1917 Learning Rate 0.00132694\n",
            "Batch 551 Loss 0.9465 Learning Rate 0.00132544\n",
            "Batch 561 Loss 1.1839 Learning Rate 0.00132396\n",
            "Batch 571 Loss 1.0842 Learning Rate 0.00132247\n",
            "Batch 581 Loss 0.9442 Learning Rate 0.001321\n",
            "Batch 591 Loss 0.9416 Learning Rate 0.00131952\n",
            "Batch 601 Loss 0.9707 Learning Rate 0.00131806\n",
            "Batch 611 Loss 0.5647 Learning Rate 0.00131659\n",
            "Batch 621 Loss 1.0523 Learning Rate 0.00131513\n",
            "Batch 631 Loss 2.0387 Learning Rate 0.00131368\n",
            "Batch 641 Loss 0.9835 Learning Rate 0.00131223\n",
            "Batch 651 Loss 0.9747 Learning Rate 0.00131079\n",
            "Batch 661 Loss 1.4241 Learning Rate 0.00130935\n",
            "Batch 671 Loss 1.126 Learning Rate 0.00130791\n",
            "Batch 681 Loss 1.0489 Learning Rate 0.00130649\n",
            "Batch 691 Loss 1.5377 Learning Rate 0.00130506\n",
            "Batch 701 Loss 0.9818 Learning Rate 0.00130364\n",
            "Batch 711 Loss 0.9113 Learning Rate 0.00130222\n",
            "Batch 721 Loss 0.8983 Learning Rate 0.00130081\n",
            "Batch 731 Loss 1.032 Learning Rate 0.00129941\n",
            "Batch 741 Loss 1.1291 Learning Rate 0.00129801\n",
            "Batch 751 Loss 1.2179 Learning Rate 0.00129661\n",
            "Batch 761 Loss 1.1632 Learning Rate 0.00129522\n",
            "Batch 771 Loss 1.0207 Learning Rate 0.00129383\n",
            "Batch 0 validation loss: 0.6308980584144592\n",
            "Batch 1 validation loss: 0.6179077625274658\n",
            "Batch 2 validation loss: 2.0114712715148926\n",
            "Batch 3 validation loss: 0.7448288202285767\n",
            "Batch 4 validation loss: 0.855948269367218\n",
            "Batch 5 validation loss: 0.8052154779434204\n",
            "Batch 6 validation loss: 0.865577220916748\n",
            "Batch 7 validation loss: 0.822398841381073\n",
            "Batch 8 validation loss: 0.9118716716766357\n",
            "Batch 9 validation loss: 0.8788572549819946\n",
            "Batch 10 validation loss: 0.8618573546409607\n",
            "Batch 11 validation loss: 1.0639225244522095\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 6 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.7939 Learning Rate 0.00129258\n",
            "Batch 11 Loss 0.8271 Learning Rate 0.0012912\n",
            "Batch 21 Loss 0.9308 Learning Rate 0.00128983\n",
            "Batch 31 Loss 0.8403 Learning Rate 0.00128845\n",
            "Batch 41 Loss 0.6555 Learning Rate 0.00128709\n",
            "Batch 51 Loss 0.7787 Learning Rate 0.00128573\n",
            "Batch 61 Loss 1.2886 Learning Rate 0.00128437\n",
            "Batch 71 Loss 1.05 Learning Rate 0.00128301\n",
            "Batch 81 Loss 0.9608 Learning Rate 0.00128166\n",
            "Batch 91 Loss 0.9615 Learning Rate 0.00128032\n",
            "Batch 101 Loss 1.2087 Learning Rate 0.00127898\n",
            "Batch 111 Loss 1.0483 Learning Rate 0.00127764\n",
            "Batch 121 Loss 1.1914 Learning Rate 0.00127631\n",
            "Batch 131 Loss 0.7232 Learning Rate 0.00127498\n",
            "Batch 141 Loss 0.8254 Learning Rate 0.00127365\n",
            "Batch 151 Loss 0.7444 Learning Rate 0.00127233\n",
            "Batch 161 Loss 0.8457 Learning Rate 0.00127102\n",
            "Batch 171 Loss 0.7777 Learning Rate 0.00126971\n",
            "Batch 181 Loss 0.7939 Learning Rate 0.0012684\n",
            "Batch 191 Loss 0.7332 Learning Rate 0.00126709\n",
            "Batch 201 Loss 0.8983 Learning Rate 0.00126579\n",
            "Batch 211 Loss 0.8077 Learning Rate 0.0012645\n",
            "Batch 221 Loss 0.8292 Learning Rate 0.00126321\n",
            "Batch 231 Loss 0.7286 Learning Rate 0.00126192\n",
            "Batch 241 Loss 0.8016 Learning Rate 0.00126063\n",
            "Batch 251 Loss 0.7918 Learning Rate 0.00125935\n",
            "Batch 261 Loss 0.7832 Learning Rate 0.00125808\n",
            "Batch 271 Loss 0.6825 Learning Rate 0.00125681\n",
            "Batch 281 Loss 0.8199 Learning Rate 0.00125554\n",
            "Batch 291 Loss 0.8544 Learning Rate 0.00125427\n",
            "Batch 301 Loss 0.7703 Learning Rate 0.00125301\n",
            "Batch 311 Loss 0.9111 Learning Rate 0.00125175\n",
            "Batch 321 Loss 0.9428 Learning Rate 0.0012505\n",
            "Batch 331 Loss 1.9269 Learning Rate 0.00124925\n",
            "Batch 341 Loss 0.8372 Learning Rate 0.001248\n",
            "Batch 351 Loss 0.9462 Learning Rate 0.00124676\n",
            "Batch 361 Loss 0.7232 Learning Rate 0.00124552\n",
            "Batch 371 Loss 0.803 Learning Rate 0.00124429\n",
            "Batch 381 Loss 0.7926 Learning Rate 0.00124306\n",
            "Batch 391 Loss 0.8918 Learning Rate 0.00124183\n",
            "Batch 401 Loss 0.7544 Learning Rate 0.00124061\n",
            "Batch 411 Loss 0.8678 Learning Rate 0.00123939\n",
            "Batch 421 Loss 1.0027 Learning Rate 0.00123817\n",
            "Batch 431 Loss 0.9852 Learning Rate 0.00123696\n",
            "Batch 441 Loss 0.8612 Learning Rate 0.00123575\n",
            "Batch 451 Loss 0.694 Learning Rate 0.00123454\n",
            "Batch 461 Loss 1.0342 Learning Rate 0.00123334\n",
            "Batch 471 Loss 0.9773 Learning Rate 0.00123214\n",
            "Batch 481 Loss 1.0179 Learning Rate 0.00123094\n",
            "Batch 491 Loss 1.0208 Learning Rate 0.00122975\n",
            "Batch 501 Loss 0.8835 Learning Rate 0.00122856\n",
            "Batch 511 Loss 0.9704 Learning Rate 0.00122738\n",
            "Batch 521 Loss 0.9537 Learning Rate 0.0012262\n",
            "Batch 531 Loss 0.8024 Learning Rate 0.00122502\n",
            "Batch 541 Loss 0.8279 Learning Rate 0.00122384\n",
            "Batch 551 Loss 1.0549 Learning Rate 0.00122267\n",
            "Batch 561 Loss 0.8928 Learning Rate 0.0012215\n",
            "Batch 571 Loss 1.0867 Learning Rate 0.00122034\n",
            "Batch 581 Loss 0.8665 Learning Rate 0.00121918\n",
            "Batch 591 Loss 0.8975 Learning Rate 0.00121802\n",
            "Batch 601 Loss 0.7038 Learning Rate 0.00121687\n",
            "Batch 611 Loss 0.7794 Learning Rate 0.00121571\n",
            "Batch 621 Loss 0.7144 Learning Rate 0.00121457\n",
            "Batch 631 Loss 0.8553 Learning Rate 0.00121342\n",
            "Batch 641 Loss 0.8355 Learning Rate 0.00121228\n",
            "Batch 651 Loss 0.8149 Learning Rate 0.00121114\n",
            "Batch 661 Loss 1.4383 Learning Rate 0.00121\n",
            "Batch 671 Loss 0.9244 Learning Rate 0.00120887\n",
            "Batch 681 Loss 0.7486 Learning Rate 0.00120774\n",
            "Batch 691 Loss 0.9973 Learning Rate 0.00120662\n",
            "Batch 701 Loss 0.9311 Learning Rate 0.00120549\n",
            "Batch 711 Loss 0.862 Learning Rate 0.00120438\n",
            "Batch 721 Loss 0.8764 Learning Rate 0.00120326\n",
            "Batch 731 Loss 0.8818 Learning Rate 0.00120215\n",
            "Batch 741 Loss 0.787 Learning Rate 0.00120104\n",
            "Batch 751 Loss 1.056 Learning Rate 0.00119993\n",
            "Batch 761 Loss 0.8788 Learning Rate 0.00119882\n",
            "Batch 771 Loss 0.6554 Learning Rate 0.00119772\n",
            "Batch 0 validation loss: 0.42376968264579773\n",
            "Batch 1 validation loss: 0.39499369263648987\n",
            "Batch 2 validation loss: 1.1745940446853638\n",
            "Batch 3 validation loss: 0.5683516263961792\n",
            "Batch 4 validation loss: 0.6695708632469177\n",
            "Batch 5 validation loss: 0.6224197745323181\n",
            "Batch 6 validation loss: 0.6847373247146606\n",
            "Batch 7 validation loss: 0.6440747380256653\n",
            "Batch 8 validation loss: 0.6749747395515442\n",
            "Batch 9 validation loss: 0.6987730860710144\n",
            "Batch 10 validation loss: 0.6564464569091797\n",
            "Batch 11 validation loss: 0.8089222311973572\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 7 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.7604 Learning Rate 0.00119673\n",
            "Batch 11 Loss 0.8356 Learning Rate 0.00119564\n",
            "Batch 21 Loss 0.7605 Learning Rate 0.00119455\n",
            "Batch 31 Loss 0.5082 Learning Rate 0.00119346\n",
            "Batch 41 Loss 0.1508 Learning Rate 0.00119237\n",
            "Batch 51 Loss 0.687 Learning Rate 0.00119129\n",
            "Batch 61 Loss 0.603 Learning Rate 0.00119021\n",
            "Batch 71 Loss 0.6633 Learning Rate 0.00118913\n",
            "Batch 81 Loss 0.5406 Learning Rate 0.00118805\n",
            "Batch 91 Loss 0.8889 Learning Rate 0.00118698\n",
            "Batch 101 Loss 0.7091 Learning Rate 0.00118591\n",
            "Batch 111 Loss 0.6645 Learning Rate 0.00118485\n",
            "Batch 121 Loss 0.7432 Learning Rate 0.00118378\n",
            "Batch 131 Loss 0.6828 Learning Rate 0.00118272\n",
            "Batch 141 Loss 0.6583 Learning Rate 0.00118167\n",
            "Batch 151 Loss 0.6282 Learning Rate 0.00118061\n",
            "Batch 161 Loss 0.7186 Learning Rate 0.00117956\n",
            "Batch 171 Loss 0.6794 Learning Rate 0.00117851\n",
            "Batch 181 Loss 0.7302 Learning Rate 0.00117747\n",
            "Batch 191 Loss 0.6454 Learning Rate 0.00117642\n",
            "Batch 201 Loss 0.6011 Learning Rate 0.00117538\n",
            "Batch 211 Loss 0.6946 Learning Rate 0.00117434\n",
            "Batch 221 Loss 0.6836 Learning Rate 0.00117331\n",
            "Batch 231 Loss 0.7312 Learning Rate 0.00117228\n",
            "Batch 241 Loss 0.7475 Learning Rate 0.00117125\n",
            "Batch 251 Loss 0.5815 Learning Rate 0.00117022\n",
            "Batch 261 Loss 0.7373 Learning Rate 0.00116919\n",
            "Batch 271 Loss 0.7417 Learning Rate 0.00116817\n",
            "Batch 281 Loss 0.6084 Learning Rate 0.00116715\n",
            "Batch 291 Loss 0.6074 Learning Rate 0.00116614\n",
            "Batch 301 Loss 0.8546 Learning Rate 0.00116512\n",
            "Batch 311 Loss 0.6727 Learning Rate 0.00116411\n",
            "Batch 321 Loss 0.6502 Learning Rate 0.00116311\n",
            "Batch 331 Loss 0.7033 Learning Rate 0.0011621\n",
            "Batch 341 Loss 0.7604 Learning Rate 0.0011611\n",
            "Batch 351 Loss 0.8107 Learning Rate 0.0011601\n",
            "Batch 361 Loss 0.9538 Learning Rate 0.0011591\n",
            "Batch 371 Loss 0.6767 Learning Rate 0.0011581\n",
            "Batch 381 Loss 0.6966 Learning Rate 0.00115711\n",
            "Batch 391 Loss 0.6723 Learning Rate 0.00115612\n",
            "Batch 401 Loss 0.6667 Learning Rate 0.00115513\n",
            "Batch 411 Loss 0.7615 Learning Rate 0.00115415\n",
            "Batch 421 Loss 0.7198 Learning Rate 0.00115316\n",
            "Batch 431 Loss 0.7827 Learning Rate 0.00115218\n",
            "Batch 441 Loss 0.7629 Learning Rate 0.00115121\n",
            "Batch 451 Loss 0.7349 Learning Rate 0.00115023\n",
            "Batch 461 Loss 0.7969 Learning Rate 0.00114926\n",
            "Batch 471 Loss 0.8438 Learning Rate 0.00114829\n",
            "Batch 481 Loss 0.6946 Learning Rate 0.00114732\n",
            "Batch 491 Loss 0.8818 Learning Rate 0.00114635\n",
            "Batch 501 Loss 0.6881 Learning Rate 0.00114539\n",
            "Batch 511 Loss 0.6197 Learning Rate 0.00114443\n",
            "Batch 521 Loss 0.6901 Learning Rate 0.00114347\n",
            "Batch 531 Loss 0.6142 Learning Rate 0.00114252\n",
            "Batch 541 Loss 0.6518 Learning Rate 0.00114156\n",
            "Batch 551 Loss 0.978 Learning Rate 0.00114061\n",
            "Batch 561 Loss 0.6894 Learning Rate 0.00113966\n",
            "Batch 571 Loss 0.6447 Learning Rate 0.00113872\n",
            "Batch 581 Loss 0.731 Learning Rate 0.00113777\n",
            "Batch 591 Loss 0.7099 Learning Rate 0.00113683\n",
            "Batch 601 Loss 0.8739 Learning Rate 0.00113589\n",
            "Batch 611 Loss 0.6922 Learning Rate 0.00113496\n",
            "Batch 621 Loss 0.7661 Learning Rate 0.00113402\n",
            "Batch 631 Loss 0.7394 Learning Rate 0.00113309\n",
            "Batch 641 Loss 0.9672 Learning Rate 0.00113216\n",
            "Batch 651 Loss 0.6935 Learning Rate 0.00113123\n",
            "Batch 661 Loss 0.6894 Learning Rate 0.00113031\n",
            "Batch 671 Loss 0.7425 Learning Rate 0.00112938\n",
            "Batch 681 Loss 0.8117 Learning Rate 0.00112846\n",
            "Batch 691 Loss 0.8314 Learning Rate 0.00112755\n",
            "Batch 701 Loss 0.8428 Learning Rate 0.00112663\n",
            "Batch 711 Loss 0.7737 Learning Rate 0.00112572\n",
            "Batch 721 Loss 0.8826 Learning Rate 0.0011248\n",
            "Batch 731 Loss 0.6146 Learning Rate 0.00112389\n",
            "Batch 741 Loss 0.7377 Learning Rate 0.00112299\n",
            "Batch 751 Loss 0.9052 Learning Rate 0.00112208\n",
            "Batch 761 Loss 0.8155 Learning Rate 0.00112118\n",
            "Batch 771 Loss 0.6785 Learning Rate 0.00112028\n",
            "Batch 0 validation loss: 0.4027182161808014\n",
            "Batch 1 validation loss: 0.3379262387752533\n",
            "Batch 2 validation loss: 1.0077354907989502\n",
            "Batch 3 validation loss: 0.45589905977249146\n",
            "Batch 4 validation loss: 0.5518943667411804\n",
            "Batch 5 validation loss: 0.49124956130981445\n",
            "Batch 6 validation loss: 0.5633066892623901\n",
            "Batch 7 validation loss: 0.5130077600479126\n",
            "Batch 8 validation loss: 0.5552563667297363\n",
            "Batch 9 validation loss: 0.5248106122016907\n",
            "Batch 10 validation loss: 0.5341277718544006\n",
            "Batch 11 validation loss: 0.6066977977752686\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 8 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 1.3508 Learning Rate 0.00111947\n",
            "Batch 11 Loss 0.572 Learning Rate 0.00111857\n",
            "Batch 21 Loss 0.65 Learning Rate 0.00111768\n",
            "Batch 31 Loss 0.5253 Learning Rate 0.00111678\n",
            "Batch 41 Loss 0.5803 Learning Rate 0.00111589\n",
            "Batch 51 Loss 0.6959 Learning Rate 0.00111501\n",
            "Batch 61 Loss 0.6296 Learning Rate 0.00111412\n",
            "Batch 71 Loss 0.4831 Learning Rate 0.00111324\n",
            "Batch 81 Loss 0.5066 Learning Rate 0.00111235\n",
            "Batch 91 Loss 0.6399 Learning Rate 0.00111147\n",
            "Batch 101 Loss 0.5773 Learning Rate 0.0011106\n",
            "Batch 111 Loss 0.5247 Learning Rate 0.00110972\n",
            "Batch 121 Loss 0.4635 Learning Rate 0.00110885\n",
            "Batch 131 Loss 0.6064 Learning Rate 0.00110797\n",
            "Batch 141 Loss 0.594 Learning Rate 0.00110711\n",
            "Batch 151 Loss 0.4996 Learning Rate 0.00110624\n",
            "Batch 161 Loss 0.5182 Learning Rate 0.00110537\n",
            "Batch 171 Loss 0.6714 Learning Rate 0.00110451\n",
            "Batch 181 Loss 0.14 Learning Rate 0.00110365\n",
            "Batch 191 Loss 0.5744 Learning Rate 0.00110279\n",
            "Batch 201 Loss 0.5652 Learning Rate 0.00110193\n",
            "Batch 211 Loss 0.5764 Learning Rate 0.00110108\n",
            "Batch 221 Loss 0.5812 Learning Rate 0.00110022\n",
            "Batch 231 Loss 0.1916 Learning Rate 0.00109937\n",
            "Batch 241 Loss 0.547 Learning Rate 0.00109852\n",
            "Batch 251 Loss 0.636 Learning Rate 0.00109767\n",
            "Batch 261 Loss 0.5678 Learning Rate 0.00109683\n",
            "Batch 271 Loss 0.5664 Learning Rate 0.00109599\n",
            "Batch 281 Loss 0.689 Learning Rate 0.00109514\n",
            "Batch 291 Loss 0.6248 Learning Rate 0.0010943\n",
            "Batch 301 Loss 0.6985 Learning Rate 0.00109347\n",
            "Batch 311 Loss 0.561 Learning Rate 0.00109263\n",
            "Batch 321 Loss 0.5623 Learning Rate 0.0010918\n",
            "Batch 331 Loss 0.5749 Learning Rate 0.00109096\n",
            "Batch 341 Loss 0.6033 Learning Rate 0.00109013\n",
            "Batch 351 Loss 0.5712 Learning Rate 0.00108931\n",
            "Batch 361 Loss 0.6037 Learning Rate 0.00108848\n",
            "Batch 371 Loss 0.8159 Learning Rate 0.00108766\n",
            "Batch 381 Loss 0.727 Learning Rate 0.00108683\n",
            "Batch 391 Loss 0.6313 Learning Rate 0.00108601\n",
            "Batch 401 Loss 0.7054 Learning Rate 0.00108519\n",
            "Batch 411 Loss 0.6472 Learning Rate 0.00108438\n",
            "Batch 421 Loss 0.5849 Learning Rate 0.00108356\n",
            "Batch 431 Loss 0.5823 Learning Rate 0.00108275\n",
            "Batch 441 Loss 0.6356 Learning Rate 0.00108194\n",
            "Batch 451 Loss 0.5937 Learning Rate 0.00108113\n",
            "Batch 461 Loss 0.6534 Learning Rate 0.00108032\n",
            "Batch 471 Loss 0.6214 Learning Rate 0.00107951\n",
            "Batch 481 Loss 0.7143 Learning Rate 0.00107871\n",
            "Batch 491 Loss 0.911 Learning Rate 0.00107791\n",
            "Batch 501 Loss 0.6967 Learning Rate 0.00107711\n",
            "Batch 511 Loss 0.6214 Learning Rate 0.00107631\n",
            "Batch 521 Loss 0.617 Learning Rate 0.00107551\n",
            "Batch 531 Loss 0.6231 Learning Rate 0.00107471\n",
            "Batch 541 Loss 0.7508 Learning Rate 0.00107392\n",
            "Batch 551 Loss 0.6256 Learning Rate 0.00107313\n",
            "Batch 561 Loss 0.5567 Learning Rate 0.00107234\n",
            "Batch 571 Loss 0.8397 Learning Rate 0.00107155\n",
            "Batch 581 Loss 0.7519 Learning Rate 0.00107076\n",
            "Batch 591 Loss 0.714 Learning Rate 0.00106998\n",
            "Batch 601 Loss 0.6307 Learning Rate 0.0010692\n",
            "Batch 611 Loss 0.6128 Learning Rate 0.00106842\n",
            "Batch 621 Loss 0.6106 Learning Rate 0.00106764\n",
            "Batch 631 Loss 0.622 Learning Rate 0.00106686\n",
            "Batch 641 Loss 0.5982 Learning Rate 0.00106608\n",
            "Batch 651 Loss 0.6912 Learning Rate 0.00106531\n",
            "Batch 661 Loss 0.6651 Learning Rate 0.00106453\n",
            "Batch 671 Loss 0.628 Learning Rate 0.00106376\n",
            "Batch 681 Loss 0.6419 Learning Rate 0.00106299\n",
            "Batch 691 Loss 0.5986 Learning Rate 0.00106222\n",
            "Batch 701 Loss 0.6998 Learning Rate 0.00106146\n",
            "Batch 711 Loss 0.5642 Learning Rate 0.00106069\n",
            "Batch 721 Loss 1.5231 Learning Rate 0.00105993\n",
            "Batch 731 Loss 0.6375 Learning Rate 0.00105917\n",
            "Batch 741 Loss 0.643 Learning Rate 0.00105841\n",
            "Batch 751 Loss 0.6642 Learning Rate 0.00105765\n",
            "Batch 761 Loss 0.815 Learning Rate 0.0010569\n",
            "Batch 771 Loss 0.6416 Learning Rate 0.00105614\n",
            "Batch 0 validation loss: 0.49883466958999634\n",
            "Batch 1 validation loss: 0.2806640863418579\n",
            "Batch 2 validation loss: 0.7840237021446228\n",
            "Batch 3 validation loss: 0.40212151408195496\n",
            "Batch 4 validation loss: 0.4570792019367218\n",
            "Batch 5 validation loss: 0.4218963384628296\n",
            "Batch 6 validation loss: 0.4862827956676483\n",
            "Batch 7 validation loss: 0.45274075865745544\n",
            "Batch 8 validation loss: 0.4533703327178955\n",
            "Batch 9 validation loss: 0.472936749458313\n",
            "Batch 10 validation loss: 0.4385148286819458\n",
            "Batch 11 validation loss: 0.5725027918815613\n",
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 9 %%%%%%%%%%\n",
            "\n",
            "Batch 1 Loss 0.8977 Learning Rate 0.00105546\n",
            "Batch 11 Loss 0.5679 Learning Rate 0.00105471\n",
            "Batch 21 Loss 0.602 Learning Rate 0.00105396\n",
            "Batch 31 Loss 0.474 Learning Rate 0.00105321\n",
            "Batch 41 Loss 0.5942 Learning Rate 0.00105247\n",
            "Batch 51 Loss 0.434 Learning Rate 0.00105172\n",
            "Batch 61 Loss 0.4477 Learning Rate 0.00105098\n",
            "Batch 71 Loss 0.4411 Learning Rate 0.00105023\n",
            "Batch 81 Loss 0.5254 Learning Rate 0.00104949\n",
            "Batch 91 Loss 0.4756 Learning Rate 0.00104876\n",
            "Batch 101 Loss 0.5448 Learning Rate 0.00104802\n",
            "Batch 111 Loss 0.6548 Learning Rate 0.00104728\n",
            "Batch 121 Loss 0.5317 Learning Rate 0.00104655\n",
            "Batch 131 Loss 0.4192 Learning Rate 0.00104581\n",
            "Batch 141 Loss 0.4533 Learning Rate 0.00104508\n",
            "Batch 151 Loss 0.516 Learning Rate 0.00104435\n",
            "Batch 161 Loss 0.5028 Learning Rate 0.00104363\n",
            "Batch 171 Loss 0.6023 Learning Rate 0.0010429\n",
            "Batch 181 Loss 0.5455 Learning Rate 0.00104217\n",
            "Batch 191 Loss 0.4533 Learning Rate 0.00104145\n",
            "Batch 201 Loss 0.5087 Learning Rate 0.00104073\n",
            "Batch 211 Loss 0.5435 Learning Rate 0.00104001\n",
            "Batch 221 Loss 0.5692 Learning Rate 0.00103929\n",
            "Batch 231 Loss 0.4778 Learning Rate 0.00103857\n",
            "Batch 241 Loss 0.4785 Learning Rate 0.00103785\n",
            "Batch 251 Loss 0.4858 Learning Rate 0.00103714\n",
            "Batch 261 Loss 0.5899 Learning Rate 0.00103643\n",
            "Batch 271 Loss 0.2035 Learning Rate 0.00103571\n",
            "Batch 281 Loss 0.5301 Learning Rate 0.001035\n",
            "Batch 291 Loss 0.5779 Learning Rate 0.00103429\n",
            "Batch 301 Loss 0.6102 Learning Rate 0.00103359\n",
            "Batch 311 Loss 0.5809 Learning Rate 0.00103288\n",
            "Batch 321 Loss 0.5476 Learning Rate 0.00103218\n",
            "Batch 331 Loss 0.5007 Learning Rate 0.00103147\n",
            "Batch 341 Loss 0.4839 Learning Rate 0.00103077\n",
            "Batch 351 Loss 0.5019 Learning Rate 0.00103007\n",
            "Batch 361 Loss 0.976 Learning Rate 0.00102937\n",
            "Batch 371 Loss 0.5139 Learning Rate 0.00102868\n",
            "Batch 381 Loss 0.5338 Learning Rate 0.00102798\n",
            "Batch 391 Loss 0.4835 Learning Rate 0.00102729\n",
            "Batch 401 Loss 0.6258 Learning Rate 0.00102659\n",
            "Batch 411 Loss 0.5803 Learning Rate 0.0010259\n",
            "Batch 421 Loss 0.5914 Learning Rate 0.00102521\n",
            "Batch 431 Loss 0.6999 Learning Rate 0.00102452\n",
            "Batch 441 Loss 0.5545 Learning Rate 0.00102383\n",
            "Batch 451 Loss 0.559 Learning Rate 0.00102315\n",
            "Batch 461 Loss 0.6423 Learning Rate 0.00102246\n",
            "Batch 471 Loss 0.7378 Learning Rate 0.00102178\n",
            "Batch 481 Loss 0.5256 Learning Rate 0.0010211\n",
            "Batch 491 Loss 0.4825 Learning Rate 0.00102042\n",
            "Batch 501 Loss 0.6748 Learning Rate 0.00101974\n",
            "Batch 511 Loss 0.569 Learning Rate 0.00101906\n",
            "Batch 521 Loss 0.6455 Learning Rate 0.00101838\n",
            "Batch 531 Loss 0.4939 Learning Rate 0.00101771\n",
            "Batch 541 Loss 0.7892 Learning Rate 0.00101703\n",
            "Batch 551 Loss 0.5721 Learning Rate 0.00101636\n",
            "Batch 561 Loss 0.558 Learning Rate 0.00101569\n",
            "Batch 571 Loss 0.502 Learning Rate 0.00101502\n",
            "Batch 581 Loss 0.5817 Learning Rate 0.00101435\n",
            "Batch 591 Loss 0.8173 Learning Rate 0.00101368\n",
            "Batch 601 Loss 0.6777 Learning Rate 0.00101302\n",
            "Batch 611 Loss 0.5244 Learning Rate 0.00101235\n",
            "Batch 621 Loss 0.5122 Learning Rate 0.00101169\n",
            "Batch 631 Loss 0.5769 Learning Rate 0.00101103\n",
            "Batch 641 Loss 0.5072 Learning Rate 0.00101037\n",
            "Batch 651 Loss 0.4888 Learning Rate 0.00100971\n",
            "Batch 661 Loss 0.5363 Learning Rate 0.00100905\n",
            "Batch 671 Loss 0.5689 Learning Rate 0.00100839\n",
            "Batch 681 Loss 0.606 Learning Rate 0.00100774\n",
            "Batch 691 Loss 0.508 Learning Rate 0.00100708\n",
            "Batch 701 Loss 0.5507 Learning Rate 0.00100643\n",
            "Batch 711 Loss 0.5889 Learning Rate 0.00100578\n",
            "Batch 721 Loss 0.7006 Learning Rate 0.00100513\n",
            "Batch 731 Loss 0.5267 Learning Rate 0.00100448\n",
            "Batch 741 Loss 0.5955 Learning Rate 0.00100383\n",
            "Batch 751 Loss 0.5536 Learning Rate 0.00100318\n",
            "Batch 761 Loss 0.5481 Learning Rate 0.00100254\n",
            "Batch 771 Loss 0.7954 Learning Rate 0.00100189\n",
            "Batch 0 validation loss: 0.3192414343357086\n",
            "Batch 1 validation loss: 0.28700950741767883\n",
            "Batch 2 validation loss: 0.7493873238563538\n",
            "Batch 3 validation loss: 0.3412642180919647\n",
            "Batch 4 validation loss: 0.4361801743507385\n",
            "Batch 5 validation loss: 0.3740496337413788\n",
            "Batch 6 validation loss: 0.3861698508262634\n",
            "Batch 7 validation loss: 0.3732485771179199\n",
            "Batch 8 validation loss: 0.384757936000824\n",
            "Batch 9 validation loss: 0.3823656737804413\n",
            "Batch 10 validation loss: 0.362923264503479\n",
            "Batch 11 validation loss: 0.4406208395957947\n",
            "Translation:\tAs daughters of God , you were born to lead . \n",
            "Target:\tAs daughters of God , you were born to lead . \n",
            "\n",
            "Translation:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "Target:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "\n",
            "Translation:\tThe other counselor was a prominent judge in the city . \n",
            "Target:\tThe other counselor was a prominent judge in the city . \n",
            "\n",
            "Translation:\tMy wife , Harriet , was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "Target:\tMy wife , Harriet , was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "\n",
            "Translation:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "Target:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "\n",
            "Translation:\t“ And he that receiveth my Father , receiveth the kingdom of my Father ; therefore all that my Father hath shall be given unto him . ” \n",
            "Target:\t“ And he that receiveth my Father receiveth my Father ’s kingdom ; therefore all that my Father hath shall be given unto him . ” \n",
            "\n",
            "Translation:\tAlthough it is good and working for physical protection and healing during our mortal existence , our attention should be focused on the spiritual miracles that are available to all of God ’s children . \n",
            "Target:\tWhile it is good to pray for and work for physical protection and healing during our mortal existence , our supreme focus should be on the spiritual miracles that are available to all of God ’s children . \n",
            "\n",
            "Translation:\tAnother regret people expressed was that they failed to become the person they felt they could or should have been . As they looked back in the mirror , they realized that they never lived up their potential , too many songs remained unsung . \n",
            "Target:\tAnother regret people expressed was that they failed to become the person they felt they could and should have been . When they looked back on their lives , they realized that they never lived up to their potential , that too many songs remained unsung . \n",
            "\n",
            "Translation:\tThe Savior taught , “ Ye shall always do to those who repent and be baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always remember me \n",
            "Target:\tThe Savior taught : “ This shall ye always do to those who repent and are baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always remember me ye shall have my Spirit to be with you ” ( 3 Nephi 18:11 ) . \n",
            "\n",
            "Translation:\tAfter several years he returned to his home . However , the people refused to acknowledge his growth and improvement . To them , he was still just old “ so - and - so , ” and they treated him that way . Eventually , this good man faded away to a shadow of his own without being \n",
            "Target:\tAfter several years he returned to his hometown . However , the people refused to acknowledge his growth and improvement . To them , he was still just old “ so - and - so , ” and they treated him that way . Eventually , this good man faded away to a shadow of his former successful self without being able to use his marvelously developed talents to bless those who derided and rejected him once again . What a loss , both for him and the community ! \n",
            "\n",
            "Translation:\tThe blessings you receive as I served serve others are many . I have sometimes said , “ Oh , I ’ve got to make my visiting teaching visits ! ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I saw as a burden rather than a burden , \n",
            "Target:\tThe blessings you receive as you serve others are many . I have sometimes said , “ Oh , I ’ve got to get my visiting teaching done ! ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I was looking at it as a burden rather than a blessing . ) I can honestly say that when I went visiting teaching , I always felt better . I was lifted , loved , and blessed , usually much more than the sister I was visiting . My love increased . My desire to serve increased . And I could see what a beautiful way Heavenly Father has planned for us to watch over and care for one another . \n",
            "\n",
            "Translation:\tAnother journal entry reads : “ The … miracle for me occurred in the Family History office of felt calm Olsen , who presented me with a printed and someday known to the my forefathers of all the known of the Ancestral File to the king sent to the society . They came mostly from the records of the \n",
            "Target:\tAnother journal entry reads : “ The … miracle for me occurred in the Family History office of Mel Olsen who presented me with a printout of all my known ancestral pedigrees taken from the update of the Ancestral File computerized records sent into the genealogical society . They came mostly from the records of the four generation ’s program the Church called for many years ago . I had been overwhelmed with the thought of the huge task ahead of me to gather all my ancestors’ research records from family organizations to get them all in the computer for the first computerized distribution of the Ancestral File . And there they all were , beautiful , organized and laser printed and sitting there on the desk before me . I was so thrilled and so overwhelmed I just sat there stunned and then began to cry I was so happy . … For one who has <unk> , painstakingly researched for <unk> years , the <unk> of all these records is truly exciting . And when I think of the hundreds of thousands of people who are now or soon will be computerizing huge blocks of censuses and private research disks … I am so excited . It is truly the Lord ’s work and He is directing it . ” \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3083AaetOtSz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Print out the 1-layer results again so we can look at the spanish"
      ]
    },
    {
      "metadata": {
        "id": "w-D0JMTdEoyg",
        "colab_type": "code",
        "outputId": "075615b6-96a3-4497-c547-98220e678606",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "cell_type": "code",
      "source": [
        "# Decode the model to produce translations (first sentence in validation set)\n",
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    \n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    \n",
        "    print(\"Original:\", end=\"\\t\")\n",
        "    for i in range(1, batch.src.size(0)):\n",
        "        sym = SRC.vocab.itos[batch.src.data[i,0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    \n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    \n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:\thijas de Dios , nacieron para liderar . \n",
            "Translation:\tAs daughters of God , you were born to lead . \n",
            "Target:\tAs daughters of God , you were born to lead . \n",
            "\n",
            "Original:\tse turbe vuestro corazón ni tenga miedo ” . \n",
            "Translation:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "Target:\tLet not your heart be troubled , neither let it be afraid . ” \n",
            "\n",
            "Original:\totro consejero era un juez importante de la ciudad . \n",
            "Translation:\tThe other counselor was a prominent judge in the city . \n",
            "Target:\tThe other counselor was a prominent judge in the city . \n",
            "\n",
            "Original:\tesposa Harriet era la mejor para hallar algo inspirador , edificante o cómico para compartir ; \n",
            "Translation:\tMy wife , Harriet , was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "Target:\tMy wife , Harriet , was always the best at finding something inspirational , uplifting , or humorous to share . \n",
            "\n",
            "Original:\tY Jesús les dijo : Seguid orando ; y ellos no cesaban de orar ” ( 3 Nefi 19:26 ) . \n",
            "Translation:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "Target:\t“ And Jesus said unto them : Pray on ; nevertheless they did not cease to pray ” ( 3 Nephi 19:26 ) . \n",
            "\n",
            "Original:\ty el que recibe a mi Padre , recibe el reino de mi Padre ; por tanto , todo lo que mi Padre tiene le será dado ” . \n",
            "Translation:\t“ And he that receiveth my Father , receiveth the kingdom of my Father ; therefore all that my Father hath shall be given unto him . ” \n",
            "Target:\t“ And he that receiveth my Father receiveth my Father ’s kingdom ; therefore all that my Father hath shall be given unto him . ” \n",
            "\n",
            "Original:\tes bueno orar y trabajar por la protección física y la sanación durante nuestra existencia mortal , nuestra atención suprema debe centrarse en los milagros espirituales que están al alcance de todos los hijos de Dios . \n",
            "Translation:\tAlthough it is good and working for physical protection and healing during our mortal existence , our attention should be focused on the spiritual miracles that are available to all of God ’s children . \n",
            "Target:\tWhile it is good to pray for and work for physical protection and healing during our mortal existence , our supreme focus should be on the spiritual miracles that are available to all of God ’s children . \n",
            "\n",
            "Original:\tmás que las personas lamentaron fue el no llegar a ser la persona que sentían que podrían o deberían haber sido . Al mirar su vida en retrospectiva , se daban cuenta de que nunca estuvieron a la altura de su potencial ; habían quedado demasiadas cosas sin hacer . \n",
            "Translation:\tAnother regret people expressed was that they failed to become the person they felt they could or should have been . As they looked back in the mirror , they realized that they never lived up their potential , too many songs remained unsung . \n",
            "Target:\tAnother regret people expressed was that they failed to become the person they felt they could and should have been . When they looked back on their lives , they realized that they never lived up to their potential , that too many songs remained unsung . \n",
            "\n",
            "Original:\tSalvador enseñó : “ Y siempre haréis esto por todos los que se arrepientan y se bauticen en mi nombre ; y lo haréis en memoria de mi sangre , que he vertido por vosotros , para que testifiquéis al Padre que siempre os acordáis de mí . Y si os acordáis siempre de mí , tendréis mi Espíritu para que esté con vosotros ” ( 3 Nefi 18:11 ) . \n",
            "Translation:\tThe Savior taught , “ Ye shall always do to those who repent and be baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always remember me \n",
            "Target:\tThe Savior taught : “ This shall ye always do to those who repent and are baptized in my name ; and ye shall do it in remembrance of my blood , which I have shed for you , that ye may witness unto the Father that ye do always remember me . And if ye do always remember me ye shall have my Spirit to be with you ” ( 3 Nephi 18:11 ) . \n",
            "\n",
            "Original:\tde varios años , regresó a su ciudad natal . Sin embargo , la gente se negó a reconocer su progreso y superación ; para ellos , él aún era solo “ fulano de tal ” , y lo trataron de esa manera . Con el tiempo , ese buen hombre se esfumó , quedando en la sombra de la persona de éxito que fue , sin poder utilizar los talentos que maravillosamente desarrolló para bendecir a aquellos que una vez más lo ridiculizaron y rechazaron . ¡ Qué gran pérdida , para él y la comunidad ! \n",
            "Translation:\tAfter several years he returned to his home . However , the people refused to acknowledge his growth and improvement . To them , he was still just old “ so - and - so , ” and they treated him that way . Eventually , this good man faded away to a shadow of his own without being \n",
            "Target:\tAfter several years he returned to his hometown . However , the people refused to acknowledge his growth and improvement . To them , he was still just old “ so - and - so , ” and they treated him that way . Eventually , this good man faded away to a shadow of his former successful self without being able to use his marvelously developed talents to bless those who derided and rejected him once again . What a loss , both for him and the community ! \n",
            "\n",
            "Original:\tbendiciones que ustedes reciben al servir a los demás son muchas . A veces he dicho : “ ¡ Ay , tengo que hacer mis visitas ! ” . ( Ésas fueron las ocasiones en que me olvidaba de que estaba visitando y enseñando a mujeres . Ésas fueron las ocasiones en que lo veía como un peso , más que como una bendición . ) Sinceramente , puedo decir que , cada vez que hacía las visitas , siempre me sentía mejor ; era edificada , amada y bendecida , por lo general mucho más que la hermana a la que yo visitaba . Mi amor aumentaba ; mi deseo de servir era mayor ; y podía ver qué método maravilloso ha establecido el Padre Celestial para que velemos y nos cuidemos mutuamente . \n",
            "Translation:\tThe blessings you receive as I served serve others are many . I have sometimes said , “ Oh , I ’ve got to make my visiting teaching visits ! ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I saw as a burden rather than a burden , \n",
            "Target:\tThe blessings you receive as you serve others are many . I have sometimes said , “ Oh , I ’ve got to get my visiting teaching done ! ” ( Those were the times I forgot I was visiting and teaching women . Those were the times I was looking at it as a burden rather than a blessing . ) I can honestly say that when I went visiting teaching , I always felt better . I was lifted , loved , and blessed , usually much more than the sister I was visiting . My love increased . My desire to serve increased . And I could see what a beautiful way Heavenly Father has planned for us to watch over and care for one another . \n",
            "\n",
            "Original:\tanotación dice : “ El … milagro tuvo lugar para mí en la oficina de Mel Olsen , en Historia Familiar , quien me dio una copia impresa de todos los cuadros genealógicos que yo conocía sacados de los registros computarizados y actualizados de Ancestral File , enviados a la sociedad genealógica . La mayoría de ellos provenían de los registros de cuatro generaciones del programa que la Iglesia pidió por muchos años . Me había sentido abrumada al pensar en la enorme tarea que tenía delante de mí de juntar toda la información sobre mis antepasados de las organizaciones familiares para ponerlos todos en la computadora para la primera distribución computarizada de Ancestral File . Y allí estaban todos , hermosos , organizados e impresos con laser , sobre el escritorio enfrente de mí . Sentí tanto entusiasmo y emoción que me quedé sentada impresionada y comencé a llorar . Estaba tan feliz … Para alguien que ha investigado tenaz y cuidadosamente por treinta años , la <unk> de esos registros era en verdad emocionante . Y cuando pienso en los cientos de miles de personas que ahora o muy pronto computarizarán enormes conjuntos de padrones y discos de investigación privada me siento muy emocionada . Es en verdad la obra de Dios y es Él quien la dirige ” . \n",
            "Translation:\tAnother journal entry reads : “ The … miracle for me occurred in the Family History office of felt calm Olsen , who presented me with a printed and someday known to the my forefathers of all the known of the Ancestral File to the king sent to the society . They came mostly from the records of the \n",
            "Target:\tAnother journal entry reads : “ The … miracle for me occurred in the Family History office of Mel Olsen who presented me with a printout of all my known ancestral pedigrees taken from the update of the Ancestral File computerized records sent into the genealogical society . They came mostly from the records of the four generation ’s program the Church called for many years ago . I had been overwhelmed with the thought of the huge task ahead of me to gather all my ancestors’ research records from family organizations to get them all in the computer for the first computerized distribution of the Ancestral File . And there they all were , beautiful , organized and laser printed and sitting there on the desk before me . I was so thrilled and so overwhelmed I just sat there stunned and then began to cry I was so happy . … For one who has <unk> , painstakingly researched for <unk> years , the <unk> of all these records is truly exciting . And when I think of the hundreds of thousands of people who are now or soon will be computerizing huge blocks of censuses and private research disks … I am so excited . It is truly the Lord ’s work and He is directing it . ” \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dzgdEJTwKEm8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I printed out the original Spanish for each sentence because I suspected that when we use fewer heads of attention, the system is not as able to correctly translate phrases where spanish grammar puts words in a different order than english, so there is not as direct of a one-to-one mapping between words. It can still switch adjective order for adjacent words ('existencia mortal' does become 'mortal existence', as it should, even with one layer), but it has trouble with longer phrases with fewer layers. In some cases, though, the increasing freedom to move away from the original structure goes a bit awry in an overfitting-esque way.\n",
        "\n",
        "A couple of illustrative examples:\n",
        "\n",
        "\"The … miracle for me occurred in the Family History office of Mel Olsen\"\n",
        "* Spanish: “El … milagro tuvo lugar para mí en la oficina de Mel Olsen, en Historia Familiar\"\n",
        "* 1 layer: \"The … miracle for me occurred in the Family History office of felt calm Olsen\" (a bit confused)\n",
        "* 2 layers: \"The miracle … occurred to me in the Family History office of Grandmother\" (even more confused)\n",
        "* 6 layers: \"The … miracle for me occurred in the Family History office of Mel Olsen\" (correct!)\n",
        "\n",
        "\"And he that receiveth my Father receiveth my Father’s kingdom\" \n",
        "* Spanish: \"y el que recibe a mi Padre, recibe el reino de mi Padre\" \n",
        "* 1 layer: \"And he that receiveth my Father, receiveth the kingdom of my Father\" (maintaining Spanish word order)\n",
        "* 2 layers: \"And he that receiveth my Father ’s kingdom\" (lost part of the sentence, but got the English word order)\n",
        "* 6 layers: \"And he that receiveth my Father receiveth my Father ’s kingdom\" (correct!)\n",
        "\n",
        "\"Oh, I’ve got to get my visiting teaching done!\"\n",
        "* Spanish: \"¡Ay, tengo que hacer mis visitas!\"\n",
        "* 1 layer: \"Oh, I’ve got to make my visiting teaching visits!\" (follows Spanish grammar, but makes sense)\n",
        "* 2 layers: \"Oh, I’ve got to get my visiting teaching\" (follows English grammar, but loses a word)\n",
        "* 6 layers: \"Oh, I’ve been on my visits\" (misinterprets the sentence completely)\n"
      ]
    },
    {
      "metadata": {
        "id": "FJMFRhE3J0FT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}