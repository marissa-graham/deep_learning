{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 7 (Transformer--GC Data).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marissa-graham/deep_learning/blob/master/Lab_7_(Transformer_GC_Data).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zm37fELbgEMh",
        "colab_type": "code",
        "outputId": "c91e79fc-8b98-480c-d894-492ed036aac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "cell_type": "code",
      "source": [
        "!wget -O ./text_files.tar.gz 'http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2018:es-en-general-conference.tar.gz' \n",
        "!tar -xzf text_files.tar.gz\n",
        "\n",
        "!pip3 install torch spacy torchtext==0.2.3\n",
        "!python -m spacy download en\n",
        "!python -m spacy download es"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-21 05:01:38--  http://liftothers.org/dokuwiki/lib/exe/fetch.php?media=cs501r_f2018:es-en-general-conference.tar.gz\n",
            "Resolving liftothers.org (liftothers.org)... 50.62.229.1\n",
            "Connecting to liftothers.org (liftothers.org)|50.62.229.1|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18318204 (17M) [application/octet-stream]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]  17.47M  7.06MB/s    in 2.5s    \n",
            "\n",
            "2018-10-21 05:01:40 (7.06 MB/s) - ‘./text_files.tar.gz’ saved [18318204/18318204]\n",
            "\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.16)\n",
            "Requirement already satisfied: torchtext==0.2.3 in /usr/local/lib/python3.6/dist-packages (0.2.3)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.3.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy) (2018.1.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.15.2)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.12.0)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.3) (4.27.0)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy<0.4.4->spacy) (0.5.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.10.15)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.11.0)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.0->spacy) (0.9.0.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already satisfied: es_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.0.0/es_core_news_sm-2.0.0.tar.gz#egg=es_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/es_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/es\n",
            "\n",
            "    You can now load the model via spacy.load('es')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uv_0hn9iiAuf",
        "colab_type": "code",
        "outputId": "cee3e275-5bd9-40fc-ddc8-e73acc863282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchtext import data, datasets\n",
        "import spacy\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "import time\n",
        "\n",
        "print(glob.glob('./*'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./sample_data', './en-es_gc_2010-2017_en.txt', './text_files.tar.gz', './en-es_gc_2010-2017_es.txt', './en-es_conference.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_xHyEvxNjAW2",
        "colab_type": "code",
        "outputId": "77e695ca-a43d-4095-b853-c606952e0dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "gc_data_txt = ['./en-es_gc_2010-2017_en.txt', './en-es_gc_2010-2017_es.txt']\n",
        "gc_data_csv = './en-es_conference.csv'\n",
        "\n",
        "all_rows = []\n",
        "with open(gc_data_csv) as f:\n",
        "    i = 0\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        i += 1\n",
        "        #all_rows.append([row[0], row[1], row[2]])\n",
        "        print(row)\n",
        "        if i > 15:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'en', 'es']\n",
            "['0', 'The temple holds a place at the very center of our most sacred beliefs, and the Lord asks that we attend, ponder, study, and find personal meaning and application individually.', 'El templo tiene un lugar en el centro mismo de nuestras creencias más sagradas y el Señor nos pide que asistamos, meditemos, estudiemos y encontremos significado personal y aplicación individual.']\n",
            "['1', 'We will come to understand that through the ordinances of the temple, the power of godliness is manifest in our lives and that because of temple ordinances, we can be armed with God’s power, and His name will be upon us, His glory round about us, and His angels have charge over us.', 'Llegaremos a entender que mediante las ordenanzas del templo, el poder de la divinidad se manifiesta en nuestra vida y que, gracias a las ordenanzas del templo, podemos estar armados con el poder de Dios y Su nombre estará sobre nosotros, Su gloria nos rodeará y Sus ángeles nos guardarán.']\n",
            "['2', 'I wonder if we are fully drawing upon the power of those promises.', 'Me pregunto si estamos recurriendo completamente al poder de esas promesas.']\n",
            "['3', 'Sisters, even the very youngest in this audience can rise up in faith and play a significant role in building the kingdom of God.', 'Hermanas, incluso las más jóvenes de esta audiencia pueden levantarse en fe y desempeñar una función importante en la edificación del reino de Dios.']\n",
            "['4', 'Children begin gaining their own testimonies by reading or listening to the scriptures, praying daily, and partaking of the sacrament in a meaningful way.', 'Los niños empiezan a obtener su propio testimonio al leer o escuchar las Escrituras, orando diariamente y participando de la Santa Cena de una manera significativa.']\n",
            "['5', 'All children and young women can encourage family home evenings and be full participants.', 'Todos los niños y las mujeres jóvenes pueden alentar a que se haga la noche de hogar y ser participantes plenos.']\n",
            "['6', 'You can be the first one on your knees as your family gathers for family prayer.', 'Pueden ser los primeros en arrodillarse cuando su familia se reúna para la oración familiar.']\n",
            "['7', 'Even if your homes are less than ideal, your personal examples of faithful gospel living can influence the lives of your family and friends.', 'Incluso si sus hogares no son tanto como ideales, su ejemplo personal de vivir el Evangelio con fidelidad puede ser una influencia en la vida de sus familias y amigos.']\n",
            "['8', 'Young women of the Church need to see themselves as essential participants in the priesthood-directed work of salvation and not just as onlookers and supporters.', 'Las mujeres jóvenes de la Iglesia necesitan verse a sí mismas como participantes esenciales en la obra de salvación dirigida por el sacerdocio y no solo como espectadoras y seguidoras.']\n",
            "['9', 'You hold callings and are set apart by those holding priesthood keys to function as leaders with power and authority in this work.', 'Ustedes tienen llamamientos y son apartadas por quienes poseen las llaves del sacerdocio para funcionar como líderes con poder y autoridad en esta obra.']\n",
            "['10', 'See 2 Corinthians 5:7.', 'Véase 2 Corintios 5:7.']\n",
            "['11', 'Doctrine and Covenants 50:24.', 'Doctrina y Convenios 50:24.']\n",
            "['12', 'October 1, 2016', '1 de octubre de 2016']\n",
            "['13', 'We are surrounded by such an astonishing wealth of light and truth that I wonder if we truly appreciate what we have.', 'Estamos rodeados de una riqueza de luz y verdad tan extraordinaria que me pregunto si realmente apreciamos lo que tenemos.']\n",
            "['14', 'How blessed we are to assemble again in this worldwide conference under the direction and leadership of our dear prophet and President, Thomas S. Monson.', 'Qué bendecidos somos de reunirnos nuevamente en esta conferencia mundial bajo la dirección y el liderazgo de nuestro querido profeta y presidente, Thomas S. Monson.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T3twhHtgscCE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model architecture and stuff from before"
      ]
    },
    {
      "metadata": {
        "id": "JkOh5YOmsbNP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Called in Encoder, Decoder, EncoderLayer, DecoderLayer, MultiHeadedAttention\n",
        "def clones(module, N):\n",
        "    \"\"\"Produce N identical copies of a layer (for multi-headed attention).\"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "# Called by MultiHeadedAttention\n",
        "def attention(query, key, value, mask=None, dropout=0.0):\n",
        "    \"\"\"Compute scaled dot product attention.\"\"\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    \n",
        "    # jupyter: dropout default is None, and this line becomes\n",
        "    # if dropout is not None: p_attn = dropout(p_attn)\n",
        "    # Effectively does the same thing\n",
        "    p_attn = F.dropout(p_attn, p=dropout)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "# Used to generate predictions\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Standard linear + softmax generation step.\"\"\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "# Standard piece of the model; called by make_model\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"Two linear transformations with a ReLU in between.\"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        # Torch linears have a `b` by default. \n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "# Allow the model to pay attention to relative positions of tokens\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sinusoid-based positional encoding to allow the model to extrapolate to \n",
        "    sequence lengths longer than those seen in training.\n",
        "    \n",
        "    One sinusoid of a different frequency for each dimension in the embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model) # 2d\n",
        "        #position = torch.arange(0, max_len).unsqueeze(1) \n",
        "        #div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "        #                     -(math.log(10000.0) / d_model))\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).double()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).double() *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        \"\"\"\n",
        "        pe = np.zeros((max_len, d_model))\n",
        "        position = np.arange(0, max_len).reshape((max_len, 1))\n",
        "        div_term = np.exp(np.arange(0, d_model, 2) * -np.log(10000.0/d_model))\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "        pe = torch.tensor(pe, dtype=torch.long).unsqueeze(0)\n",
        "        \"\"\"\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)#.float()\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Learned embedding model for conversion of input/output tokens.\n",
        "class Embeddings(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Scale by sqrt(d_model) (to match the scaled dot product?)\n",
        "        return self.lut(x) * math.sqrt(self.d_model)\n",
        "    \n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"\"\"Take in model size and number of heads.\"\"\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.p = dropout\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "                             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "    \n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"Basic layer normalization.\"\"\"\n",
        "    \n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2*(x - mean)/(std + self.eps) + self.b_2\n",
        "    \n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"A residual (skip) connection followed by a layer norm.\"\"\"\n",
        "    \n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        # Apply residual connection to any sublayer with the same size\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "# This gets fed to an Encoder class\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"An encoder layer consists of self-attention and feed-forward.\"\"\"\n",
        "    \n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"Follow Figure 1 in the paper for connections.\"\"\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "    \n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"The main encoder is a stack of N encoder layers.\"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A decoder layer consists of self-attention, source-attention, and \n",
        "    feed-forward.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"The main decoder is a stack of N decoder layers.\"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "    \n",
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, \n",
        "               dropout=0.1):\n",
        "    \"\"\"\n",
        "    Put all the pieces of the model together, given the source and target \n",
        "    vocabularies, number of layers for the encoder and decoder, dimensions for\n",
        "    the model embedding, dimensions for feed-forward, number of heads to use \n",
        "    for multi-headed attention, and dropout parameter.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convenience definition to make copies of things\n",
        "    c = copy.deepcopy\n",
        "    \n",
        "    # Attention, feed-forward, & positional encoding to feed Encoder and Decoder\n",
        "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    \n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # Important from their (whose?) code. Initialize params w/ Glorot or fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model\n",
        "\n",
        "# Note: This part is incredibly important. \n",
        "# Need to train with this setup or the model is very unstable.\n",
        "\n",
        "# Called by get_std_opt to make model_opt in main\n",
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
        "    \n",
        "# Used to make 'criterion' in main\n",
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        \n",
        "        # If the mask is not empty\n",
        "        if mask.nelement() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))\n",
        "\n",
        "# Make the batches to iterate over for training and validation\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "                \n",
        "# Called in main to make 'model_opt'\n",
        "def get_std_opt(model):\n",
        "    \"\"\" Set up default parameters for the Noam Optimizer. \"\"\"\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), \n",
        "                             eps=1e-9))\n",
        "\n",
        "# Called by make_std_mask \n",
        "def subsequent_mask(size):\n",
        "    \"\"\"\n",
        "    Mask out subsequent positions to ensure predictions for position i can only\n",
        "    depend on the known outputs at positions less than i.\n",
        "    \"\"\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "\n",
        "# Called to set up the MyIterator things\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"\"\"\n",
        "    Ensure that the batch size padded to the maximum batchsize does not\n",
        "    surpass a threshold.\n",
        "    \"\"\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)\n",
        "\n",
        "class Batch:\n",
        "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "        self.src_mask = src_mask\n",
        "        self.trg_mask = trg_mask\n",
        "        self.ntokens = ntokens\n",
        "        \n",
        "# Called by data_gen and rebatch\n",
        "def make_std_mask(src, tgt, pad):\n",
        "    \"\"\" Hide future words and batch padding. \"\"\"\n",
        "    src_mask = (src != pad).unsqueeze(-2)\n",
        "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(\n",
        "        tgt_mask.data))\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "# Called on the stuff in MyIterator at each epoch\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"\"\"\n",
        "    Ensure that we have very evenly divided batches with minimal padding\n",
        "    (despite variations in sentence length).\n",
        "    \"\"\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
        "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n",
        "\n",
        "# Loss function (compute each timestep separately to optimize memory)\n",
        "def loss_backprop(generator, criterion, out, targets, normalize):\n",
        "    \n",
        "    assert out.size(1) == targets.size(1)\n",
        "    total = 0.0\n",
        "    out_grad = []\n",
        "    for i in range(out.size(1)):\n",
        "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
        "        gen = generator(out_column)\n",
        "        loss = criterion(gen, targets[:, i]) / normalize.float()\n",
        "        total += loss.data[0]\n",
        "        loss.backward()\n",
        "        out_grad.append(out_column.grad.data.clone())\n",
        "    out_grad = torch.stack(out_grad, dim=1)\n",
        "    out.backward(gradient=out_grad)\n",
        "    return total\n",
        "\n",
        "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], \n",
        "                             batch.ntokens) \n",
        "                        \n",
        "        model_opt.step()\n",
        "        model_opt.optimizer.zero_grad()\n",
        "        if i % 10 == 1:\n",
        "            print(\"Batch\", i, \"Loss\", np.round(loss.item(),4), \n",
        "                 \"Learning Rate\", np.round(model_opt._rate,8))\n",
        "            \n",
        "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    for i, batch in enumerate(valid_iter):\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], \n",
        "                             batch.ntokens) \n",
        "        print(\"Batch\", i, \"validation loss:\", loss.item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qivCD21sfM3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Different for GC dataset"
      ]
    },
    {
      "metadata": {
        "id": "uw_4XhtPjP8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up tokenizing of input--modify slightly for en/es vs. en/de\n",
        "spacy_es = spacy.load('es')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_es(text):\n",
        "    return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "#for i in range(10):\n",
        "#    print(tokenize_es(all_rows[i][2]), tokenize_en(all_rows[i][1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_Vpf2m898SM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set up a few special tokens\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "\n",
        "# data is a torchtext module, remember\n",
        "SRC = data.Field(tokenize=tokenize_es, pad_token=BLANK_WORD)\n",
        "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "gc_data = data.TabularDataset(gc_data_csv, 'csv',\n",
        "                              fields=[('Index', None),('trg', TGT),('src', SRC)],\n",
        "                              skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLR7lByZmFG4",
        "colab_type": "code",
        "outputId": "5d4dbbdd-0574-4032-dfff-d0b91db6b189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "print(gc_data[0])\n",
        "print(gc_data[0].src, gc_data[0].trg)\n",
        "train, val = gc_data.split(split_ratio=0.99)\n",
        "print(len(gc_data))\n",
        "print(len(train), \":\", len(val))\n",
        "\n",
        "MAX_LEN = 100\n",
        "MIN_FREQ = 1\n",
        "\n",
        "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
        "\n",
        "print(len(SRC.vocab))\n",
        "print(len(TGT.vocab))\n",
        "\n",
        "# Set up a few constant parameters\n",
        "n_src = len(SRC.vocab)\n",
        "n_tgt = len(TGT.vocab)\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.data.example.Example object at 0x7f7ba3441278>\n",
            "['El', 'templo', 'tiene', 'un', 'lugar', 'en', 'el', 'centro', 'mismo', 'de', 'nuestras', 'creencias', 'más', 'sagradas', 'y', 'el', 'Señor', 'nos', 'pide', 'que', 'asistamos', ',', 'meditemos', ',', 'estudiemos', 'y', 'encontremos', 'significado', 'personal', 'y', 'aplicación', 'individual', '.'] ['The', 'temple', 'holds', 'a', 'place', 'at', 'the', 'very', 'center', 'of', 'our', 'most', 'sacred', 'beliefs', ',', 'and', 'the', 'Lord', 'asks', 'that', 'we', 'attend', ',', 'ponder', ',', 'study', ',', 'and', 'find', 'personal', 'meaning', 'and', 'application', 'individually', '.']\n",
            "105786\n",
            "104728 : 1058\n",
            "46989\n",
            "32037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3M_gOaDsoNDw",
        "colab_type": "code",
        "outputId": "ce5b32e1-7cfa-470e-d243-fadee4dc331e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "# Batch size is 4096 in the colab notebook and 12000 in the jupyter\n",
        "BATCH_SIZE = 4096\n",
        "\n",
        "# Set up the model \n",
        "model = make_model(n_src, n_tgt, N=6)\n",
        "model_opt = get_std_opt(model)\n",
        "model.cuda()\n",
        "\n",
        "# Set up the label smoothing to penalize overconfidence\n",
        "criterion = LabelSmoothing(size=n_tgt, padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "\n",
        "# Set up training and validation iterators\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True)\n",
        "\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), \n",
        "                                                          len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "num_train_batches = 0\n",
        "for i, batch in enumerate(train_iter):\n",
        "    num_train_batches += 1\n",
        "num_valid_batches = 0\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    num_valid_batches += 1\n",
        "print(\"\\nNumber of training batches:\", num_train_batches)\n",
        "print(\"Number of validation batches:\", num_valid_batches, \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:263: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of training batches: 781\n",
            "Number of validation batches: 10 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fd0pWI7KtAd8",
        "colab_type": "code",
        "outputId": "49627fe3-3354-41e3-af9d-1798a618778f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(15):\n",
        "    print(\"\\n\\n%%%%%%%%%% EPOCH \" + str(epoch) + \" %%%%%%%%%%\\n\")\n",
        "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, \n",
        "                model_opt)\n",
        "    valid_epoch((rebatch(pad_idx, b) for b in valid_iter), model, criterion)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "%%%%%%%%%% EPOCH 0 %%%%%%%%%%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-33731378ff6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n%%%%%%%%%% EPOCH \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" %%%%%%%%%%\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, \n\u001b[0;32m----> 4\u001b[0;31m                 model_opt)\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrebatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-02e44fd61aa0>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_iter, model, criterion, opt, transpose)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-33731378ff6d>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n%%%%%%%%%% EPOCH \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" %%%%%%%%%%\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, \n\u001b[0m\u001b[1;32m      4\u001b[0m                 model_opt)\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrebatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-02e44fd61aa0>\u001b[0m in \u001b[0;36mrebatch\u001b[0;34m(pad_idx, batch)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \"\"\"\n\u001b[1;32m    399\u001b[0m     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_std_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-02e44fd61aa0>\u001b[0m in \u001b[0;36mmake_std_mask\u001b[0;34m(src, tgt, pad)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_std_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;34m\"\"\" Hide future words and batch padding. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0msrc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0mtgt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/aten/src/THC/generated/../THCTensorMathCompare.cuh:84"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cpew83y5tYSq",
        "colab_type": "code",
        "outputId": "fce567dc-a6ae-4748-9f5b-08d7da84f894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1691
        }
      },
      "cell_type": "code",
      "source": [
        "# Predict a translation using greedy decoding for simplicity\n",
        "# You must use the jupyter EncoderDecoder class to run this!\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    \n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], \n",
        "                       dim=1)\n",
        "    return ys\n",
        "\n",
        "# Decode the model to produce translations (first sentence in validation set)\n",
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "print(TGT.vocab)\n",
        "\n",
        "for i, batch in enumerate(valid_iter):\n",
        "    \n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    print(out.size())\n",
        "    #print(out)\n",
        "    print(batch.trg.data.size())\n",
        "    #print(batch.trg.data)\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    \n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        print(batch.trg.data[i,0])\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<torchtext.vocab.Vocab object at 0x7f8418965c18>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  return Variable(arr, volatile=not train)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t0 \n",
            "Target:\ttensor(4, device='cuda:0')\n",
            "0 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(13, device='cuda:0')\n",
            "5 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(13, device='cuda:0')\n",
            "5 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(11, device='cuda:0')\n",
            "9 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t0 \n",
            "Target:\ttensor(10, device='cuda:0')\n",
            "1 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(11, device='cuda:0')\n",
            "9 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(13, device='cuda:0')\n",
            "5 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t0 \n",
            "Target:\ttensor(7, device='cuda:0')\n",
            "4 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(4, device='cuda:0')\n",
            "0 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(12, device='cuda:0')\n",
            "7 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(11, device='cuda:0')\n",
            "9 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 83])\n",
            "Translation:\t0 \n",
            "Target:\ttensor(11, device='cuda:0')\n",
            "9 tensor(3, device='cuda:0')\n",
            "\n",
            "\n",
            "torch.Size([1, 60])\n",
            "torch.Size([3, 62])\n",
            "Translation:\t2 \n",
            "Target:\ttensor(10, device='cuda:0')\n",
            "1 tensor(3, device='cuda:0')\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hCWPArgj07J7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}